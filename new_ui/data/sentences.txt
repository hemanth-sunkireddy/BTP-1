Hi everyone.
In this video, we're going to talk about multilinear regression.
So last time we talked about multilinear regression with the higher order terms of a single variable, and this time we're going to talk about multilinear regression model when there are multiple variables.
So we're going to see how to interpret these coefficients and then how to inspect whether these coefficients are significant.
And we're going to talk about how to select features.
and what to consider when we select features.
And we'll talk about highly correlated features and multicollinearity.
What are they?
How does it affect the model?
And what can we do when we select features?
And we're going to talk about what other things to consider when we select features.
And lastly, we'll talk about what to do when there are interactions between the features.
So, as you know, the multilinear regression model can be formulated by this.
So, all the variables are linearly combined to represent the model.
to predict the target variable Y.
And the coefficient, each coefficient is an average effect of that variable to Y target variable.
When we consider all other variables are independent and fixed.
This assumption may not be true in general.
So variables or the predictors might be correlated in real world scenario.
And also, we also assume these coefficients are constant.
However, that might not be true in general.
So if there is an interaction between two variables or more, This constants or coefficient may not be true constant but is a function of some other variables.
So we'll talk about what to do when it happens.
Let's take an example that we saw previously.
It's a house price prediction.
So house price is a Y target variable and all other variables are the features.
We're going to inspect the types of variables and see if they are suitable for linear regression model.
So what are the types of variables?
There can be real value number and there could be categorical variable.
And categorical variable can have ordinal and non-ordinal categorical variable.
What is ordinal categorical variable?
These are categories that have meaning in their order.
So something like age group or grade, abc or 1234, those can be ordinary categorical variable because they have a meaning in their order.
The examples of non-ordinary categorical variables are male, female, race, ethnicity, and so on.
Some classes they don't have any meaning in their order.
So we can permute the orders of categories and they don't have an effect.
So non-ordinal categories are difficult to use in linear regression model because in linear regression model, a variable value times the coefficient present how much of the value in target variable is contributed by that variable.
Therefore, if the variable can be permuted arbitrarily, It's not easy to use in the linear regression.
However, there are ways to use these non-ordinary categorical variables in the linear regression.
For example, we can code male, female into 0 or 1 or 1 or 0 or sometimes minus 1 to 1.
So if we choose one of these, it will work.
And then how about race?
Let's say race had only three categories, Asian, Black, and Caucasian.
So this variable has three categorical values.
So we can convert this into individual three binary categorical variables.
So is the person Asian or not?
Is the person black or not?
Is the person Caucasian or not?
However, we don't need all three of them because if the two are known, the other one can be known as well.
So they are dependent.
So if we just get rid of one of them.
and use only 2 into the model, then it works better.
So in general, if the non- ordinary categorical variable had n categories, we could convert them into binary categorical variables.
But you have to also consider whether you want to include n-1 new features into your model.
So what if you had a really large n?
Do you want to add large n-1 features into your model?
Probably not.
That is an example of this zip code.
So zip code had 70 something categories and I didn't want to add 70 something new binary variables into my model because my model then becomes too big.
So hopefully the other variables such as latitude and longitude can capture some information about the location of the house.
So I'm gonna just get rid of zip code and then use other variables.
Before we build a model, let's have a qualitative inspection.
So, in this case, we're gonna see the correlation between the price and all other variables and see if which variables might be useful to predict the price.
So, square foot living could be useful, grade could be useful, and some other variables such as square foot above or square foot living 15 could be useful.
By the way, this square foot living 15 is a square foot living of similar 15 houses.
So therefore, This must have some redundant information as square foot living and also the square foot living is a square foot above plus the square foot basement so they are linearly dependent.
So they must have redundant information and this redundant information shows the high correlation between the features.
So these two variables have really high correlation because they are linearly dependent and this square foot living and square foot living 15 because they are similar in the definition they are also highly correlated.
So we identified a few variables that are correlated to each other and some variables are linearly dependent to each other.
This can be also visually inspected in the pair plot.
So pair plot is distribution plot between two features.
So in the diagonal element, it shows the distribution of itself, the feature itself.
And the off-diagonal element shows the distribution between one feature to the another.
So for example, square foot above and square foot leaving had a really high correlation and you can see very skinny distribution of the data.
Actually, this is a very good indication of a collinearity, which we will talk about later, but essentially that happens because these two features are dependent each other or nearly dependent each other.
Okay, so we've qualitatively inspected variables and their correlations and we found that some features may have some redundant information and they may cause some problems.
So with that in mind, let's see what happens if we throw all the features into the model and fit to the data.
So here are the results summary table and we can see that model fit with the R squared is almost 0.7, which is good value.
And previously we didn't explain what this f-statistic value is and what the p-value for the f-static is.
And this actually shows whether there is at least one significant variable in the model.
So...
The null hypothesis for F-test would be all the coefficient values are 0.
So the F-value is defined by this formula.
So TSS minus RSS divided by number of features and divided by RSS times n minus p minus 1.
So this is a formula for F-test.
And then the bigger this number, we are more sure about there is at least one.
significant variable in the model.
So in our case, the F statistic value is a big and the p-value for that is almost a zero.
That means it's smaller than certain threshold of error rate.
Therefore, we can conclude that our model has at least one significant variable.
So let's have a look at the p-values for individual variables and we can see immediately that some variables have insignificant coefficient values.
So, square floors have a really high p-value.
As well as sales months have high p-value.
So, we can reject these features because their coefficient values are essentially zero.
Alright, so after removing the features that has a high p-values, we get this result.
So, R-squared value is similar.
f-statics value is still large.
and let's inspect the t-score and the p-values of each individual coefficients and all of them looks statistically significant and that's good.
However, this is not the complete story because we still see some features such as these three linearly dependent each other still exist in the model and they still have a high correlation value.
So we're going to talk about some better ways to automatically add or remove the features in the next video.
In this video, we're going to talk about feature selection method and things to consider when we select features.
So last time, we tried to fit the model that has all the features inside and found that some of the coefficients were not significant.
So after removing those, we tried again and found that the coefficients are significant, but still we had some linearly dependent features in it.
So it's a lot of manual process to figure out which features to select.
So instead of doing that, we're going to introduce some method that automatically selects the features.
So the first method is called the forward selection, which add the feature one by one by looking at the one that maximizes the R-squared value.
So the add feature that maximizes the R-squared value, and that's the forward selection.
And another method is called the backward selection, which starts from the full model.
So by full model, I mean there are all the features inside the model already.
And remove the one feature that has maximum p-value.
So remove xj that has maximum p-value and we repeat this process until we reach the tolerance of the p-value or stop character.
Another good method is called a mixed selection which combines the forward selection and backward selection.
Which means that we add some feature that maximize the R-squared first and then fit the new model and inspect the result and see if there are coefficients that are insignificant and if there are insignificant coefficients, just remove the features.
And then we add another feature again and then inspect the result and remove all the features that have large p-values and so on.
So let's compare the result.
So for selection gives this result that it adds a square for living first and then latitude and view and grade and so on.
And it doesn't have a stock criteria so we're gonna just fit all of them to the last feature.
And then this is result from the backward selection.
It starts from the full model and then it removes one by one.
So it first removed the floors and then secure-floor lot, second and sales months removed and so on all the way to the top.
And the year built was the last one that was removed.
But as you can see, the feature importance are the orders are very different from the floor selection because the floor selection cares about the R-squared value whereas the backward selection cares about the .
p-value and as you can imagine the mix selection resembles the four selection result.
However, it stops at some point.
So because it also look at the maximum r squared, the order orders are pretty similar to four selection, but then at some point adding another feature will lead always have a p value that's larger than certain criteria, so it stops there.
So actually practically mix selection is a good way to use.
as a feature selection.
So here is the result of the correlation matrix after we select the features from mixed selection.
So good news is that now we don't have the linearly dependent feature such as such as a square foot above.
These are gone, but still we see large correlation values between the features.
So let's talk about correlated features.
Why do they occur?
So high correlation among features may occur from different regions.
One of them would be redundant information.
So when the features are linearly dependent on each other, the information is redundant and they may have a high correlation.
And when there is an underlying effect such as confounding or causality, the features may be highly correlated.
So for example, ice cream sales and the sharks attack.
They have nothing to do with each other.
However, they can be caused by hot weather.
So, hot weather here is called confounding and then because of this confounding, ice cream sales and then sharks attack, they will have a high correlation in the data.
And the example of causality is heart disease can lead to heart attack and diabetes doesn't cause heart attack directly, but it can cause a heart disease, some type of heart disease and then cause a heart attack.
So when we look at the diabetes and then heart attack, they may look highly correlated.
And in some cases, just the variables are correlated in nature.
So for example, number of bedrooms and the size of house, they don't cause each other.
They don't have confounding.
However, they are just correlated.
So they may have high correlation.
So we mentioned that it is problematic when there are highly correlated features.
And why is that?
When the predictors are highly correlated, the coefficient estimate becomes very inaccurate.
And also, the interpretation of the coefficient.
as a variable contribution to the response becomes inaccurate.
So when there is a high correlation between features more than 0.7, we consider it's problematic.
And this is especially called collinearity when two features are very similar to each other.
Like we saw previously in the pair plot that the distribution of the data was very skinny between feature 1 and feature 2, for example, then they are very collinear.
Well, it's not always possible to detect the collinearity using correlation metrics.
Because if there are multiple variables that are involved in the collinearity, they may look like okay in the correlation matrix.
However, they could be still collinear.
And this special case is called the multicollinearity.
So beside the correlation matrix, variance inflation factor is a better way to detect this multicollinearity.
So VIF is defined by this formula.
The VIF of a coefficient beta i.
is 1 over 1 minus R squared value of this fitting and this fitting is actually not fitting the target variable Y but fitting that variable Xi using all other variables.
So using other variables that are not this one and we are fitting the model and we're gonna get the R squared value and we can get the VIF value.
So if the VIF value is larger than 5 in general, or sometimes 10, it means that there is strong multicollinearity.
So let's have a look which variables had multicollinearity in the original model that we had all the features in it.
So clearly, square foot living, square foot above, square foot basement, they are linearly dependent each other.
So they show strong multicollinearity.
And then after we have a mixed selection, some of the redundant features are gone.
For example, this one is gone.
And let's inspect.
Square-foot living has still high VIF value, but it's much better than previous one because one of the dependent feature was gone.
And I think that was the only one that was bad here.
All right, but when we see the correlation matrix, there are still some highly correlated features.
Like for example this one.
So let's remove these highly correlated features.
after the mixed selection.
And when we remove those variables with a very high correlation to the square foot living, we end up with a much lower VIF value for square foot living because the collinearity is gone.
Here are some things to consider when we select features.
So we talked about model fitness.
Four selection gives a maximum model fitness by adding one features at a time.
And also we talked about removing variables with the insignificant coefficients.
So backward selection was good at this.
And if we combine this, we can have mixed selection.
We also talked about some problems that may occur when you have a multi-core linearity.
And we haven't talked about this, so let's have a look.
So here is a graph that shows the performance of each model.
This is including intercepts.
So this is just an intercept and this model is intercept 1.
feature and so on all the way to 14 feature plus intercept.
So it shows here.
And this star represents the model performance after we removing the variables with the high correlation.
So when we remove the highly correlated features, they may have a better estimation of the coefficient value and then better interpretation.
However, it can have some less performance.
And another thing that we can think about is that do we need all these 14 features?
It seems that the model complexity 6 or 7 gives a efficient result.
This is still a less performance than having 14 features.
However, it's good enough.
So we can consider that as well.
And by looking at the VIF of this six feature model, it has a pretty good VIF as well.
So here is all the results from the models that we considered so far.
So again, this number of features include the inner set.
So it's actually 19 feature model plus one inner set.
And mix selection gives a 14 number of features selected and so on.
And then if we look at just the feature coefficient for square foot living, the coefficient values are all different.
All of them are statistically significant.
However, if you can see the coefficient values, they are very different.
In particular, this model removed all the features that are highly correlated to the square foot living.
Therefore, the coefficient value for square-foot living is more accurate and more interpretable.
So this means that when we increase square-foot living by 1, then the house price goes up by $313.
On the other hand, the coefficient value for square-foot living is lower in other models.
And that's because the other models still had other variables that were highly correlated to the square-foot living.
Therefore, the coefficient values are not accurate.
and all these correlated features, they kind of share the contribution to the house price.
Okay, so lastly, let's talk about what to do when there are interactions.
So what is interactions?
Interactions can happen when this coefficient is not constant, but is a function of some other variable, say x3.
So in that case, what we want to do is that we're going to have interaction term, so x1 times x3, and then assign another coefficient.
Let's say beta and then add to the model.
And not only this, we can also do all the combinations such as adding x1 times x2, x2 times x3 and all combinations.
And we can also have a higher order terms, something like that.
In that case, we have infinite menu of features and we don't want to do that.
But however, we can just choose the order.
Maybe the maximum order is just one feature times another.
and then we can add them up.
Then we have a problem of how to select all these many combination features.
Again, we're going to apply the same method that we talked about before.
So, mixed selection method is a good way to do that.
One thing that is a little different from the previous case is that when we have this interaction term, then we must include also β1x1 plus β3x3 And sometimes you might see these coefficient values may not be significant.
However, we should still include these terms in order to have this term.
So with that difference, having interaction terms in the model is the same as having multiple features in the model.
Hi everyone, in this video we're going to talk about hyperparameters of decision trees.
So as a quick review, here is how decision tree splitting works.
So from the root node, it has samples and it's gonna pick a feature and its threshold value to minimize the sum of the MSE of the splitted node.
So like this, and then it will further split and pick another feature and threshold value.
like this.
And we also talked about different metrics for different tasks.
So for the regression trees, we use MSC, MAE, or RSS to split the node.
And for classification tasks, the tree uses Gini and entropy, or information gain sometimes, to split the node.
And in this video, we're going to talk about some usage in SKLang, how to fit the models.
some useful functions and we'll talk about hyperparameters of the decision trees that we need to pick values such that we minimize overfitting.
So here are some references that you can look at the document and they have useful stuff.
So we simply import decision tree regressor and classifier from the sklon tree module and for example if it was classification task we can construct a model by just simply calling this decision tree classifier.
and then fit the data, the features and the labels, and here are the snapshots from the document that shows that it has many many other options.
Alright, so we'll talk about some of them.
Another useful function that is also contained in that escapelontree module is the plot tree.
When we pass the fitted object to the plot tree function, it's going to return some list of text objects and then also the visualization of this.
We can also use export graphics function from sk1 tree to make a fancier visualization.
To do that we're gonna use graphics and some other modules and the usage will look like this.
We just pass this object fitted object and then it's going to convert this text object to a graph object and then the image function will create an image out of this.
So it will look like this.
So if you see more red and more blue it means that the node is more pure and if you see white node that means it's kind of 50-50 or very mixed there.
So it's a little bit fancier but essentially kind of the same.
Decision trees, while they are easy and useful to understand, they have some drawbacks.
They are very easy to overfit so we're gonna talk about some strategies to prevent overfitting.
So first strategy is stopping the tree to grow.
It's called all-stopping.
And second strategy is called pruning.
We'll talk about that later.
And another good strategy is ensembleing the trees.
All right, so how do we stop the tree grow only?
We have a bunch of hyper parameters listed here and we can pick some values such that we can stop the tree grow.
So for example, maxDepth will limit the tree, the depth of the tree, so that it can stop growing when it reaches certain depths.
And meanSampleSplit will make the node stop splitting when it has a less number of samples arrived in that node.
And meanSampleSleep also can stop tree grow further or node split further when it has a certain number of samples in the leaf node.
So they are kind of similar.
mean weight fraction lift are also similar.
It is a continuous version of mean samples lift, so instead of number of samples, we'll look for the weight fraction of the node.
And mean impurity decrease also stops splitting at that node if the impurity decrease from that node is negligible or less than certain number.
And max features also can help with the overfitting because it can make the model less flexible by looking at the less number of features when we have so many features.
And there are more design parameters in the sklearn implementation of decision trees, which you can also look at the documentation, but we'll focus on just a few.
So the most direct way to prevent overfitting in the decision tree is a max depth.
So by just limiting the depth, we can directly make the tree not grow.
And the minimum sample width is also very useful.
So the smaller the number of the sample of the leaf, that means the model is more flexible.
So if you want to make the model less flexible, so less overfit, then increase this number.
Another one to try is an impurity decrease.
However, you will have to know some values, so you will have to give some trial and error.
An impurity decrease is calculated as this one.
So when there are n samples in the parent node and it splits to an L and an R.
the impurity decrease or information gain is given by the impurity of the original node minus the weighted sum of the impurity of the children node so the weights will be the fraction of the sample numbers times the impurity of the left box and the weight of the right box times impurity of the right box.
So that's the impurity decrease.
So you will pick some value threshold.
and see what happens.
Other useful options that you can use when you build a model is the max features.
So it's going to limit the feature number and usually square root or log options are popular.
Square root is more popular by the way.
And class weight by default is none, but if you use a balance, it usually gives a better performance, especially true when you have imbalanced labels.
And CCP-alpha is used when you use minimal complexity pruning.
So we'll talk about this more in detail in the pruning video.
So how do we choose its hyperparameter values?
We might have some heuristic values or just try a few values.
However, we can also do a pragmatic approach like grid search.
Unfortunately, sklearn library also have a very convenient tool.
called gridSearchCV.
It does grid search as well as cross-validation so that it makes sure it's not just a one-pick value that was out of luck, but it does cross-validation, which will split the data into by default five chunks and then and it's going to fit the model and then get the accuracy from this chunk and this chunk and then it will average the result.
And it will give the result that which model hyperparameter gave the best result.
So from model selection module, we can call the gridSearchCV.
And this is individual decision tree classifier.
I just happen to call RF, but you can call whatever.
And then these parameters are dictionary that shows that which hyperparameters and which values you want to change to.
So I gave some different options.
And then...
I put these two objects in the grid search CVE and after fitting the grid search object with the data, we get some we can we can call the result by dot best estimator.
It will return what was the best estimator and gives the hyper parameter values here and dot best score will give the what was the accuracy value for the classification when we use these hyper parameters.
Alright so these are some handy tools.
So we showed how to use sklearn library for constructing decision trees and how to use grid search to find the hyperparameter values.
In the next video, we're going to talk about pruning the decision trees as a part of strategies of preventing overfitting.
Hey everyone, in this video we're going to talk about decision tree classifier and their split criteria.
So decision tree classifier look exactly like decision tree regressor.
This is a representation of the decision tree classifier in the HERT dataset.
It's binary class classification, so at the end of the day in the terminal node we'll have a few samples.
It seems like it has only one or two samples or just a few at each terminal node.
When we don't stop growing tree in the middle, it will just fully grow until it has a pure node, everything pure in the terminal node.
Alright, so if we zoom in some first a few nodes, it will look like this.
So like Decision Tree Regressor, it will pick a a criteria So which feature to split on and which feature value on that feature to split on.
So for example, out of these 13 features, it chose a thal and less than equal to the value of 4.5.
It will split whether it's true, satisfy this condition or not.
And then it will lead to children rules.
So then the next question we can ask is how does decision tree classifier pick this split criteria.
So it works very similar to decision tree regressor.
So in decision tree regressor, there was some samples in the original boxes and then we picked the criteria such that the splitted box, so true and false, we measure MSC here.
It doesn't have to be MSC.
It could be RSS or MAE.
So MSC of the left box and MSC of the right box.
we have different you know choices of how to split the box, the original box.
So we'll go through this is feature one, this is feature two, then it will try to split everything in every possible way like this and then measure the resulting left and right, left and right, left and right, and then pick the one, pick one split that actually gave the best result.
By best I mean the minimize the total MSC.
So, it similarly works that way, except that now the metric that we use to calculate this left box and right box result is Gini instead of MSE.
So Gini is a measure of impurity.
So decision tree classifier measures the impurity of the left box and then the impurity of the right box and then it will also inspect all the split possibilities like this.
every combination and it will pick the one that gives the minimum total impurity.
All right, okay, so again the regression decision tree regressor has MSC or RSS, same as RSS, and MAE as a metric that helps the finding split criteria.
And for classification, we have three choices.
It could be more but you know these three are most popular.
So Gini is a measure of impurity.
and it looks like this.
Actually when you look at the RSS which is a measure of variance of that box, Gini is somewhat similar because when you just think about coin flip problem, this is a variance of the Bernoulli probability distribution function.
So Gini somehow it's measuring the variance like RSS does or MSC does, but Gini is a measure of impurity.
So I just wanted to mention that they have some similarity.
An entropy is a measure of uncertainty.
So uncertainty in the information theory means that when there is a packet, we don't know the value of the packet, whether it's a 0 or 1.
If we don't know fully, then the uncertainty is maximized, therefore the entropy is maximized.
However if we have certain information that maybe 80% of chance that it's a 0 and maybe 20% of chance it's 1, then we have certain information and less uncertainty than 50-50 chance.
So the entropy measures that and the formula looks like this.
So there is minus sign over here and the information gain is a difference in the entropy of the one parent node and it's a binary split the children node.
So some of these two entropies left box and right box or it could have some weight as well if they have different number of samples.
So we'll see them in more detail.
Alright, so Gini and entropy, they measure the same kind of property, purity or impurity or uncertainty, they are similar concepts.
So you can kind of intuitively think about this case, you have some bag and if everything is kind of fully mixed, you have blue marble and red marble kind of mixed 50-50 like this.
and our goal is to separate them to everything pure in the two small bags and if everything is perfectly separated we'll have all blues in one bag and all red in the other bag so we have full visibility and we are like 100% sure that which one is which.
If they are not separated well enough, then maybe they will have 80% of blue marbles in one bag and 20% of red marble in the same bag and the other one has 80% of red marbles and then 20% of blue marble.
And if the separation was bad, then everything will be just 50-50.
Something like this.
So there is no useful information in this case.
We didn't gain any information.
Everything is just mixed together so we don't know which bag contains which color.
However, there are certain information here.
We have some certainty and this is...
100% certain that we know which one is which.
So this is pure and this is, we can say maximally impure and somewhat impure.
And this is uncertain.
This is certain.
So our goal is to make the node split such that the splitted nodes are as pure as possible.
So to do that, we're gonna use Gini and entropy as a metric and the decision tree split algorithm will inspect every possible split along one feature and will also scan every features and will pick the split criteria.
Alright, so split criteria using Gini index.
So Gini function look like this.
So in the binary case, it's symmetric.
So we can draw the function like this and it's a maximum and 50-50 mixture and then it's a zero at the pure node.
So let's have some example.
Let's practice calculating Gini.
So what's the Gini of this entire box?
Well, as a burr puck, it's a fully mixed five cats and five tigers.
So the genie would be one half.
So genie for the binary case, one half is the maximum value.
We can calculate the two.
So in this box, five out of ten sample is a cat.
So one half.
times 1 minus 1 half is 1 half as well.
Plus, when you switch the label as a tiger, the probability of having tiger in this box is 1 half, same as 1 minus 1 half.
So altogether it's going to be 1 half.
So that's the genio of this entire box.
Now, let's say we had some split like this.
And what is the genio of this left box?
Well, the left box is pure.
Everything is cat.
So we're gonna have 0, Ginny, in the left box.
In the right box, we have a 1 cat out of 6 samples, and the 5 are tigers, so the probability of cat is 1 6, and the probability of tiger is 5 6.
So according to this formula, it's symmetric, so 1 6 times 1 minus 1 6 is 5 6.
plus 5 6 1 minus of that is 1 6 again so it's going to be 5 18th so that's the genie of the right box all right so let's talk about entropy so entropy is again measure of uncertainty if it's a 50-50 mixture in the binary class it's the most uncertain and This is when we use a natural log, but it's also common to use a base of 2, log2, when we have a binary class classification.
Actually, you can use anything like log of base 2 or 10 or natural log.
Doesn't matter, but I think the natural log is the most popular choice.
But anyway, if we use log base of 2 when the binary class classification, the maximum entropy value becomes 1.
but in this case because I used the natural log, it's some weird value here.
But anyway, that is the same as that it's the max at 50-50 mixture and it's zero at pure nodes.
All right, and information gain is a reduction in entropy.
So reduction means the entropy of the original box and minus the summed or total entropy of the split as a result of the split.
So let's have a look.
So total entropy of unsplited box is 1.
When we use the log base 2, everything is like 50-50 so we get entropy value of 1.
And let's say we split to this part again.
So the left would be 0 because everything is pure as a cat and the right box would be non-zero.
again and we use the formula then we get this so 1 6 is the probability of having cat and 5 6 is the probability of having tiger so when we calculate that the entropy of the right box is 0.65 so information gain in this case would be the original entropy which is 1 and then minus the weighted sum of those left box and right boxes.
So for left box, so we give the weight as a number of fraction of the sample.
So originally there were 10 samples, but then the left box only has a 4.
So the fraction or the weight of the left box would be 0.4.
And then the entropy of the left box is 0, so times 0 here.
Minus...
the fraction or weight of the right box is 0.6 and then the entropy of the right box is 0.65. so have this and when you calculate the information gain then we get 0.61. okay as an exercise let's have a look at this example.
so you are asked to find the width split among these three choices gives the maximum information gain.
so first choice is red split.
it will give this split.
The second choice would be green split and the third choice would be the blue split.
So which one do you think will give the maximum information gain?
You can use eyeball or you can calculate it.
Alright the answer is red and the hand waving way to explaining is that it's going to give the most number in the pure node in the left box and then also the right box is the most pure So for the red split, left box and right box, the probability of having cat is 1 and the probability of having tiger is 0, so it's very pure.
For the right box, the probability of having cat is 1, 6 and the probability of having tiger is 5, 6.
For the green split, we had one tiger in the left box and then So it's 1, 0, and then the impure box give not much gain in terms of information because you know 4 out of 9 is tiger and 5 out of 9 is cat.
So before the split they were 50-50 so 10 samples 5 cat 5 tigers so it was 5 out of 10, 5 out of 10, so in terms of changes it didn't change much.
And then the number of a pure sample is only one here.
For the blue, it has a three tigers versus everything else, so in the pure node the probability is like this, but only had the three numbers in the sample.
This is 4, this is 1.
And as a result, out of 7, 2 are tigers and 5 of them are cats.
So when you just eyeball these numbers, what do you think?
It seems like the red split gave the most pure result on the pure node and also more pure among these 3 choices.
We can be more quantitative and use an entropy formula.
So using entropy formula, we can do the red is going to be 1 minus 0.4 times 0 minus 0.6 times minus 1 times 1 6 log 1 6 plus 5 6 times log 5 6.
And for green, 1 minus 0.1 times 0 minus 0.9 times 1 times 59 log 59 plus 49 times log 49 and for blue minus 0.3 times 0 minus 0.3 7 times minus 1 times 2 sevenths log 2 sevenths plus 5 sevenths plus log 5 sevenths.
Well so yeah if we calculate that with the calculator we're gonna get a result like so this one gives a reduction of 0.61 and this one gives a reduction of 0.11 only.
It's very small reduction and then for blue it's gonna give a reduction of approximately 0.4.
So yeah, you can see quantitatively that this one gave the most reduction in entropy.
So most information gained too.
Alright, so that was it for the practice and as a summary, we talked about different metrics for regression task and classification task in the decision tree split.
So for the decision tree regressor, use MSC or RSS.
or MAE to choose the split character.
And for decision tree classifier, it uses a Gini, Entropy, and Information Gain.
And we talked about what their formula is and how to calculate them.
Alright, so in the next video, we're going to talk about how to prune the decision tree.
this error will optimize the parameter values so that this prediction value will be as close as possible to the target value.
In non-parametric models, the parameter doesn't exist.
Therefore, the question is, well, how do we optimize the model such that this prediction value gets as close as possible to the target value?
The model has hyperparameters.
usually.
They may not have, but usually they should have.
And then these non-parametric models sometimes use this error or sometimes they don't, but uses some other quantity to optimize the model.
So we'll get to that.
So examples of non-parametric models are KNN, K-nearest neighbor, which is the simplest machine learning algorithm, and then decision trees.
that uses a tree-like model.
We'll get to that later.
And support vector machine which uses distance between the points and the decision boundary or hyperplane.
So k-nearest neighbor works like this.
So imagine I have training data that looks like this.
Red dots and blue dots and the task is to classify my data points whether it's red or blue.
And let's say I have data points to classify here, and I don't know whether it's red or blue.
And k-nearest neighbors says that just take k numbers of nearest neighbors and classify to the majority of them.
So let's say if I have k equals 1, I take the closest one, I take one nearest neighbor which is red, so my green point is going to be red in this case.
If I said, if I had three neighbors, then I have two blues and one red, therefore my green points will be classified as 2.
by the majority rule, a voting rule.
If I had 5, if I had a k equals 5, then now I have a 3 red neighbors and 2 blue neighbors, so it's going to be classified as red now.
You might have noticed two things.
First, this green point kind of flips between red and blue, and second, the choice of k number is odd number, why is that?
Because If I have an even number, then I might have tie.
I might have just two red and two blues, and I don't know what to choose then.
So that's why we usually use odd number for the k values for KNN model.
Another thing you might have noticed is that why is this green swing between red and blue?
It is just happened to be that this green sits on the decision boundary.
For example, let's say this side is red and this side is blue and this green just at the right in between so it can swing.
But that's not very important.
I just wanted to show it can happen.
And another question you might ask is that can KNN do other than classification?
Yes, it can.
You can also do the regression.
The difference would be that instead of taking the majority rule here, if it's a regression, it's going to take the average of these five values for example when the k equals five.
Kaelin uses distance metric, for example, Manhattan distance and Euclidean distance.
Euclidean distance is a simple distance between these two points, whereas Manhattan distance would be the delta x plus delta y, for example.
There are more distance metrics that you can use, but these two are pretty popular.
So let's have an example.
This is from famous iris dataset.
To display more conveniently, I only use two features and then two classes of iris.
So you can see some blue points and red points are kind of mixed in some area.
So it's hard to separate.
So this two graph shows that the decision boundary, I can model.
In each case, k values are different.
And now I have a question for you.
Which of these k's have a smaller k number?
Okay, we'll see the answer here.
The answer was the left one has a smaller k value.
In fact, it was k cos 1.
And as you can see here, as the k increases, the decision boundary becomes smoother and smoother.
When the k is small, let's say 1, then I only have to consider just one neighbor.
So if my data points here, I only consider this one, and the next time my data points here, then I consider this one.
Therefore, the decision boundary can be very granular.
Therefore, it can fit to the very complex data like this.
Whereas if I have to consider many neighbors, when I'm here, I will have to consider 61 neighbors and then count red versus blue and decide which one is more dominant here, in this case red.
So the decision boundary can be very smooth in this way because I'm kind of averaging out a lot of data points.
All right, so this might remind you the concept of bias and variance.
So how's the bias and variance in K and N?
So here are some quiz.
Which model has a larger bias?
When the K is small or when the K is large?
The answer is when we have a larger K, we have a larger bias.
Why is that?
Because a K and N model with the larger K is a simpler model and it's less flexible.
you saw in the previous slides that the decision boundaries are much smoother for when K is larger.
The simpler model, which is a less flexible model, has a larger bias because it simplifies the real-world data, therefore it introduces more bias and more assumption about data.
Alright, another question.
Which model has a larger variance when the K is small or when the K is large?
So larger variance happens when the model is more flexible, therefore we can guess that the small k, k and n should have a larger variance.
So how do we determine the optimal k value?
As you saw previously that the training error goes down as the model complexity increases, test error goes down in the beginning but then it has an optimal value and it goes up again as the model complexity increases.
because the model is too complex to the data, therefore it's overfitting.
It's not generalizing very well.
So that point happens here.
So around k equals 21, the optimal value happens and the test error is minimized, whereas it can go up if we keep increasing the model complexity.
So you can see that this side is more complex model.
When the k value gets smaller, the model gets more complex, more flexible, and the other side becomes simpler and has a larger bias, larger variance.
So that's the relationship between the k and then, k of the k and n and the bias and variance.
All right.
So more KNN properties.
As we saw, it's a simple and memory-based algorithm.
Memory-based means that it just needs all the training data in order to inference.
And its time complexity is the order of number of samples times the number of features.
There can be K here as well, but if you had to rank K neighbors anyway, then there are some clever algorithms.
It's not, when you measure the time actually, it doesn't go very linearly because there are some better sorting algorithms, but anyway Time complexity is roughly n times m where this is number of samples and this is number of features and we can just support that by doing some, a few experiments.
So this data comes from OpenML and this has a 90 More than 90,000 samples with the 100 features and the task is to classify binary class.
So at k equals 9, which was the optimal value, the train time for k and n linearly increases as the number of samples increases.
Another data supports that as well.
Instead of increasing the number of samples, this time by increasing number of features, the train time also goes linearly.
So this data was...
classifying three different boundary types of the gene sequences and has 180 binary features.
Training the logistic regression on the same data set and measuring the training time can give some comparison.
Surprisingly, this KNN model is very efficient.
Well, it is usually said that KNN is slow.
because it has to measure all the distance between the points in the training data set.
So it's said to be slow, but with this number of samples, it's not terribly bad.
It has a training time very small compared to the logistic regression.
It's surprisingly fast.
And logistic regression might be slow just because it uses a fancy second derivative optimization algorithm that can run many times as well.
This graph shows that the there is an optimal k-value for the KNN model.
The test accuracy has some optimal value at certain k-value, which is 7 in this case.
So another property that KNN has is that it suffers severely from curse of dimensionality.
What is the curse of dimensionality?
Curse of dimensionality is that the model performs very poorly when we have a lot of features.
so that there is a curse when the dimension is high.
To see that, we're going to do some experiments.
I just plotted explained variance ratio from PCA.
By transforming our 180 features using principal component analysis, it's going to rank the combination of these 180 features in order of importance.
That is called explained variance ratio.
And this gradual increase of this explained variance ratio tells that a lot of these features are all important.
If only a few of these features were important, then this explained variance ratio graph would look like this.
Like very sharply increase up until point, that means more than 90% of variance would be explained by just only few features.
However, this graph shows that it gradually increases.
That means all features are kind of important.
So with that in mind, let's have a look and compare with the logistic regression.
So because most of these features are important, in logistic regression you can see that the test accuracy still increases as the number of features increases.
However, in KNN, as you can see, with the various values of K, for the various k values.
It has some peak value at very small dimension of features and then it sharply decrease the performance.
So that's the curse of high dimensionality.
So let's fix it.
Our optimal value k equals 7 and as you can see the test accuracy dropped very sharply as we increase the number of features that are included in the model.
So why does curse of dimensionality happen here?
It happens because intuitively the number of data points in the given volume of this height dimension sharply decreases when this dimension becomes high.
Therefore, we need more data points in order to have the same level of accuracy.
However, with the fixed data size, the concentration of data decreases dramatically.
Therefore, we have degradation of performance in accuracy when the dimension is too high.
But it's not that simple.
Researchers have found that if the features are highly correlated to each other, it may suffer less because the effective dimension is less than the number of features.
But anyway, still, K-NN suffers from curse of dimensionality.
So when this happens, you want to use smaller number of features and avoid from being high dimension.
when you are using KNN.
Also, not only the KNN, other machine learning models that use the distance metric in their algorithm can suffer from curse of dimensionality.
So you might choose wisely which model to use when your dimension is too high, unless you can or you want to reduce the number of features.
Alright, so in this video we talked about KNN as an example of non-parametric model, which is the simplest machine learning model and we talked about its property, its bias variance, its hyperparameter k, and how it behaves when the k increases or k decreases, and its properties such as curse of dimensionality.
We'll talk about more sophisticated models, non-parametric models, in the next videos.
All right.
Hey everyone.
In this video, we're going to talk about decision trees.
So, so far we talked about some examples of parametric models such as linear regression and logistic regression, which they have parameters or coefficients inside, and we used different metrics to optimize those.
And then we had the KNN for example of non-parametric model, which does not have parameter inside.
However, we use this metric.
to make a decision.
And decision tree is another non-parametric method which is a little bit more complex than KNN.
So let's have a look.
So what is a decision tree?
Let's take an example.
These are the photos of two different kinds of mushroom and one of them is edible and the other one is deadly poisonous.
So which one do you think is edible?
It is difficult to tell because they look very similar.
And in fact, the upper one is edible and the lower one is called a death cat.
The decision tree may look like this.
Let's say we have different samples of mushroom data and then from this first node it's asking some criteria whether it's large or not.
So let's say this one is large and then classify to large equals yes.
This one as well and these two are not large so we'll arrive to this node.
Let's say we call this node 2 and this node 3.
Alright from the node 2 there is another criteria or ask questions whether it's yellow or not.
So this one is not yellow so end up the edible and this one is yellow so therefore it's poisonous.
From the node 3 both of them are spotted so we'll go to node4 asking whether they have fall smell or not.
Let's say this one had a fall smell therefore it's poisonous and this one did not so therefore it is edible.
So decision tree works like this.
It splits the samples from each node depending on their criteria.
Let's talk about some terminology here.
So node at the top is called the root node and contains all the samples to begin with.
And then as the samples travel through these different node splits, when it arrives at the terminal nodes that doesn't split anymore, those nodes are called the leaf nodes and they are highlighted as green here.
And all other nodes between, they're called intermediate nodes and including root node, they also have decision criteria.
Therefore they are decision nodes.
So how the model learns to make a decision?
So as we mentioned, linear regression minimizes the MSE to learn to make a decision by optimizing their parameter values.
Same goes for logistic regression except that the criteria is now cross entropy.
KNN has no parameters, therefore no optimization, however uses a distance metric to make a decision.
Decision tree similarly does not have parameters, however uses other metrics such as MSA for regression task and entropy or Gini for classification task.
And they split nodes as we've seen before.
So decision tree regressor works like this.
So the goal is to split the samples into two boxes such that the MSA is minimized as a result of this split.
So let's say I have different options to split among these different features.
Let's say I have data that has two features only and six data points.
And then I want to split this into two boxes and I don't know how yet that will minimize the total sum of MSE.
So I have a choice of splitting along x1 or I have a choice of splitting along x2.
So another choice that we should make is that okay let's say I chose x2 to split then which value of x2 should I split?
Should I split here or here, here or here?
So these all decisions will be made by looking at the MSC of each split.
So again, we have different choices for making splits along X1.
For example, I can split this way and maybe left and right and measure the MSC and let's say this split criteria was A, then I can see if split by this split criteria and measure the MSC here and then sum them up and this total MSC I record it and then now I'm gonna move different split criteria so let's say this is called B then X1 is less than or equal to B And then I'm gonna measure this MSE for left and right boxes and then record the total MSE and I keep this procedure.
Let's say this one is C, D, E. Then I have five different split criteria along X1 feature.
Then I also record this MSE.
So that's for X1 and I can do the same for x2 so again it also have five different split criteria and then we can call it like big A, big B, C, D, something like that along the feature x2.
So as a result we have ten different values for MSC as a result of ten different split options and what we want to do is to inspect this MSC values and then pick the one that makes the minimize MSC.
So let's say this split criterion gave the smallest MSE among these 10 different choices, then now it becomes my split criterion for my root node.
So by root node, I mean the first box that we're given.
So this is my root node.
And then as we just saw, let's say this was the best split that minimized MSE, then now these two boxes becomes the split node.
So this node at the root node had six data points, and now we have...
split into two boxes left and right and each of them has three samples and the decision criteria at the root node was x1 less than equal to c. So if we keep doing this procedure, we're gonna reach to some terminal node or stopping criteria then the tree stops there.
So this is how the decision tree regressor works and the decision tree classifier works similar way.
except that it's not MSC but uses some other metric.
So we'll talk about that later.
Okay so let's have a look at the real data.
This data set is called faculty salary data set recording faculty salary at all the 90s and the task is to predict the assistant professor salary.
And it has four features and 50 samples.
But for simplicity to visualize I only use the two features and then depth equals two.
So depth in the tree means that how many levels that we go to grow the tree.
So this is depth equals zero at the root node, and this is steps one, and this is steps two.
If you don't specify the depth limit, the tree will grow until it has only one sample at the leaf node, or if there is another stopping criteria, it will stop there.
Anyway, this is the original data that had a salary mean value of 43,000, and then it had a 50...
samples.
So we have 50 samples and a value here at the root node.
And as we saw before, we will find all the split criteria.
That means that the decision tree will inspect all these split points along x0 and then along x1.
So it's going to split at the in the middle between the two samples.
So in the middle here, middle here, all the way to here and then it's gonna measure MSC's and then you will figure out which one to split.
So that's how it measures MSC here and then as a result it found that the splitting X1 54.95 at this point will make the MSC the lowest from the root node.
So depending on the answer to this criteria, we'll have these two And then each, from the each node, we'll also find another split criteria for next split.
And for example, this R1F box split criteria was splitting at feature x0 at the value 84.25.
And then it's gonna give this split, and then it leads to these two boxes.
Each of them have a mean value of 46,000 and 42,000.
Alright, if we further do that, the same procedure for this node will end up with this result.
So this was a simple example for how decision tree regressor works.
And in the next video, we'll talk about decision tree classifier.
Hello everyone.
We're going to talk about optimization in logistic regression to determine the values of the coefficients.
Alright so we're going to start by introducing a new concept called the maximum likelihood.
So what is the likelihood function?
Likelihood function is a product of all the probabilities that correspond to labels.
So for each sample the probability of classifying the label correctly And multiplying all these probabilities for each sample is called the likelihood function.
And by maximizing this likelihood, we can determine the coefficient values for the logit in the logistic regression.
By the way, this likelihood function is not only for the logistic regression, but it occurs again and again in machine learning, and it's a common theme.
This principle applies to all parametric models.
So if we maximize the likelihood, the parameters get determined.
And we're going to derive from this maximum likelihood.
This is especially for the binary class classification because we only delete the label equals 1 or 0.
So we're going to start by this likelihood function for binary class classification, and then we're going to derive the loss function from it.
So let's have a look.
So here is some example where we have y1 and we set it to 1 and y2 equals 1 and y true value for third example would be 0.
4 is 0 and y5 is 1.
And let's say we are using logistic regression and feed our features to get the predicted value I had but actually what logistic regression model produced at the output is the probability.
So this is a sigmoid function that we saw before.
So sigmoid function takes this form and this represents the probability of the label becomes 1.
So with that we can have a probability for sample number 1 and probability for sample number 2, probability for sample number 3 and so on.
And we'd like to construct the likelihood that means when the probability represent the probability of the label being 1 we're gonna have to flip some of them like this one so that it has the probability that represent the y equals 0 so to do that we're gonna change the sign and multiply all of the probabilities together and this quantity becomes a total probability of having all of these labels classified correctly.
So all y1, y2, etc.
are correct.
So this says that the correct probability that means we would like to maximize this probability so that our model can correctly classify all the examples.
So let's maximize this.
That's why this is called a maximum likelihood.
So likelihood function is the probability the total probability of classifying everything correctly.
So it takes this form and we would like to maximize.
And now we have some trouble because there are so many multiplications here.
This is not very easy to you know calculate so we're gonna take a log to the entire term so that we change this to the summation.
So summing up all the examples that has yi label 1.
Actually this has to be log pi and sum when the case is y equals 0.
And we can even make it more clean by having this one summation instead of two.
So instead of having when the case is 0 or 1, I'm gonna just set to everything has to be 1 and then I'm going to add yi here so it becomes the multiplication plus if I change the variable here to be 1 minus yi then when this is 1 this quantity becomes 0 so we can combine these two terms together so make it a little clean So this is our final form for the log likelihood.
So this is log likelihood and we want to maximize this quantity.
So actually maximizing log likelihood is the same as minimizing the loss function.
So we can define a loss function as just inverse of this.
So take a minus sign here and the same formula.
So that's our loss function and this is called binary cross entropy.
And this binary cross entropy is very common in binary class classification so we'll use this cross entropy loss function very often.
Alright so cross entropy again is a generalized form as this.
So these two are probability distribution and usually the one that's here means that the probability distribution that's kind of a label or true value So it can come from the data or you can come from the labels Where is this one?
The probability that goes into the log is predicted value So essentially we are measuring the difference between the true labels and the predicted probability So that's the meaning of the cross entropy and I omitted a category but if you have more than two categories, they have index for the category.
Alright so that's a cross entropy and for binary case where the category is only 0 and 1, we'll have this formula.
We derived from the maximum likelihood.
So searching parameters involves optimization.
So again the feature goes into model and the model has parameters.
the model will predict the value and with the target value, the loss function will compare this prediction and target and produce some error.
So if the error is bigger, then it's going to change the parameter value more.
And when we do this cycle multiple times, we're going to get the parameter values optimal value.
So that's optimization and this is parameter update procedure and we're going to use a gradient descent to update the parameters.
Alright, so let's talk about gradient descent.
So this error surface is actually from MSC loss, which is from linear regression, but the reason why I just draw here is that it's easier to draw than cross entropy.
So the loss looks like this, and let's say you're a skier and you're an advanced skier that you're not afraid to go to steep slope, and let's say you want to get to the base as soon as possible.
So what is your strategy here?
So you're gonna follow steepest slope, right?
So let's measure a gradient along the coefficient a, and let's measure the gradient along the coefficient b, and we're gonna update our weight, which is the parameter values for a and b, according to this gradient.
So that effectively makes this skier to go to this direction and follow the steepest slope.
Alright, so that's the intuition for gradient descent, and to do that mechanically, we'll have to make a derivative for loss function.
So loss function for MSC looks like this.
It has a residual squared.
and then the gradient along a coefficient is a partial derivative of loss function with respect to a and the function here is takes a form of f squared so it's like df squared of dx and it will be 2f times df dx so this is called a chain rule if you have a function that's a function of some other function which is a function of something else like this you can take a chain so making derivative of this, let's say this is x and again g is of another function of x.
Then what we will do is take a derivative f with respect to its argument which is g and then times dg, dg and then finally we can do dg dx.
So it looks complicated but actually when you look at it carefully it's not.
So when there is a multiple nested function you take the derivative conveniently as this and take the chain rule.
So using that if you take the chain rule here, it's 2f so there is a 2 here and therefore this 2 is gone and this is f and this is the fd w which is df dA.
So there is x here and for derivative of loss function with respect to b coefficient.
Then also takes a f here and then df db which is 1.
So there is nothing here.
So this is the formula for gradient descent for MSA loss function and weight update rule says the weight is updated such that it's the old value of the weight minus some constant alpha times the gradient of the loss function with respect to that weight.
This is called the learning rate by the way.
And the bigger the value, the bigger the step size.
So if the learning rate is big, then the step is bigger.
And be careful if it's too big, then it can pass the solution like this.
And if the learning rate is very small, we're gonna take a small step toward the goal like this.
So if the learning rate is too small, it's going to take a lot of steps and longer time.
So usually when you do the gradient descent optimization, you will have to choose this learning rate.
So therefore, this learning rate is a hyperparameter.
Hyperparameter means that some kind of...
parameter that you will have to, the user will have to choose.
So learning rate is one of them.
We don't have to worry about it for the logistic regression because the logistic regression uses another form of gradient descent.
Actually that uses second derivatives rather than first derivative like in the gradient descent and that is called the Newton method.
We'll talk about it very soon.
Alright so gradient descent for binary cross entropy laws.
Let's calculate this.
So BC law looks like this.
derived and then this is a sigmoid function and we're gonna take a derivative of loss function with respect to w and g is a function of w and x plus b so d loss d w is going to be d loss d sigma because loss is a function of sigma and then it's going to be d sigma d g because Sigma is a function of G and then DG DW okay so one at a time this value is going to be I'm gonna remove this for now because it's clean that way okay Y divided by Sigma so this is Sigma by the way this is Sigma derivative of log x is 1 over x so minus sign is from here and then minus 1 so that's my d log d sigma and then d sigma dg is sigma times 1 sigma times 1 minus sigma I'm not gonna show here but you can prove this easily and this is very good formula that sigma-weight function is very convenient as the derivative is itself times 1 minus itself and that's why it's used in the many gradient descent application.
Alright so we're gonna use that and then dG dW is simply X.
So combining all this together we're gonna get dL d w is going to be minus times sigma times 1 minus sigma and x.
Similarly, we can do the derivative for the bias.
And it will take the same thing except this part.
So there is just 1 here.
So that's the gradient along a coefficient w and bias B.
And we're gonna apply the same principle to update our weights.
So this could be either w or bias.
They're the coefficients and then dLows times the learning rate.
Okay so that was it.
Alright, so Newton's method, it's an extension to gradient descent method.
So gradient descent method only used the first derivative of loss function and its update rule was this.
Gradient with respect to w of the loss function.
Whereas Newton's method will use both the first and second derivative.
So first derivative here and then second derivative here.
And by the way this term is called a Hessian.
So in a matrix form it will look like a Hessian inverse and the gradient matrix times alpha minus omega like that.
And then the reason why Newton's method can be good is when we have a very flat gradient it can be very slowly converging.
However if there's a Hessian that's dividing this small gradient then Hessian is also small, then this can boost the speed of the convergence when the gradient is very small.
So the Newton's method converges faster, that means it requires a less number of iterations given the same learning rate for the gradient and the Newton's method.
However, it has a drawback.
So the memory that requires for one iteration, the Newton's method scales as n squared, whereas Newton.
Where is the gradient method when it takes O ?
And for the time complexity, Newton's method per iteration is going to be n cubed, whereas gradient method is n. So this is more expensive per iteration.
It's great that it can require less iteration, but given the same number of iterations, gradient descent requires less memory and less time.
This n is the number of parameters.
So if you have a lot of parameters like in the neural network, neural network typically have millions or billions of parameters, Newton's method or similarly second derivative method will be very slow.
So usually the neural network optimization utilize a gradient descent method rather than the second derivative method.
But in logistic regression and other parametric models in machine learning, where the number of parameters are smaller, we don't have to worry about that.
So that's why I.S.K.
Lohn and other similar packages using Newton's method or similar method to optimize the parameters.
Alright so let's have a look at some simulation here.
So this is a gradient descent and this is a Newton method and then they start from the same place and then you're gonna see shortly that they shoot very fast to the bottom and then it will go toward the goal.
Compared to the gradient descent method which goes very slowly when the gradient is small at the bottom, Newton's method is faster.
And that's because again, because this small gradient is divided by small h so it gains more boost when the gradient is flat here.
So that's some kind of intuition and then that's it for this video.
So in next video, we'll talk about performance matrix.
Hi everyone.
We're going to talk about introduction to logistic regression.
So brief review of machine learning problems.
So in machine learning, we have supervised learning with labels and unsupervised learning, which doesn't have label and reinforcement learning with feedback signals.
And we're going to focus on supervised learning.
And largely, it has two tasks.
regression and classification.
And in classification, binary class and multi-class classification we're going to talk about.
And previously we talked about linear regression can do the regression task.
And logistic regression we're going to talk about in this video.
Although its name says regression, it's actually for classification.
Especially it's useful for binary class classification.
There are some ways to do the multi-class classification with the logistic regression method.
But it's going to require some engineering to do that and other models that we're going to talk about it later and some of them will not talk about it in this course.
They can do different things.
So for example, support vector machine can do both regression and classification.
But similar to logistic regression, it's usually good for binary class rather than multi-class.
But it can work on multi-class.
If you engineer the label and some algorithms inside of the model correctly.
And then decision trees can do everything.
So you can do regression and binary class, multi-class without any problem.
Also, it's nice that you can take categorical variable very efficiently.
Neural network, same thing, can do everything and many other models that we may not introduce in this course can do different things.
So with this high-level, High-level introduction, let's dive in and let's talk about what is the binary class classification.
It is essentially yes or no problem.
So the label is binary.
So for example, credit card default, whether this customer that uses a credit card will likely to default or not given like some historic data.
And maybe there is insurance claims and some insurance claim can be fraudulent.
So that can be a binary class classification.
Spam filtering, given this email text, is this spam or not?
Medical diagnosis, given this patient information and lab tests and data, is this person have a disease or not?
Survivor prediction, given this patient's information and history and things like that, whether this patient will survive for next five years or not.
How about customer retention?
Is this customer behavior?
is likely to charm or not, so that marketing action can be taken.
Image recognition, various kinds, can also be binary class classification.
For example, is this animal dog or cat?
And sentiment analysis, given this text or Twitter sentences, what is the sentiment?
Is this negative or positive?
Things like that.
So as you can see, binary class classification can have a variety of different types of data input.
It could be tabulated data, it could be image, it could be text, it could be even speeches.
So that determines the binary class or not is actually entirely for the label instead of the data itself or the features itself.
So a brief example, we can talk about some...
breast cancer diagnosis problem.
So this is one of the features that can determine whether this tumor is malignant or not.
So it can be a binary classification problem.
And we want to have some kind of threshold or some decision value that above this value, maybe we are more sure that this is going to be malignant.
And maybe below this certain value, maybe it's less likely to be malignant.
So building a logistic regression model will help us to find this threshold value, which is called the decision boundary, by the way.
And if you have more than one feature, let's say we have two features, it can be shown as a 2D diagram like here, and our decision boundary will be likely to be a line instead of a threshold value.
So maybe this side is malignant and this side is likely to be benign.
Alright, so logistic function provides some convenient way to construct model like this.
So logistic function look like this.
It's between 0 and 1 and it smoothly connect the line between 0 and 1.
And there is a sharp transition around the certain threshold value.
Let's say this is 0, but it could be any other value.
And this represent, because it's between 0 and 1.
logistic function can be a probability function.
And actually the logistic function has another name called the sigmoid.
So this is also called a sigmoid function and the form takes this one.
So the G is the linear combination of the features with this weight and bias like we did in the linear regression.
And then this G goes through a nonlinear function 1 over 1 plus e to the minus G.
and then this entire function as a function of z is called a sigmoid and takes the shape of this curve here.
By the way, this g is called logit and this is a relative decision boundary.
So when it's set to zero, that means this is our threshold value and the probability here is going to be this one and this g is zero then it's 1 half, so it's going to meet the 0.5.
So with the 0.5 threshold, so above 0.5, we can say this is going to be malignant, and below 0.5 probability, we can say it's going to be benign.
Well, some people might ask why don't we use linear regression instead, and maybe we can fit it like here.
We can fit this and then maybe find some threshold, and it can also fit the probability of 0.5.
We can try to do that.
It's not easy.
First of all, we will have to find out where this threshold is and maybe we can just fit the line first and then just figure out which value will give 0.5 threshold.
But if we do that, it gives a different threshold value to the logistic regression.
So the one problem with the linear regression model if it fitted and then find the threshold where the probability value becomes 0.5, is that it's not very interpretable.
Where is the logistic regression with the sigmoid function?
It is a well-defined probability function, so it's very interpretable that we can find where probability becomes 0.5 and this gives the right threshold for us.
So let's talk about decision boundary more.
So in univariate case, where we have only one feature.
The decision boundary is a point where it meets the probability equals 0.5 so the equation looks like this and you can get the value out of it.
If we have two features the data will lie in the two-dimensional space and then the decision boundary becomes a line so we can find the line equation here which will draw this line.
If it's a multivariate have a multidimension more than 3, the decision boundary will be a hyperplane.
Okay, let's talk about what if we have multiple categories.
So instead of having yes or no problem, maybe we can have multiple categories, which is maybe we would like to predict whether this animal is cat or dog or maybe cow.
So for the logistic regression, the logit, which is a decision boundary, takes this form.
And then for softmax, which is multinomial, this has another name, multinomial logistic regression, has this form.
So they are very similar except that there is now an index for K category.
So this is index for category.
So for example for category number one, we can construct this model.
So there will be different weights assigned to each category and for each feature.
And now we did this logit.
So for logistic regression, we use the sigmoid function as a probability and we show this form but it can be rewritten as this form as well.
and this is very similar to softmax.
The softmax function takes the same form as this one, except that it now has an index for the category.
And then instead of this, now it has all the summation over all the possible exponents of these corresponding categories.
Alright, so softmax is called multinomial logistic regression.
However, there is another similar way that we can use the original logistic regression for multi-categories.
So maybe category A, B, C. We can construct such that it is a binary class classification for A versus not A, which we will have to combine these two cases.
So this is maybe logistic regression model 1, and this is logistic regression model 2.
We're going to do B versus not B, and then we're going to construct third model that says C versus not C.
And this approach is called one versus the rest, an OVR problem.
So there are, you know, different ways to get the multi-category classification done.
So one is, like we mentioned, we use a multinomial approach which is a softmax.
And another way to do is using OVR.
So you can find a Sklearn library that uses utilize these two, but I think Softmax or Multinomial is more common and you will see later other classification models such as SVM and decision trees, they have a preferred way of being Multinomial versus logistic or maybe some model is more convenient to use one versus the other, so we'll talk about that later.
By the way, both OVR and Softmax, their probabilities for categories they sum to one, so for example probability for A plus probability for B plus probability for being C category for the sample number 1, they sum to 1.
So that's the same for logistic and the softmax regression.
However, there could be some problem where maybe there are A, B, C category and we don't necessarily need to pick one of them, but maybe the category doesn't exist at all.
So neither cat nor dog nor cow but something else, then this should be 0, 0, 0, then In that case, it's called multi-label problem.
I know it sounds strange because label and categories, what's the difference?
But this type of problem where we don't necessarily have to pick one of them in the categories are called multi-label problem versus if we have to pick one of the the categories, then it's a multi-class problem.
And both the logistic and softmax models, they are for multi-class classification.
And then there can be some other ways to treat the multi-label problem, but we can still use the same models, but we will have to construct the labels differently and construct the training process a little differently.
So that's a little bit of difference, but you will see more often the multi-class classification problems than multi-level problems, but just keep in mind that they exist.
Alright?
But anyway, Softmax regression can give this kind of visualization.
So let's say we had only two features in the dataset and the data will lie in the 2D plane and this is going to be the decision boundary that Softmax will give us.
You can see more examples here.
Alright, and this ends our video.
And then in the next video, we're going to talk about how optimization works in logistic regression and how the coefficients are determined.
Hello everyone.
In this video, we're going to talk about performance metrics in classification.
So here is the example for binary class classification where the label is 1 when the tumor is malignant and the label is 0 when the tumor is not malignant.
So we created a logistic regression model based on this feature and let's talk about some terminology here.
So this region where both the labels and the predictions are positive is called a true positive.
And this region where the labels and the predictions are both negative, we call true negative.
And this region is called false positive because the prediction says it's positive, but actually the label says it's negative.
On the other hand, this region is called false negative because the prediction says negative, whereas the label says it's positive.
So we would like to build a model that maximizes the number of true positives and true negatives, whereas we want to minimize false positives and false negatives.
This false negative and false positive have different names as well.
And false positive is called type 1 error, whereas false negative is called type 2 error.
If those terminology confuses you, then you can remember this funny picture.
Type 1 error is like telling a man that he is pregnant.
Whereas type II error is like telling a pregnant woman that she is not pregnant.
So both are bad and sometimes they are in trade-off.
So depending on the situation and what are important to us in the problem, we'll have to consider one more seriously than the other.
So we talked about true positive, true negative, and then false positive and false negative cases.
A formal way to express that in a table is called confusion matrix.
So confusion matrix is like this.
There is a prediction label.
There is a target label.
and they are in the table that this is true positive and this is true negative and this is false positive this is false negative and sometimes depending on the notation they may be the row and the column may be exchanged and this can be calculated by the scalar matrix confusion matrix module and when you use a confusion matrix module it will give the labels as row and then prediction as column, but they don't display this so Sometimes it's confusing but you can figure out by looking at the data Okay, so let's say we have all the numbers that we collected from this confusion matrix and now let's calculate some performance metrics So the most popular one in classification is accuracy Accuracy is number of correct answers divided by all the data points So it's a measure of how many are accurate out of all the data points.
And true positive rate, in other words, recall or sensitivity, is a measure of how many are truly positive out of all the positive cases in the data.
So this is all the positives.
And this is true positive in the data.
Okay, and another metric...
True negative rate, which is similar to true positive rate except that they are kind of flipped.
Another name for it is specificity or selectivity.
Measures how many are true negative out of all the negative cases in the data.
So it's negative cases in the data.
By data, I mean the labels.
Another good measure that we often use is a positive predictive value.
Or in other words, precision.
measures how many are correctly classified as true positive out of prediction from the prediction.
And false positive rate, in other words, fallout rate tells us how many are false positive out of all the negative cases.
So how many were falsely classified as positive when it was actually negative.
So it's although negatives from the data and how many of them are falsely classified as positive.
So actually as you can see it is 1 minus right here and similarly false negative rate, in other words miss rate is also how many of positive in the data are falsely classified as negative and this is actually 1-tPr.
So they are related to each other.
F1 score is a good metric because oftentimes there is a trade-off between recall and precision.
So recall and precision and in some cases recall is a good metric but we want to see both of them together so in the case we want to use F1 score because it has both of the precision and recall inside.
So F1 score is usually robust metric so it's good to use.
All right so not only the F1 score here are also good metrics that are robust in you know different kinds of situations.
One of them is called ROC curve which is a receiver operating characteristic curve and it shows like this.
So in the x-axis it has a false positive rate and its y-axis it has a true positive rate.
And this red dotted line represents the random guess.
So if the curve goes this way, closer to this left top corner, this means it's good.
We have small false positive rate and large true positive rate, so that's good.
However, if the curve is below this random guess, then it's bad.
That means we have a high false positive rate and small true positive rate.
So this side is good and this side is bad.
So that's ROC curve and this is a graphic way to tell.
However, if you want to see a number then we can use area under the curve AUC.
So we measure the area under the curve.
So for example this curve the example the area under the curve will be this value.
So usually between 0 and 1.
the bigger the value, it's better.
So that was ROC and AUC.
Okay so when to use these different kinds of metric?
So we have a number of choices, accuracy, sensitivity, specificity, precision, fallout rate, miss rate, F1 score, AUC, and confusion metrics.
So AUC, ROC.
In general rule of thumb when the data is when the label is very imbalanced accuracy might be really bad so for example if your data is 99.9% negative and maybe 0.1% positive and maybe your model says 100% negative then it's going to give a fantastic accuracy 99.9% 0.9% correct if it says everything is negative.
So that's not good.
So accuracy may have some pitfall there.
So usually it's a good idea to use accuracy when you have a balanced data.
And recall, which is true positive divided by all the positive cases in the data.
They are used mostly when we want to capture as many positive cases as possible.
So even though we kind of sacrifice false positives.
So when it's good for, so for example cancer detection, cancer detection by missing someone having cancer, if we miss the cancer of a patient then the patient is at risk.
So there is a high cost associated with missing.
So in that case we want to use recall because we want to capture as as much of post cases as possible.
And in that case also we want to look at false negative rate.
If you have too much false negatives in the data, then we are in trouble.
So we want to look at both of them at the same time.
If we have a high cost of missing something.
And on the other hand, the false post rate or false alarm rate can be used when the cost of false alarm is high.
So for example, spam mail.
So having spam mail is fine.
It's just annoying.
However, if you have a false alarm, that means it's going to erase the important mail.
then it's problematic.
So we want to avoid these false alarms, then we want to look at the false positive rate.
Which is also similar to specificity or sensitivity, so we want to look at it as well.
Precision is used when we want to be very sure about the action.
So for example, when we identify scammers in PayPal or Venmo, and we'd like to inactivate their account, then we want to be very sure because otherwise we can just delete innocent user's account and it'll make the customer unhappy.
All right, so in summary, some performance metrics can be considered more important than the other depending on the situation and what's important in your problem.
However, these performance metrics are robust and can be used in almost any cases.
All right, so let's talk about cross entropy as a performance metric.
So, why do we want to use cross entropy and not accuracy?
Because accuracy, although it doesn't work very well in imbalanced data, if the data is balanced, it's pretty good.
And it's intuitive and interpretable.
But why do we want to use cross-entropy?
In a nutshell, cross-entropy can use more granular information about how the prediction is more confident, whereas accuracy only says whether it's correct or not.
Why is that?
Let's have a look.
So, for example, this case.
This is model A.
Model A's have accuracy of 2 3rd because it's correct two times and incorrect one time for these three samples.
And when you see, although it's correct here, the confidence is not that great.
Whereas the incorrect ones, the confidence is too confident for the incorrect answer.
So we can say this model A is not very good model.
Maybe we can compare to model B, which has the same accuracy to third.
However, when it's correct, it's pretty confident for the correct answers.
And when it's not correct, maybe it's not sure.
So maybe it makes sense.
So in this case, if we used cross entropy, it can discern these two different cases.
So it will give a better score, which is a lower cross entropy value.
for the better model and higher cross entropy value for the less working model.
So that's some intuition behind why you might want cross entropy and not accuracy, although accuracy might be more intuitive.
Okay, hello everyone.
In this video, we're going to talk about sklearn library usage for logistic regression.
Okay, so logistic regression module is inside of sklearn.linear model.
And it has a bunch of options here.
And interestingly, it already has regularization terms.
And they actually depend on the type of the solar.
So by default, solar is LBFGS.
And then in that case, by default, it uses L2 regularization.
and it already does the fit intercept equals true which is better to have in linear model and you don't have to worry about a lot of things here but you might want to change class weight equals balanced if you have an imbalance labels then it will automatically weight your class labels so you have a better a slightly better performance and in case you have a more than binary class it's going to automatically apply some multi-class and usually most of time it will apply the softmax which is multinomial And then, so there are several solver types.
Usually you don't have to worry about it, but if you want to try out different solvers, you can try.
All of them uses some sophisticated second derivative or similar method.
So for njobs, if you have a multiple core CPU, then you can utilize it so you can have less computation time with the parallelization.
If you do the njobs equals minus one, it's going to use all the CPU cores in your computer.
Alright, that was how the module looks like and then let's have a look at how to use it.
So basic usage is like this.
So you can just call the module and you can throw in your preferred options and then you can do the .fit and inside of this fit function, you're gonna throw your data.
So it's features for the training and this y is the labels for the training.
And you can call this object as model or some other name.
And from this model object, after this fitting has been done, it has some number of useful stuff inside.
So for example, model.coef underscore will give us the coefficient values for all the features in this feature matrix.
And intercept is a separate, so you will have to do model.intercept underscore, then it's going to give the value for intercept.
The model.predict parentheses and throw your data.
such as test data or train data or any data that you want to get the prediction out then it's going to give the binarized prediction so Y prediction another good comment is predict proba and you can throw in your data features then it's going to produce the probability so row output from sigmoid you can produce you can use that and plot this kind of graph or you can inspect the probability.
Alright, so let's further talk about some example.
So in this example, I'm going to split my original data x and y into train chunk and then test chunk, and that's done by this very popular, very popular function called train test split.
It's inside of sklearn model selection, and be careful of these detailed names.
because sometimes they may upgrade and they may change the names as they change the directory of their you know sub libraries and things like that.
But I think for now it's valid.
So we'll use that.
So we'll call the LogisticRegressionModule and you can name it differently if it's too long.
So I named it as a LR and then I'm gonna throw my preferred options.
There's no reason why I chose it but I just chose it.
for example.
And then I'm gonna fit my data, x train and y train, and if your y train label matrix shape is not correct, it's going to complain.
Use label.
You can use that in that case.
And I'm gonna call my object clf this time, and then if you want to get an accuracy, so this score uses accuracy by default, you can do fitted model.score and throw your test data and it's going to give some result.
So for this example the result was accuracy of 0.96 which is pretty good.
If you get rid of this option it might be slightly lower but yeah that's your choice.
We can use other kinds of metric and they are all in this sklearn.metrics module.
So for example I can predict Yp and then most of them requires Ytrue and then Yprediction.
So I'm gonna throw in there.
So accuracy score function requires Ytrue and then Yprediction.
So I throw that order.
Recall score as well and precision and F1 score.
As a result, it gives these numbers.
Alright.
And then I can do the confusion matrix using confusion matrix function.
And again it needs Ytrue value and Yprediction.
prediction value and it also requires labels.
As we mentioned in the previous lecture, this is going to be y prediction and this is going to be the label.
Okay so more examples.
We can also draw precision recall curve.
So previously we talked about ROC curves and precision recall curve is similar.
So ROC curve had true positive rate versus false positive rate.
precision recall curve is precision versus recall and has this shape.
So ROC curve it was better when it's close to the left top corner and precision recall curve is better when it's close to the right top corner because we want to high precision and high recall as well.
So using precision recall curve function it also requires true value and prediction probability actually rather than label, binarized label.
So I'm using this predict proba and then the column one actually gives the probability of being label being one so I'm gonna use that and I can just further draw this curve.
So ROC curve also works as a similar.
So I use ROC curve and then it's going to output FPR, TPR, and the threshold that was used to calculate these kind of spots and then the result looks like this and I've included the random guess and AUC score as well.
The AUC score can be also automatically calculated using this function, out of series score.
Again, it needs a true label and then the prediction probability.
So it's very handy.
So we talked about some barosymmetrics and how to get the coefficient values from VTID model.
But how about statistics?
Unfortunately, the logistic regression module in sklearn doesn't give statistics right away.
So we have two choices.
One is using the stats model library as we did before as in linear regression.
So instead of linear regression, we can use dot logit module and then throw our data.
So be careful that their order of feature and label is different here.
So they take the label first and then the features.
and then similarly we can do the dot fit here and it's gonna give summary table.
So different from linear regression that gave a lot of other metric like r squared or just r squared and many other metrics such as f statistics but here they don't have that perhaps because we don't need that in the nonlinear case.
However it does give the coefficient value and then the standard error for that and then g test instead of t test but they are kind of the same.
So here we can see that this p value is very small so this coefficient value is significant.
Another way to do it using sklearn library is bootstrapping.
So bootstrapping is like this.
As a reminder, this is the original sample and then we can resample it multiple times like this.
We can resample with the replacement so you might see some duplicate data.
samples and then we can fit the model so logistic model here here separately and then get the coefficient values and we can do the statistics conveniently there is a module exist called the bagging classifier which is essentially a wrapper so this is class inside of skl and angsangbo module and then takes the base estimator so it can be any estimator so any kinds of model not only the logistic regression but you can do linear regression or you can do tree models and others.
You can throw in here and then number of estimators means that how many times we will bootstrap and then fit the model.
And it says the bootstrap is true so it's going to use bootstrapping.
It can do the bootstrap features.
That means it can also select the features randomly but we don't need that here.
Ob score means the out of bag so it can set aside some of the bootstrap samples and then it can use it as a validation.
purposes.
And jobs we can do also a minus one then it will utilize all the computing resources that we have.
Alright so using that we can use as this.
So bagging classifier is a wrapper and inside the wrapper we're gonna throw our base estimator which is a logistic regression model and in the logistic regression model I'm gonna use weight equals balanced.
because it will give a slightly better result and then number of estimators is a thousand and then for this wrapper I'm gonna do the dot fit and throw my data here and as a result I call this object resulting object as a CLF and then I can pull some useful things from it so for example dot estimators underscore will give all the fitted model objects inside of list and since I asked for number of estimators equals thousand, it's going to have a thousand models inside.
And then I can pull some of the model's coefficients.
So for example my first model coefficient values are like this.
So I use the two features here.
So it's going to give two coefficient values and then one intercept value.
And I can pull all of this from this list.
and then do the statistics.
So I draw histogram first to see how they look like and because n is reasonably big they look like normal distributions skewed sometimes but roughly they have some mean and some width.
Alright so what do I do with this all thousand values for each coefficients?
I can do the t-test.
So there is a convenient Python package here scipy stats.
ttest one sample.
We are doing ttest for the p values.
So the usage is like this.
So I put the list of coefficients.
So I'm gonna put one kind of coefficient at a time and then it can be in a for loop by the way and then this value is the mean that it wants to compare with.
So for the hypothesis testing, the null hypothesis says that my coefficient value is zero.
Therefore, I can put 0 here.
The alternative says that my coefficient is not 0.
And to test that, we're gonna pull out the p-value and if p-value is smaller than certain threshold, I'm gonna choose 5% error.
That means if p-value is smaller than 0.025, because t-test or g-test, they have two wings.
So if p-value is smaller than this value.
That means my coefficient value is significant, right?
So the result I can pull out and print.
It has two components inside.
The t-statistic value and then the p-value.
And I can pull each of them by just doing dot and their name.
So I just pulled t-statistic value for example, but you can pull the p-value as well.
Furthermore you can console this documentation.
So as you can see, All coefficients are very significant.
Few hundreds t values away from the zero and p values are all zeros.
So all of them are significant.
And in this video we talked about how to use logistic regression module from the sklearn and then we talked about how to use the various metrics from sklearn metrics module.
We talked about also how to do the bootstrapping using the bootstrapping wrapper.
Hey everyone, in this video we're going to talk about gradient boosting.
So previously we talked about generic boosting algorithm that we iteratively add a stamp model to our initial model and each stamp model fits the data to predict the residual from each stage.
And with the shrinkage parameter we add this stamp model iteratively.
and also the residual gets smaller and smaller as we go through this iteration.
And then as an output, we're going to have the combined model.
Gradient boosting is a generalization of this boosting algorithm.
Instead of fitting the residual, which is y minus fx at each stage, we're going to use gradient of a loss function.
So if you remember loss function is some generalization form of measuring some error.
So we're going to measure an error by having a data x and y and our prediction yp, which is essentially the fx.
So this can be MSE or RSS in the regression.
So for example, something like this.
Or some other function if it's classification.
So this loss function can be very general form.
And this can be a measure of error, but loss function is more generalized form.
So by measuring the gradient of loss function with respect to our change of model at each iteration, we can measure the gradient of the loss function.
And the goal is to fit our tree to predict the negative gradient minus g instead of just pure residual.
So that's a little bit difference from our previous model.
And everything else is the same.
So we're going to see more in detail here.
So we start by fitting our initial model to minimize the loss function.
This is something similar to minimizing entropy or minimizing MAC loss for regression in decision tree.
So we're going to have some split.
And then for each iteration, we're going to calculate the negative gradient, which is again gradient of loss function with respect to the change of this function.
And with this...
gradient value, we're gonna fit the stump tree to this training data to predict this negative gradient value.
And this will give some set of parameters while it's fitting and then we will update our loss function using this updated parameter values.
And also we're gonna update the function and as we go this iteration, we're gonna have this additive model as a result.
So let's talk about why we want to use a gradient instead of just a residual.
So if we use just generic boosting algorithm, which is a greedy algorithm, which will look into all the possible split of a stump or small tree, and then it will pick one that gave the best split.
That means it will choose the parameters such that the reduction in residual is the biggest.
So, measuring gradient of this multi-dimensional space is very similar to this greedy approach, but it's even better because it's going to choose the direction that's the steepest descent.
So steepest descent in terms of reducing the loss function.
And when you think about classification problem where we chose some different function like entropies or Gini in decision tree classifier instead of whether it's right or wrong, which is residual in classifier, that is more true to how the decision tree split happens.
So having loss function is more expressive in that way.
Okay, so I'm trying to convince you that the gradient boosting should in theory work better than four stepwise or generic boosting algorithm.
So let's have some comparison.
So I prepared two data.
each of which are very similar to each other.
So they are they have a small number of features, 13 features versus 20 features and they have approximately 5,000 or more samples.
The data one is a little bit difficult so having fully grown decision tree will give about 61% accuracy whereas data 2 it's a little easier.
So that decision tree fully grown will give performance of 89% accuracy.
So even though number of features and the number of samples are similar, sometimes depending on how one or more features are a good predictor of the target variable, things can be different.
But as you can imagine, the gradient boosting is much better than decision tree already.
But how about AdaBoost?
But how about comparing to AdaBoost?
So if we compare to AdaBoost, the data one on the data one gives similar result.
Both of AdaBoost and Gradient Boosting gave much better results than just the Decision Tree.
In Data 2, much better results than Decision Tree alone.
However, you can see the Gradient Boosting works slightly better than AdaBoost.
So the conclusion is that whether the Gradient Boosting is always better than AdaBoost, it depends on the data.
But most of time, it is likely to be better performing than AdaBoost.
Also, gradient boosting is less sensitive to mislabeled data.
So for example, AdaBoost is sensitive to mislabeled data because it uses a weight to each data samples.
Therefore, if the label is wrong, it's likely to suffer.
However, gradient boosting doesn't have that problem.
How about some other aspects?
So these graphs were generated at a different learning rate.
As you know from previous video, any boosting algorithm can deteriorate if learning rate is too high and number of trees are too many.
So in order to prevent overfitting, when we have a large number of trees in additive model like boosting algorithm, we need to reduce the learning rate.
So this graph shows that, and then you can see that both the boost and gradient boosting, they require smaller learning rate as the number of trees increases.
This one is time, so I ran a five-fold cross-validation for each model.
In this case, gradient boosting was time-efficient than AdaBoost, but just empirically speaking, it depends on the data.
And also, you have to keep in mind that AdaBoost uses a stump, which means the max steps equals 1, whereas a gradient boosting SK-1 library, they by default use max steps equals 3.
Now let's talk about performance comparison with the random forest even.
So the data one, which was a difficult case, we saw that the random forest didn't do much better than decision tree and boosting algorithms were much better than random forest.
Whereas this little bit easier data with the data two, all of the ensemble algorithm did better, much better than just a decision tree.
So can you say random forest which is a parallel ensemble algorithm versus boosting algorithm.
Which one would be better?
It is difficult to tell when we have such small number of features because when the random forest really shines is when the number of features are a lot.
So I prepared the data3 which has 145 features, which is a lot more features than previous data and has 3000 samples.
And single-disk entry performance is almost 70%, which is kind of medium difficulty.
Then ran three different ensemble models.
As you can see, Random Forest did better than boosting algorithm.
Now you have some sense of when to use which algorithm.
When you have a lot of features, Random Forest will work better.
When you have a smaller number of features, usually the gradient boosting will do better.
And as I mentioned before, it all depends on data too, but in general, that's the trend.
We can also think about the time.
So as you can see, the boosting algorithm takes much longer time than random forest and it's not surprising because we have a lot of number of features.
All the gradient boosting algorithms, they inspect all the features, whereas the random forest will take only subset of features and by default is square root.
So about 12 features they will only look at and the other boosting algorithm they will look at all 145 features here.
So there's an interesting feature in gradient boosting in sklearn library that it can take an option called max features so you can actually set it to random sample the features whereas other boost algorithm doesn't have this option so It will consider all the number of features, whereas gradient boosting, if you set to do something similar to random foresting, it will run faster, so you can save some time at the expense of a slight performance drop.
However, I think if you have a lot of features, it can be worthwhile.
So let me just mention briefly other useful packages.
XGBoost is an external library, so it's not part of sklearn.
However, it's a separate library that can be useful.
XGBoost is a acronym for Extreme Gradient Boost and nothing very different from gradient boosting, but they implement some other tricks such as regularization and random sampling of the data and random sampling of features like random forest do.
So, XWBoost is time efficient, also provides a good performance because of built-in regularization.
Light GBM is another external package that's not part of sklon and it makes the boosting faster by binning the value of each feature.
So if the feature has some continuous values a lot instead of looking into all these chopped values, it can bin larger size like this so it can split faster so that way it can be useful.
sklon also has a counterpart to this one so I think you can get similar results from sklon library ExtraTree is similar to RandomForest.
It's also part of sklearn library.
ExtraTree means extreme randomized tree.
It works very similarly to RandomForest in sklearn.
And the only difference is that it doesn't do bagging.
So no bagging.
But it still randomly sampled the features.
And also why it's extreme randomized?
Because it picks split value randomly instead of doing the best split.
Here is a full list of ensemble models in SKLearn libraries.
So we talked about AdaBoost and they have both classification and regression and bagging classifier would be random for something like random forest without random sampling on features.
So it has just a bagging part and extractory classifier it's the opposite.
So it does not have bagging but it random samples on the features.
Gradient boosting, we talked about it, and random forest we also mentioned.
There are some other more complicated stuff and this heat gradient boosting would be something equivalent to light GBM.
As a recap, we talked about ensemble method in this module.
Ensemble methods are ways to strengthen the decision tree model.
Decision tree model is a weak learner.
So it can overfeed and overall its performance isn't very good.
However, by taking parallel ensemble or serial ensemble, we can make the performance better.
So parallel ensemble, we talked about random forest.
So random forest is a parallel method, ensemble method, and we also talked about boosting method, which is a serial.
ang-sang-bulling method.
So this is just growing different trees, randomized.
They look different because we random sample data and features and then we just average them, right?
And in boosting, we use a smaller tree like stump and try to fit to the residual and then we additively add these small models to create a stronger model.
And we also talked about when to use this random forest versus boosting.
So random forest usually works better when there is a large number of features.
So number of features is large.
Whereas boosting can take longer because it's additive.
So we prefer using when the number of features are smaller.
However, it can also take advantage of random subsampling of features by using the max features option.
Ok, so this is the end of Triangul models.
We'll talk about Conner method in the next module.
Hello everyone.
In this video, we're going to talk about support vector machine.
So let's review briefly.
So in machine learning, we have different learning tasks.
So in this class, we focus on supervised learning.
That means given the data, we would like to predict the labels.
And this prediction task have two different categories such as regression and classification.
Regression means that the prediction value would be real valued, whereas classification, the prediction value would be the categories.
And we talked about binary class classification and multi-class classification.
And according to these different tasks, there are different models that we can apply.
So for example, linear regression applies to regression problems.
And logistic regression, although the name says regression, it is for binary class classification.
And we talked about we can generalize logistic regression using Softmax, and then we can do the multi-class classification.
or we can apply a logistic regression model to do the multi-class classification if we choose one class versus the other ones.
And then we moved on to non-parametric models such as a k-nearly neighbor and decision trees.
k-nearly neighbor doesn't have a parameter unlike linear regression or logistic regression, and it is one of the most simplest models in machine learning, and it can do both regression and classification.
Decision trees are weak learners, but it's very flexible and it's easy to interpret.
It can also do regression and classification.
And also we talked about the angsangbul method, which can apply to any model.
However, it is most beneficial for decision trees because decision trees are weak learners and by angsangbuling them, they can be a strong learner.
So for example, we talked about parallel angsangbul method, which is random forest, which we grow the trees in a decorrelated way and then average them.
Another method that we talked about was serial ensembleing method, which is a boosting method.
So instead of growing the full tree, we let them grow very slowly and small one at a time.
So we talked about adding a stump, which has one or just a few decision splits, and then we additively added them with some learning rate.
The rest of the class will talk about SVM, which is another powerful non-parametric model.
And there are some other supervised learning models that can perform well, such as a neural network.
However, we won't have a time to go deeply into neural network in this course, so we'll skip that.
Let's briefly talk about hyper parameters and what's the criteria.
So a little bit in depth.
So linear regression, there was no hyper parameters, but we need to design in the feature space how many features we want to include, how many high order terms that we want to include.
That is domain of more feature engineering, but it can be a design consideration.
And linear regression has parameters.
So, you know, w1 x1 plus w2 x2 plus intercept.
That could be, so all these w's are parameters.
Loss function for linear regression, we talked about MSC loss.
similarly RSS.
Those are loss functions that we use.
Logistic regression is very similar to linear regression except that it has a sigmoid function that threshold the probability at the end.
So there is no hyper parameter and again there is a design consideration such as how many features that we want to include and how many higher order terms that we want to include.
And parameters they are the same.
We have the same form of this and then there is a threshold, sigmoid threshold at the end, but these are the parameters and it's very much same as linear regression.
For loss function in logistic regression, it uses a binary cross entropy.
And in KNN, the K is the hyper parameter.
K means the number of neighbors that we want to consider when we decide whether a point around some other points are certain class.
Alright, and there is no parameter because KNN is a non-parametric model and there's no loss function because there is no optimization going on.
However, there is a some kind of rule how to decide.
So when there are neighbors like this, then this point here would be having more neighbors around this with this X class.
So it will classify this X.
So, in KNN, to determine which neighbors are close by, it uses a distance metric such as Euclidean distance.
So, KNN doesn't have loss function, therefore no optimization, however, it uses a distance metric in order to make a decision.
And decision trees is again non-parametric models.
So, there is no parameters, therefore there is no optimization.
However, decision trees have hyperparameters such as max-depths and What's the minimum samples in the terminal node?
Things like that.
And as optionally, if you were to do some pruning, there was something called the CCP alpha, which is set the threshold of pruning criteria.
So there was no parameter for decision trees because it doesn't have explicit optimization process.
However, it requires some criteria for splitting.
So if you remember, when the samples are in one box, when split, the decision tree models go through all these features and pick the split value of that feature which that minimize this criteria function.
So this criteria function was something like Gini index and entropy for classification MSC or RSS for regression tests.
And then we also talked about ang-sang-buling method that derives from this decision trees.
So ang-sang-buling method, they all share similar hyperparameters as decision trees.
And on top of that, they can have different, they have additional hyperparameters such as number of trees because it's going to ang-sang-bul, you know, several number of trees.
Or for boosting, it can have also learning rate.
And again, there is no parameters for this ang-sang-buling method.
And the criteria function, decision split criteria, they have the same criteria functions as decision trees.
In SVM that we're going to talk about, there is one hyperparameter called the C parameter, which we'll talk about what the role of this C parameter is.
And there is no parameter because SVM is also a non-parametric method.
However, SVM has an internally have some optimization process.
And neural networks, although we're not going to talk about deeply here, they have both parameters and hyperparameters and loss functions as well.
Alright, so let's talk about the supervector machine.
So here are some few facts about the supervector machine.
It uses a hyperplane to make a decision boundary.
We'll talk about it more later in this lecture.
And uses a kernel which is a function that applies on feature space.
And especially it's useful when we deal with the high dimensional.
feature space such as images or text.
So for example, instead of doing feature engineering on image pixels, we can apply some functions such as finding similarity between some pixel patches and then that way we can save some computation.
Because of that, support vector machine was widely used and developed during the 90s before the neural network became very popular.
It uses some mathematical kernel tricks to deal with the high dimensional data such as images.
And it is one of the high performing off-the-shelf machine learning method.
So all of the three ensemble methods, support vector machine and neural network, they are popular high performing method.
Support vector machines can do regression and classification and especially it works natively on binary class classification.
However, we can also use one versus the other method to do the multi-class classification.
Well, so let's talk about binary class classification.
It is essentially a yes or no problem.
So for example, it could be some problem like whether this credit card user will pay the debt or not, or this insurance claim is fraudulent or not, or maybe this email is spam or not.
And it can be medical diagnosis problem, whether this patient has certain disease or not, whether the patient will survive or not, whether this customer will continue for the service or not.
And as you know already, the binary class classification can take any data format as long as the label is yes or no.
So for example, image recognition can be binary class classification whether the object in the driving scene is a pedestrian or not, something like that.
Also, we can also do binary class classification on text data such as sentiment analysis.
And previously, we talked about logistic regression as a simplest model to do the binary class classification.
And as you know, this curve is a representation of a probability which is actually sigmoid function as a function of G. So this is a G and G is called logit and described by this linear combination of feature x with the weight and bias like in the linear regression.
And when G is 0, the probability of the sigmoid function becomes 0.5.
Therefore, it becomes a decision boundary.
And previously we talked about this decision boundary can be a threshold point when it's only one-dimensional feature space or it can be a line like this when it's a two-dimensional feature space and it can be a plane in the three-dimensional space or hyperplane when it's a multi-dimensional space.
So, now you know what the hyperplane is.
Now, the question is how do we find this hyperplane that becomes a decision boundary using SVM?
And we would like to find the hyperplane that separates the data points according to the right class like this.
But depending on how the data points are distributed, there could be more than one way to separate those data points.
So for example, this can be a perfect choice, but also this can be a good choice.
And this hyperplane can also separate the data perfectly.
So the question is which hyperplane should we choose?
And we're going to introduce a classifier called the maximum margin classifier and sometimes it is just called hard margin SVM.
So one thing that we can consider is that we want to train our model such that it can generalize better.
That means if we have another new data point like this, our model should be able to classify that correctly.
In other words, we would like to have a hyperplane.
that's less likely to misclassify the new data.
And how can you achieve that?
We can select the hyperplane that has the biggest margin.
So let's see what that means.
So here is the data again.
And let's say this is the hyperplane.
And these points are closest to the hyperplane.
And those are called support.
And the distance between the hyperplane to those support closes the points.
I'll call it margins.
These are margins.
The maximum margin classifier learns how to maximize the distance between the hyperplane and its supports.
Let's talk about how the maximum margin classifier finds a hyperplane.
Initially, because it doesn't know the right hyperplane, it's going to look like this.
It randomly chose a hyperplane which makes this pointer the wrong side of the margin.
When data points are wrong side of margin, it will make the loss function bigger.
And the optimizer in the SVM will try to reduce this error.
So it will adjust the coefficients of the hyperplane equation.
So now the hyperplane looks like this.
We still find the data points that are wrong side of the margin, but it is a smaller error compared to the previous one.
So smaller loss function.
And again the optimizer will try to reduce the error.
and updates its hyperplane and that look like this.
So when we go this iteration over and over again, finally the hyperplane will be optimized such that the margin between the supports are maximized.
Alright, here is a short quiz.
What happens to the separating hyperplane if we add a new data point?
The answer is that it depends where the data points get added.
So for example, if the new data point like this are added outside of the margin, it will not do anything about the hyperplane.
However, if the data points are added inside the margin or even the wrong side of the margin, the hyperplane must change.
Let's say we have new data points like this and obviously it's the wrong side of the margin.
The blue points should be upper to the hyperplane.
However, this new data point is the wrong side below the hyperplane.
So we need to relax the condition of having hard margin.
And therefore, another method called the soft margin classifier can be useful in this case.
So we'll talk about that in the next video.
Hello everyone.
In this video, we're going to talk about AdaBoost algorithm.
So previously we talked about generic boosting algorithm, which iteratively fit the stump tree to the data to predict the residual.
And then this each stump from each iteration is added together with some shrink parameter lambda here.
And this lambda helped the model to learn slowly so that we can avoid overfitting.
There are many variants of boosting algorithms.
However, these two are most used and most popular.
So we'll talk about those.
So first algorithm we'll talk about is called AdaBoost.
AdaBoost is originally developed for classification.
However, later it was developed to also do regression as well.
What makes AdaBoost interesting is that it uses weights to data samples.
That means it will make some more emphasis on the misclassified samples so that you can learn more from these errors.
and this dump fits to y instead of residual and then because it's a classification it gives a discrete values but instead of 0 or 1 we're gonna use minus 1 or 1 and then it uses exponential weight to update the data sample weights alright so Adabo's algorithm we want to have a classifier that gives a minus 1 or 1 and this This model is a linear combination of this stump model and B is the iteration.
And a little difference from the genetic boosting algorithm, this lambda B now in AdaBoost is not the shrinkage parameter, but it's kind of representing this model importance from each iteration.
So this algorithm starts by initializing all the sample weights to 1 over n, which means that all the data points are equally important.
And then we're going to repeat for B times.
that we fit the stump tree to the training data to predict the label instead of residual with the sample weight w. If you remember, the stump model is actually decision tree.
Decision tree can use a sample weight when calculating the split criteria.
So after fitting this stump model, using this stump model, and here is that, and then we compare how much accurate it is.
So this i is an identity function which will give 0 when it's correctly classified and which will give 1 when it's misclassified.
So this means that we calculate the error only using misclassified examples.
And the first iteration, these weights are all equal.
However, it's going to be updated as we go.
And using this error, we're going to calculate the model coefficient, lambda b, which again tells how much we should include this stump model into the total model.
and this lambda is given by this formula.
And sometimes you're gonna see one half in front of this formula, which is also popular convention, but with or without, it's fine.
And using this model coefficient, we're going to update the sample weight, and this sample again, when there was a misclassification, the weight of that sample becomes larger by this exponential factor.
And after we do the iteration for b times, we finally get our output model that looks like this.
The linear combination of this stump model.
And then the final sign is given by that.
So here is a brief example with the picture.
So initialize sample weights W. So in this data, these are the features and this is the target Y and this is the initial weight.
And then for this iteration, we're going to fit the stump model to training data with some sample weights.
And it's going to give some kind of output like this.
And then we notice that these two samples are misclassified.
Therefore, when we calculate the error, it's going to give a 0.2, so 2 misclassifications out of 10 examples.
And then we further calculate the model coefficients and it gives this value.
By the way, this function can go from minus infinity to infinity.
So this parameter doesn't have to be somewhere between 0 and 1, unlike the shrinkage parameter.
And then using this model coefficients, we're going to update the weight using this exponential factor that gives this weight.
So this misclassified example receives more weight, four times more than the others, and this one as well receives a bigger weight.
And then we can normalize this weight so that these all examples, some of these weights becomes one.
Alright, so let's have a look at some usage.
So AdaBoost is available in sklearn ensemble module.
So AdaBoost and sklearn both have a classifier and regressor.
So classifier has these kind of options and base estimator is not specified and it's a decision tree classifier with the maximum depth equals one, that means it's a stump.
You can also see this learning rate on top of this lambda b which was the weight to the model.
There is also learning rate as a hyperparameter so you can reduce the learning rate if you want to make the Adaboost classifier run slowly.
And by default, the semi.r algorithm is used, which is a real Adaboost.
This r comes from real Adaboost.
They make a use of predict probability.
the probability of being each class instead of using just a binarized classifier.
So SEMI-R is advanced version of original AdaBoost algorithm SEMI.
And it is good for multi-class classifier, but it also works better for the binary class classification.
So you can just leave it as is and use it.
Here are some more resources on how this real AdaBoost algorithm, which is SEMI-R, is a little bit better than real AdaBoost algorithm.
the original discrete AdaBoost algorithm.
And again, this boosting algorithm gives much better performance than just one stump as well as the fully grown decision tree.
So being AdaBoost originally developed for the classification problem, can AdaBoost also do regression?
The answer is yes.
You just need to call the AdaBoost regressor in sklearn-angsangbul module.
And everything is very similar.
It also accepts a learning rate.
And the only difference in the regressor is that we can specify the loss function, which is by default is a linear loss.
Okay, let's talk about how good is the AdaBoost.
So I picked two different datasets, each of which have about 5000 samples and then 20 features.
And as you can see, depending on the problem difficulty, the absolute accuracy can be different.
However, regardless of its difficulty, the boosting algorithm is always better than fully grown decision trees.
So this is decision tree fully grown and this is boosting algorithm.
So this left graph being more difficult case, so overall accuracy isn't too good, but this right one is a little easier data, so they had a higher accuracy.
As you can see here, boosting algorithm can have overfitting as well if the running rate is too big and number of trees are too big as well.
So there is a trade-off between the running rate and the number of trees.
Alright, so that's it for this Adaboost video, and we're going to talk about gradient boost in the next video.
Hello everyone, in this video we're going to talk about ensemble method second part, boosting.
Previously we talked about the trees have a problem that they are weak learner and they can overfit very easily.
So the first idea we used to address this issue was let's try to ensemble them by introducing diversity.
which were trained on different subsets of data, we can have diversified trees and then hopefully the averaging this diversified tree will give better performance than just picking one single tree.
On top of that, we also add an idea that we can further de-correlate the trees, so make sure we actually pick the trees that are different from each other so that we can have a true diversification.
So the random forest used that idea.
How it did that?
So not only training the trees in the different subset of data, we also, when we sample the data, we also random sample the features and that was implemented in the random forest.
So again, bagging in random forest, bagging random samples of data, which means the row in the table and random forest also.
random sample zone features.
Therefore, it can further de-correlate the trees and both of bagging classifier and random forest.
They are parallel ang-sang-bulling method, which means the training of each tree on different subsets of data, they can be trained at the same time.
We also showed that the performance increased dramatically by ang-sang-bulling trees.
So, here is the single tree performance and here is the bagging classifier alone.
I can make this a huge difference and then on top of that if we further decorate the tree we can gain another performance increase.
Okay so we're going to introduce a second ensemble method which is a sequential ensemble whereas previously we talked about parallel ensemble.
So this sequential ensemble is called boosting.
So boosting also solves the same problem that trees are weak learner and trees overfit but instead of diversifying and averaging those different many trees, we're gonna make single tree a stronger runner.
And how do we do that?
We're gonna grow a small stump at a time to fit the error from the previous stage and then we're gonna grow another tree in another stage in the next stage to fit the error from the previous stage.
So you can think about this analogy.
When we have a big problem like this, maybe the first scientist will look at it and quickly solve the problem by this much.
leave this problem more, we need a more serious investigation, and the second scientist will ignore all this, but only look at this part, focus on this part, and then solve maybe this much.
And then the rest, the third scientist or investigator will look at it and then solve more problems and then reduce the gap of this error gradually.
So we can do the same with the small tree.
Instead of growing large tree that try to solve this big problem all at once, we're gonna have some small tree that will solve some part of this problem and then leave this error.
And the second small tree will only look at this error and try to solve it.
Then reduce the gap of error.
And third one will even further reduce the gap of the error.
So this process is called boosting.
And boosting just means that we will make one single tree to a strong learner or performs better or will boost the performance by growing the tree slowly.
link or two at a time.
So a single decision tree is grown to the maximum depth.
So it's large and try to solve the problem all at once.
Whereas boosting tree grow very simple and very little one or two depths at a time.
And the rest of the error will be fit to another small tree in the second stage and then the rest of error will be fit by another small tree.
in the third stage and we continue that and our final model will be some of these small trees.
Okay so let's have a look at algorithm.
So we're gonna initialize our model to zero that means our model doesn't know anything about our data and let's say our error is as big as the label and then we're gonna iterate for b times.
Then we'll try to fit a stump in the stage b to train data so the data x and then the label is now the residual.
In the first iteration this residual is the same as y so we try to fit the y first.
And then in the first stage since this was 0, we're gonna have our model called our stump times some constant.
This constant is less than 1.
That means we will add the stump model to our whole model by certain fraction.
And the reason why is that we want to consider our new model kind of conservatively rather than adding all of them together.
Okay?
And this helps our learning slow.
So it's something similar to learning rate.
We're gonna update the residual in the current stage that our residual is also from the previous residual minus the prediction, the shrinked prediction from our current stump model.
And then after we repeat the times, the final output model will be the sum of this shrinked stump models.
All right, so graphically it looks like this.
So here's the data and then it feeds to our first stump model and then the stump model will predict the prediction and in first stage it will be compared against the label and then its difference that we define as here, we will get the residual from the first stage and from the second stage we build another stump model and try to fit the data.
It will predict the residual predicted and we're gonna also take this residual from the first stage as a label and then we'll get the error to produce the residual from the second stage.
And as you can guess, I will continue that with the third stump model to the data and then I'll try to predict this R2.
and we're gonna have another residual from the third stage and so on.
So we just showed the generic boosting algorithm which iteratively fit the small model to residuals from the previous stage and then we add up all these small models with some shrinkage to have the final model.
Okay, so that boosting algorithm was a generic form and we're gonna introduce two different boosting algorithms that are most popular.
For example, AdaBoost uses exponential loss instead of just residual, and then AdaBoost also uses different weighting to the data points.
And it can achieve better performance by weighting more to the data points, data samples that were previously misclassified.
Another popular method is called GradientBoost, and GradientBoost method tries to fit the gradient of residual instead of residual itself.
So we're going to talk about this method in the next videos.
Hey everyone, in this video we're going to talk about how to prune trees.
So last time we talked about some ways to prevent overfitting in decision trees.
Decision trees are very easy to overfit, so to mitigate we talked about all the stopping last time.
So we talked about number of hyper parameters that can be used for stopping growing trees early.
So for example we can set the maximum depth of the tree.
So after that certain depth, the tree stops growing.
And another example was set the minimum sample leaves.
That means we set some threshold such that the number of samples need to be in the node in order to split further.
Another strategy was the information gain.
So we look at the information gain and if the gain is not enough by splitting the node, then we stop splitting there.
So this strategy can be effective for preventing overfitting, but it doesn't guarantee that the performance of the tree will be better.
So the issue is that we can have some good split after the tree stops growing, or maybe we locally look at some node and stop splitting from that node because maybe we saw the information gain wasn't enough.
However, further split can have some huge reduction in impurities, for example.
So we never know what's going to happen after a certain point.
So another idea that we can try is maybe we can let the tree grow fully and then prune back because it's a hind site we can make sure the prune tree is good enough both in performance and overfitting.
Alright so how are we gonna do that?
We're gonna use a algorithm called the minimal cost complexity pruning and this feature is implemented since two versions ago in the SQLon library.
So here is a big tree.
We grow the tree fully and we will call it T0.
And then a certain point, maybe pick this point that this is node T and the impurity can be measured as RT.
So impurity can be you know, gene index or entropy for classification task, but it could be something else like RSS or mean-scaled error if it's a regression.
So RT really means that some error measure of the node before the splitting.
And then we can add some additional penalty alpha t which term is a measure of complexity by splitting further.
So this alpha t is a measure of complexity.
It's proportional to a complexity parameter and also proportional to the number of terminal nodes from that node t. So we define a sub-tree.
everything below this node T, and we count the number of terminal nodes, in this case 3, and the bigger the subtree, that means we penalize more, that means we add more term into our error term.
So effectively the error term is larger when we add this penalization term, or regularization term.
And you can check more details in these documents.
Okay, so we talked about that this alpha is complexity parameter, and this size T is number of leaf nodes or terminal nodes in the sub-tree.
And this is again gray area is a sub-tree from that node T. So let's say this is node T and these are the leaf nodes.
And as you might guess, the impurity at the node T before the split is larger than the impurity of the sub-tree.
Otherwise, it won't split, right?
So this is generally larger than the impurity of the sub-tree.
So how do we calculate the impurity of subtree?
It's just a sum of all the impurities in the leaf node of that subtree.
Alright?
So, so far these were the pure impurities at the node T and the subtree.
The sum of the impurities into the leaf nodes.
Then now let's think about what happens if we add this complexity term or regularization term.
So each case we can add this regularization term.
So for node T, we can say the effective error at the node T.
is its plane impurity plus the complexity term, but remember it was before the split, so our complexity term, the number of terminal node is just one here, so we're gonna just add alpha here.
And let's think about the subtree hole itself.
So subtree, the effective error of the subtree is going to be the impurity of the subtree, which again is a sum of all the impurities at the terminal nodes.
plus the complexity parameter alpha times the complexity of the tree, of that subtree, which is number of leaf nodes, in this case, three.
So that is the effective error of that subtree.
And at certain point, if we pick the alpha carefully here, then we may be able to set these two numbers to be equal.
So error of this node before split and error of the entire subtree below that tree, below that node.
So the alpha that makes this possible is called alpha effective.
So this alpha effective is actually a number that will set the threshold when we can split further.
And if we do the algebra using this formula, then we get this formula.
Okay so great.
So we can define a threshold at the node T that tells whether we should split or not.
Okay, so with that, how do we do the pruning?
So we can calculate all alpha effective for intermediate nodes.
So alpha effective here, here, and every intermediate node except terminal node will have its own alpha effective and their numbers can be different.
And we have that list of that alpha effective for all the intermediate nodes and then we pick the one.
that's smallest and then remove it.
And we can iteratively remove the smallest alpha effective.
So for example, if this node had the smallest alpha effective among this all other intermediate nodes, then we can remove this node as well as its subtree, like that.
And let's say this one was the next, then we get rid of that, get rid of this.
and we repeat until we meet some criteria.
So when do we stop the pruning?
We set some threshold called alpha CCP or CCP alpha such that we stop pruning when all of the alpha effectives are bigger than this number.
That means the link strength is strong enough that we don't need to prune anymore.
And again this threshold value is called a CCP alpha in the SQL library.
So again, this alpha effective is a measure of strength of that link.
If the alpha effective is bigger, that means the split at that node was worth, so we don't prune that link.
If the alpha effective is smaller than certain threshold, that means it was not worth splitting, so we just prune that branch.
Hello everyone.
In this video, we're going to continue to talk about the support factor machine.
Last time, we talked about maximum margin classifier, another name hard margin classifier, which has a hyperplane such that the margins or the distance between the support and the hyperplane will be maximized.
So these points closest to the hyperplane are called support.
And these dashed lines or the planes that are parallel to the hard margin hyperplane are called the margins.
And the goal is to make these margins as big as possible.
Having bigger margin means that we have more safety or confidence in terms of classification.
Last time, we also mentioned that the maximum margin classifier uses internal optimization to find this hyperplane.
Before we go further, let's derive some math formula.
that can be useful for describing this optimization technique.
So here's the hyperplane.
Here is one support point that's above this hyperplane and we'd like to measure this distance, shortest distance between this point and the hyperplane.
To do that, we're going to choose an arbitrary point on the hyperplane and let's say this vector to the support point is called xA and this vector to the point that's on this hyperplane is called xb.
And now this vector will be xa minus xb.
And then we'd like to calculate the distance between this support point to the hyperplane.
To do that, we just draw a line between this projection point and this point b.
All right and this will be 90 degree and now let's say this is the vector that's normal to this hyperplane.
So we will call it n, which is normal vector, and then the angle between these two vectors, let's call it s. So between this s vector and normal vector n will be called zeta.
And this distance is d. So we would like to calculate the d which will be s scalar value times the cosine zeta, which is the same as the S vector, dot product, the unit vector n. And just an example, in the three dimension, the S vector would have three components, S1, S2, S3 for example.
If it was in p-high dimensional space, you would have a p component, S1 to Sp.
Similarly, the unit vector will also have three components in the three dimension.
So W1, W2, W3 for example.
And because it's a unit vector, we require that the length of this unit vector is 1.
so that means this would have to be 1 in three dimension.
So for D, it's going to be S1 W1 plus S2 W2 plus S3 W3 for this three-dimensional example, and we can also rewrite XA1 W1 XA2 W2.
XA 3W 3 minus XB 1 W 1 minus XB 2 W 2 minus XB 3 W 3.
And because this point B was kind of arbitrary, we don't care what that point was, but we do care about this one.
So let's just simplify XA is actually X.
then we can do x1w1 plus x2w2 plus x3w3 and we can call this guy, the rest, to be just some simple constant.
Let's say b.
So this formula is for the distance d and again if this point a was the support, then this distance between the support and the hyperplane becomes the margin.
And now let's think about how to take care of a point that's below the hyperplane.
So when it's below the hyperplane like this, let's say it's called A prime.
As you know, this cosine value will be negative.
So this quantity becomes negative when the support vector is below the hyperplane.
To take care of that case, we're going to assign a variable.
Let's say y for the point A is going to be plus 1 value.
When it's above the hyperplane.
and it's minus 1 when it's below the hyperplane.
So that gives an idea of how to combine this together and we can use this Y and this formula to make a mass expression for the optimization condition.
Something like YI.
So the I means the index for the data point.
So YI times XI, first component, the coefficient for the first component, XI2W2 all the way to xipwp for the p-dimensional hyperplane and plus b for the constant.
And this quantity needs to be greater than equal to the margin m. So this inequality equation sets the condition that the optimization needs to satisfy for all the data points.
Alright, so let's talk about how should that formula that we just derived has to change.
when you have inseparable data.
So when we have inseparable data, what we need to do is that we need to just relax the condition.
Instead of having hard margin, that will require that all the points has to be above and below these margins.
Instead, we accept some errors by softening the margin.
And this is called soft margin classifier or in other words, a support vector classifier.
So let's have a look what does the soft margin mean.
So this is the hard margin that we showed before.
We had the coefficients to each component of the vector x and then a constant.
So sum of this times the y which is plus 1 when it's above the hyperplane and minus 1 below the hyperplane.
So this value should be greater than equal to m. That was hard margin classifier.
When we say we relaxed the condition, we introduce a new variable called slack variable.
So this one.
Which helps to give some wiggle room for this m.
So, in addition to this, we have to satisfy this condition as previously.
And then, this slack variable is always positive value.
And also, we have to satisfy this condition.
So sum of this slack variable need to less than or equal to a value called c. And this c represents the budget for the error.
In other words, if c is large, then we can tolerate more errors.
And also c is a hyperparameter, so the user get to choose how much of error budget we have.
Alright, so let's talk about some definitions here.
So this is a hyperplane, as you know, and these are the margins.
And if you look at carefully, blue texts are on this side and red texts are mostly on this side.
So when the data points are above the margin, this is a safe margin.
Above the safe margin, then this is correctly classified.
So these are correctly classified.
And when the data points are on the margin itself, just saying we can just say it's on the margin.
So this is also on the margin.
How about these data?
So blue points here, red point here, they are in the wrong side of the hyperplane.
So these are wrong side.
of the hyperplane.
What about these?
These are still on the correct side of the hyperplane, but it's the wrong side of the margin.
So, there are two margins.
However, we only care this margin when it comes to blue data points and we'll care about this margin when it comes to red data points.
So blue data points that are just below the margin but above the hyperplane are called wrong side of the margin.
And this one, although it seems like on the margin because it's not sitting on its correct margin or the safe margin to the blue data point, so it's still on the wrong side of the hyperplane.
Time for this.
So, that's some kind of definitions.
And with that, let's see what happens to the slack values for all these different situations.
And remember, this is the condition for the hyperplane.
And all of this slack variable needs to be positive value, either equal or greater than zero.
When the data points are on the correct side of the margin, which means these points, these, the slack variable values for those points are zero, so it doesn't do anything on this equation so it doesn't change and satisfy the hard margin requirement.
And when it's a wrong side of margin, so anything below this margin for the blue points and anything below this but above the hyperplane for red ones, these slack variable will have some value between 0, something greater than 0 and something less than equal to 1.
If it is sitting right on the hyperplane, which is very rare, it's going to have the slack variable equals 1.
What about wrong side of hyperplane?
If it's wrong side of hyperplane, the slack variable will be larger than 1.
So this value becomes negative.
Therefore, we want to avoid that situation.
Alright, so again, the role of the C parameters.
C is the error budget that bounds the total number of errors as well as the severity of the violations.
And as we mentioned before, C is also a hyperparameter, then we need to pick the budget of the error.
So with that in mind, we're going to address three questions.
So first one would be what is the maximum number of supports in the wrong side of the hyperplane when the C is given?
And secondly, we're going to also answer what happens to the margin M when C changes whether increases or decreases.
And what does that mean in terms of bias and variance?
So the first question, what is the maximum number of supports on the wrong side of the hyperplane given the C?
So when you remember this formula, you can think that every slack variable needs to be positive value and this sum of the slack variables need to be smaller than equal to the C parameter.
That means the maximum number of adders can be C if all of the select variables are equal to 1.
Alright, so second question, what happens to the margin when C decreases?
So which one of these will have smallest C?
The answer is this one.
The smaller the C, we have smaller tolerance for the adder.
Therefore, the margins gets tighter.
So what happens to the margin when C decreases?
The answer is the margin becomes narrower.
Alright, the next question.
What happens to the bias and variance when C is small?
So small C means a tighter margin.
That means we have a less tolerance to the error.
And less tolerance to the error means that we will get a more accurate model.
That means less bias.
So bias decreases but we will have instead a higher variance.
So, as a recap, we talked about hard margin classifier which has a hyperplane that separates the support which are this closest point to the hyperplane as much as possible.
There are these overlaps.
So these are called support again and the distance between this hyperplane and these supports are called the margin.
And we derived this formula that expresses the condition that all the points need to be satisfied for the hard margin classifier.
And we also talked about some general cases where the data points are not perfectly separable.
We need to introduce a slack variable that will make this condition a little bit softer, which allows some of the data points can be wrong side of the hyperplane or wrong side of the margin.
And we also talked about C parameter, which is a hyperparameter that we set, which value acts as a budget for the total error.
So far we talked about linearly separable data, that means our hyperplane was not curved.
It was kind of straight, multidimensional.
plane, hyperplane, and we show that it's a plane can be described by this linear formula.
However, in some cases like this, there is no way to separate this data with just one hyperplane.
And for that, we'll need some other ways to separate the data like this.
In that case, we will have to use some more general form of kernel.
So we'll talk about kernel method in the next video.
Hello everyone.
In this video, we're going to talk about support vector machine with the kernel tricks.
So just a brief recap, we talked about hard margin classifier, which goal is to maximize its margin, which is the distance between the hyperplane and the support vectors.
And then, in case we had inseparable data like this, we simply added the slack variable epsilon to all this.
data points and then this epsilon just specify how much they deviates from the margin.
So like this for red points and then this amount for different blue points here.
And these slack variables need to satisfy two conditions such as it has to be no negative value and then we also define the c parameter which gives an idea how much of error budget we have.
Also we mentioned that Sometimes the data can be not possible to use one hyperplane to separate the data.
So in that case, we need to use some special trick called the kernel trick, which will be the subject of this video.
Before we go on what the kernels are, let's think about this.
So in support vector classifier, which is another name for submargin classifier, we mentioned that we have to satisfy all these conditions.
And this part is the formula for the hyperplane.
Let's call it f .
And then this beta zero and beta one and all the way to the beta p are the coefficients for this equation.
And the optimizer will find the values for these coefficients.
Now we can ask ourselves, why do we call SVM as a non-parametric method when we do see these parameters in the equation?
18.
That's very much related to the use of Connors.
You might notice that I use the term SVC, which is a Supervector Classifier, versus SVM, Supervector Machine.
It's not very important, but Supervector Machine generally refers to some generalization of Supervector Classifier, whereas Supervector Classifier usually refers to the Soft Margin Classifier.
In SKLUN, they use a different algorithm.
SVC uses a lip linear.
It's very much similar to the optimization algorithm that we use in logistic regression.
Whereas this SVM uses a libSVM algorithm which is specially made for SVM and this algorithm uses the kernels.
Alright, so let's talk about what the kernels are.
So this is again hard margin classifier and this is soft margin classifier and this is the formula for the hyperplane.
We're going to introduce a different math formula which is equivalent to this formula f .
However, we'll skip the derivation and just show the result.
So using the inner product, it is known that this formula f can be rewritten to this formula.
And this is a dot product.
So if you have xi', then this is dot product between the point i' and point i.
And this dot product represents the linear corner.
Oftentimes, we will call it as K kernel, xi and xi'.
That product again is a linear kernel.
So essentially, this is same as this one.
However, when we implement the algorithm, it will have a different time complexity.
So for example, the SVC that uses a linear library will have time complexity of number of data point times number of features.
And if we use the libSVM and solve for linear data, then it's going to take more time.
It's going to have SVM with linear corner.
It's going to take n squared times p. So by using kernel, it doesn't seem it's useful for the linear data.
However, the kernel method shines when it comes to complex data.
So let's have a look.
When we have this type of data that's not possible to separate by linear hyperplane, what we want to do is this.
So let's say a simple example, we have a data that's not linearly separable.
So in the one-dimensional, the hyperplane will be just a point.
So we need two hyperplanes in order to separate it perfectly.
However, it's not possible.
So the trick is, We can add one dimension here and then now we can separate this perfectly with this one hyperplane So adding one more dimension is a key and it's called the kernel trick.
So again, this data is not separable In 2D using linear hyperplane So what we do is we add the third dimension.
So this is a G by the way, and this is Maybe we can call it X and Y So we're gonna introduce G and maybe X here and Y here And now we can see that this data is separable with the hyperplane like this.
Adding one more dimension means that we want to make a higher order terms in the function.
Okay, so we have a p number of features in the data and then we can add a higher order terms in order to make extra dimensions to separate the data points, which was previously not separable in the linear fashion.
So we can add higher order terms like this, but then, naively, what happens is that Now our optimization need to find all these parameter values for the higher order terms Which might be a lie if you add even more higher order terms And as well as if you have a large number of features It's going to be a problem like we saw in the polynomial regression So instead of adding directly higher order terms we can use a kernel trick instead So let's make a use of this inner product We can create a function kernel function k that has this form this is the dot product and then represent a first order terms.
And by having a constant plus this dot product to the order of d, we can create the polynomial function for the high order terms.
And then we can generalize our function to be a form that has these corners.
So let's have a look when we have this type of data that might involve a nonlinear decision boundary.
We can use polynomial kernel that we just saw.
By having polynomial kernel, we can have this type of decision boundary.
Shows the data result when we had the d equals 2 for polynomial kernels.
Which nicely separates these blue points and the red points by adding another dimension to the data.
There are other types of kernels and another very famous one is called the radial kernel or sometimes called the radial basis functional kernel or RBF for short.
And it takes this form, this kind of Gaussian shape kernel.
defines the RBF corner and the result is like this.
So it's like a round shape.
Basis corner will be able to separate this data into three blobs.
So having corner is great.
You can solve some complex data.
However, we need to think ahead what kind of kernels that we should use.
So when it's a linear separable, you can see that we don't need any fancy kernels.
Just a linear kernel or linear SVM or SVC that does not use a kernel at all will solve perfectly.
Whereas RBF corner is fancy corner so the radial basis corner can also solve the problem depending on how the data look like.
So this data was generated by a blob data and linear is approvable.
So it was both linear SVM and RBF-SVM worked well.
You know different types of data like this, this shape like yin and yang or moon shape in sklon, they look like this type of data usually.
The linear SVM doesn't work very well.
However, the radial basis corner did well on this.
Other corners did not do well for this type of data.
How about this circular donut shape of data?
Linear SVM did not do very well as you can expect.
But radial kernel is perfect for this type of data because the data shape is radial.
So now you can see that The choice of kernel strongly depends on the pattern of the data.
So although the kernel is very convenient for this nonlinear data, it requires the user to think about what the data looks like and guess what the best kernel would be.
Hello everyone, in this video we're gonna talk about ensemble method and especially random forest.
So what is an ensemble?
If you hear ensemble, you might imagine this kind of image.
So individual instrument players can make some sound in the music.
However, the sound characteristic and spectrum can be limited by one instrument alone.
But if you have a collection of these different types of instruments, you can make very rich and flavorable musical sound.
And that's the ensemble.
So this kind of analogy can apply to machine learning model.
So for example, decision tree can be a weak learner.
However, if they are aggregated in certain ways, they can be much better.
So here is an intuition why the collection of machine learning model can be better.
Well, let's say we have some problem to solve in the general public community.
If you sample people that has the same race, same gender, same age group, and same kind of background, it's likely to have only represent those kind of people.
The other hand, if you have a diverse people that has a different gender, different age group, and race, and background, it's likely to have more representative of the different groups and then therefore we are likely to make a better decision.
Okay, so diversity is great.
Then how can we make our models diverse?
One idea might be maybe we can train our models on different subsets of data.
So training model on different random subset of data is called bagging.
You can think about like putting different data set into bag and then make the model trained on this bag of the data.
Well, but actually the name is not because they put the data into bags, but...
The full name is a bootstrap aggregation.
So bootstrap aggregation is like this.
You have different ways to random sample the data.
So first step would be a randomly sample subset of training data with the replacement.
So we can use the replacement.
That means we can sample the same data that we already sampled.
So let's say you can sample this yellow sections out of this whole data.
and you can also have this overlaps like this.
So that's a bootstrap process.
And then we can grow a tree on this selected data.
So tree number 1 and tree number 2, tree number 3, tree number 4, etc.
And in general, we don't let this tree prune because they may become similar to each other.
However, it is also possible in practice that we can grow prune the tree and then ensemble them.
Alright, and then after growing this tree to each different subset of data, we can ensemble them.
So ensemble method in regression, the famous method is just averaging the result and for classification we can use the voting method.
Another bonus by doing bagging is we can use the auto bag error.
So auto bag error is kind of validation error that we can test our fitted tree.
that was trained on these yellow chunks.
Then we can test on this the rest of the data that we didn't select to train on.
So let's talk about random forest.
So random forest has another added idea to the bagging.
So bagging classifier alone can give some performance boost because we can diversify our models by training on the random sample data subsets.
And in random forest on top of that We also have some process of decorrelation.
That means we random sample the features and have the model fitted to this subset of the features instead of whole features.
So why is it called the decorrelation?
Because if we have the same features all the time to grow the trees, even though the data subsets are slightly different, the individual tree might have the same structure of splitting in the same orders of so on.
So if you have a random sampling of features, individual trees grown on the subset of the data and subset of features will be likely to have a different structure from each other.
So that is why it is called a decorrelation.
So having this bagging and decorrelation together, the whole algorithm is called the random forest.
Okay great I get that why the random sampling features might help, then how do I randomly sample these features?
A rule of thumb is the square root method.
So when we have 100 features in the data, we will select 10 features in the data subset.
Here are the results of random forest classifiers.
The green line shows that we had a square root method for selecting features versus the red curve means a random forest using all samples.
So essentially that's the bagging.
So you can see some increased performance when we de-correlate the trees.
or use a smaller number of features.
Here is another result showing the power of angsangbol.
So this green star point is actually a single tree test performance.
And then as you can see, as we increase the number of trees in the angsangbol, it generally goes up and then at a certain point, they kind of behave similar.
So this blue curve, for example, is a random forest test error.
And this red curve is a bagging test error.
So both the random forest and the bagging method, they are ensembleing methods and they increase the performance a lot compared to just a single tree.
However, as you can see, decolonizing trees make it a little better than just bagging, just random sampling the data.
You can also see the out-of-bag test error.
So these are kind of validation error during the training process.
And random forests also have a cool feature.
It has a built-in feature importance.
So in SQL library you can pull out feature importance after fitting the random forest model in the data.
Oftentimes this is useful because you can figure out some feature importance and then use it as a feature selection.
So even if you want to use some different model you can still use random forest to do the feature selection and then you can build some more serious model on top of it.
So that can be a handy tool.
Alright, so far we talked about some basics of random forest, what their definition is, and why they are useful, and what kind of mechanism they work on.
And next video, we're going to talk about another angsangbuk method called boosting.
Hello everyone.
In this video, we're going to compare support vector machines' performance with other models.
Last time, we talked about kernel tricks which are used to treat the nonlinear data such as this one.
This is not linearly separable.
Therefore, SPM with the linear hyperplane wouldn't be able to separate.
So the trick was to add higher order terms.
which is essentially making the data to lie in the higher dimension like in the picture on the right.
So by adding higher dimension, we might be able to separate the data points which wasn't possible in a low dimension.
So to do that, we introduce several corners that treats a higher dimension.
So one of them was called the polynomial corner.
So with this degree d, we can specify how many higher order terms of the features we would like to include.
And as a result, we were able to separate this data, otherwise linearly not separable.
We also talked about radial basis function kernel, which is discussion shape function.
This is good for data that's a radial shape, such as a donut shape that we saw before.
So let's briefly talk about properties of SVMs.
SVM needs a feature scaling.
That means we need to normalize a feature by column so that all the features are more or less in the same range of the values.
And also their time complexity scales linearly to number of features.
That means SVM will treat well when the number of features are many.
However, SVM time complexity goes quadratics to cubic to the number of observations.
So SVM is usually good for small to medium sized data with a large number of features.
SVM also works well on sparse features.
That means even though the feature value has a lot of zeros, SVM will be able to handle gracefully.
Comparing to random forest which is also good for a large number of features.
However, random forest can be very slow if the feature values are all real values and it's dense.
Whereas SVM, it can handle more or less similarly to categorical variables.
Another property for SVM, as you know, the SVM has a C parameter.
And we mentioned the C parameter gives the budget of the error.
So small c means that the model can tolerate small error.
That means a high variance and low bias model, whereas a larger C, the model can tolerate more errors and therefore higher bias, lower variance.
In fact, the optimization under the hood of SVM looks like this.
So we talked about making all of the data points within this margin with some kind of slack variables.
That's equivalent to minimizing this loss function.
So without the mathematical proof, we're going to use it.
And this loss function is called the hinge loss.
And let's call this g. Then this is a positive value for loss.
And then it becomes zero, where this g becomes one.
And then after that, the loss stays at zero.
So this looks like a hinge.
So Therefore, the name is Hinge loss.
And then, but not only the Hinge loss, but it also has regularization term.
And this parameter lambda, regularization parameter, is proportional to C. So if we have a larger C, that means we have a higher regularization parameter.
So let's talk about SK-Lon library usage.
So SK-Lon library has a couple of functions.
So one of them is linear SPC.
support vector classifier and it uses a liblinear algorithm which does not use conor so no conor so this linear SVC function works better when the data is a linear is a problem and as you can see the penalty is L2 already and then loss function uses a slightly different one it's called the scared hinge however everything else is works similar And you can also handle multi-class classification problem.
It just uses a one versus the rest type of strategy.
Because SVC or SVM, they are built for binary class classification.
In order to do the multi-class classification, one needs to have one versus the other.
Alright, this is another classification function called SVC.
And it's using libSVM.
and it uses a kernel, so it can have different kernels.
So by default, it's using radial basis function.
However, we can also change to poly if you are using polynomial.
This degree only applies when we are using polynomial kernel.
Oh, by the way, important thing to mention, sklon has a C parameter.
However, the definition of this C hyperparameter is inverse to the textbook's notation.
In the textbook, C directly means that it's number of violations that we can handle.
However, this is the inverse of that.
So that means if we were to have more regularization, we need to make this C smaller.
All right.
In SKLUN, SVM module can also do the regression, and that function is called SVR.
So you can just simply call SVR function, and everything else is pretty much similar.
To SVC.
Just a quick explanation how the SVR works.
It is the reverse of SVC.
What do I mean by reverse is this.
This is the hyperplane and this is the margin.
Then in SVC, we want the classified points are outside of this margin.
And depending on how many errors we allow, they may be just inside the margin or something like that.
However, in SVR, we...
reverse that condition that we want them to be as close to as possible with this decision boundary.
So we kind of fit this hyperplane or line to the data and do the regression.
So yeah, that's essentially how the SVR works.
All right, so for the rest of the video, we're going to talk about SVM performance in comparison with the other high-performing models such as ensemble method.
So to do that, we prepared five data that are similar or different to each other.
All of these five data has a task of binary class classification.
And they have different number of features and different number of observations.
So first data, as you might have seen before, it has a certain features and little more than 5000 samples, of which 80% of them will be used for training.
And just naive decision tree performance after this training and testing on the testing sample is a little more than 60% accuracy.
And as you can see, some of the columns here, this data has a sparse and most categorical features.
So these are real value features.
But however, most of them in this data are categorical and a lot of them also have zero values.
The second data of our choice has 20 features, similar number of samples, similar number of training samples.
So even though the number of features and number of samples are similar to the first one, its problem is a little bit more easier.
So even the decision tree can perform well, about 90% accuracy.
And as you can see, all these features are very dense, so all of them have some numbers, and they are also real value features.
CERN data has more than 100 features and about 3,000 samples and about 80% of them will be used for training.
And decision tree performance was about 70 something percent.
And as you can see, all of these features are categorical and they are also sparse.
And data 4 has even more features, 300 and more.
And then about a little less than 6,000 samples and 80% of them will be used in training.
And decision tree performance on this data was about 70% or less.
And as you can see, all of these data have real value features and they are very dense.
All right, this is our last data.
It has even more features, about 1800 features, and it has a little less than 4000 samples.
However, we're going to select only very small number for training.
So 375 training samples and let the rest to be testing data.
We chose arbitrarily low training samples just because we wanted to show the performance of different models on the data that has more features than the training examples and also small training data.
The decision tree performance on this data is about 70%.
And this data consists of 93% of most categorical variables, and it has some real value variables as well.
With this long description about our data, here are a summary table for the performance of different models.
So you can see we compare five different models.
This decision tree and logistic regression being simple model as a baseline.
And then we also compare other high-performing models from three ang-sang-bu methods.
random forest and gradient boosting machine.
So as you can see for the data one which has a relatively small number of features and moderate size of data, you can see interestingly the logistic regression was the best model and GBM and SVM also performed reasonably well.
And because it's a relatively simple data, this didn't get to use a lot of trees.
The data number two, if you remember, is a relatively easier data in terms of having higher accuracy.
So even the decision trees and logistic regression have a high performance.
As you can see, fancier models performed a little bit better with the expense of lots of trees for the triangle symbols.
For data number 3 from our baseline, the season tree was a little more than 70%.
And logistic regression also performed reasonably well.
However, as you can see here, a little star mark, that means the sklearn library gave a little warning that the max iteration has reached.
I found it happens usually when we have a lot of features, so the optimization in logistic regression becomes little unstable.
Nevertheless, it gives some number so we can use that.
The Triang Sang Bulls and SBM, they worked pretty well.
And they gave a good result by big margin to the baseline models.
And again, you can see Triang Sang Bulls used hundreds of trees.
And then DataFour, which has even more features.
gave some similar results.
All of these tri-ensemble models and SVM, they worked better.
You can see sometimes SVM and tri-ensemble, their performance are similar or sometimes tri-ensemble are slightly better.
But as you will see later, there are some trade-offs.
For the Data 5 that had a more number of features than the number of samples, the decision tree and logistic regression, those baseline models didn't do very well.
However, the tri-ensemble and the SVM model worked much better.
By the way, all of these accuracy values are from a 5-fold cross-validation.
And also, these numbers in the parentheses are the selected hyperparameters after we do the grid search with the 5-fold cross-validation.
Alright, so performance-wise, in terms of accuracy, we saw that all of the ensemble models and SVM, they are comparable or sometimes ensemble models are better.
But how about the training time?
If they give a similar performance, however, one model gives a much shorter training time than the other, then that model that took less time to train seems a better choice, right?
So let's see here.
So the ensemble methods, random force and gradient boosting machine, they took about 100 milliseconds to a little less than 100 seconds, depending on how many number of trees they used.
And as you can see, even the number of features and the data dimension are similar.
Sometimes the other data takes much longer.
And the reason is this.
So for example, data 1 had Therefore, any tree-based method can be slower when there is a lot of real-value features.
This can be avoided if you use some models that uses a histogram-based split or split randomly instead of going through all these values.
On the other hand, SVM doesn't suffer from that problem.
So as you can see, it takes not only shorter than tree ensemble method usually, it also doesn't care whether the feature values are real-valued or the categorical.
So, in that case, we can see SVM is a little more advantageous.
So, as a conclusion, which models to use, we recommend you inspect the data first and then pick the method that's likely to be better suited for the data.
So, we recommend to use SVM model if it has large number features and small to medium size data, which means a few hundreds to few thousand, and also if the data features are mostly real-valued, it's likely that the SVM performs comparable to random forest and GBM, and it takes much less training time.
A good thing that we should keep in mind is that always try simple model first and see how it goes.
You may have to think about Occam's Razor principle, which tells that if the model performances are similar, the simpler model is always better.
On the other hand, Choice of model can be depending on your goal and also the computation resource and the data size as well.
So, for example, if you are running for a machine learning challenge, you might want to try fancy models at the expense of some training time because a little bit of higher performance would be helpful.
However, if you are dealing with a really big system with really big data, you want to go with a simple model that takes less time to get you an idea how the model and data interacts.
Okay, so the behavior of test header that goes down first and goes up later as we increase the model's flexibility can be explained by bias-variance trade-off.
So what is bias and variance?
Let's have a look at graphical explanation.
So when the bullets are well-centered and well- grouped.
They are called low bias and low variance.
When the bullets are well grouped but far away from the target center, then it has a high bias because it's far away from the center or true value.
But it has a low variance because they are well grouped.
On the other hand, if the bullets are quite spread, but it's still well centered around the target, then we can say it has a low bias and high variance.
And as you can imagine, if bullets are not close to the center but it also has a large spread, then we say it's a high bias and high variance.
So how does that translate to machine learning?
In machine learning, we have data from real life and this data can be very complex and we don't know what the true model is.
By making a model, we introduce some assumption and there is an error that's caused by a simplification by choosing our model.
and this error is called the bias.
On the other hand, variance in machine learning means a variability of the model.
So what is the variability of the model?
Let's say we had some data that look like this and then we were to fit the data with the simple model and maybe we can also fit with the more complex model that tends to have a lower error so lower bias However, if we chose different data set, like this for example, then if we fit the simple model again, they will be very similar.
But if we fit the complex model, now it's going to be a little different from the previous.
So this variability of the model is called a variance of the model.
So if the model is simple, they tend to have low variance.
They don't change much even though we change the training data.
But when we have a more flexibility in the model, they may change quite a bit depending on how we choose the training samples.
So they tend to have high variance.
All right, so simpler model tends to have a high bias and low variance.
So it will correspond to this one.
A more complex or flexible model tend to have a lower bias but has a higher variance.
So it will be this case.
In machine learning, a lot of models are either this case or this case.
There is a trade-off between the two.
That's where the bias-variance trade-off coming from.
Sometimes if the model is not very good, then you may encounter this case.
So some type of model, such as a deep neural network with some other tricks, they may have low bias and low variance.
But most of cases, we have the trade-off between the bias and variance.
So back to our test error, why it goes down and then goes up.
Because of the bias-variance trade-off.
So when we have this is model complexity.
This is test error.
Or error in general.
The bias goes down as our model complexity increases and the model variability goes up as our model complexity goes up.
And when you use a squared error, you can actually derive the general relationship between bias and variance to the test error.
So test error, let me see, can be written as a variance of the model, estimated model, and then the bias of the estimated model also and squared plus some irreducible error, the variance of the residuals.
You can have a look at the supplemental note but this is the result and according to this the test error is a sum of this variance of the model and bias scaled of the model.
So in the end, our test header will have a shape of this because it adds this too and then there is some irreducible header from the residuals.
So that's how the test header shape looks like this.
However, in reality, depending on your model and data, your test header may look, just go down and then flattens and that's very common, whereas your training header goes down and down down.
And sometimes the simple model fits well to the data already.
In that case you may have already good test error for the simple model as well like this.
And then it goes up like this.
Something like this.
And also note that this doesn't have to be squared error.
It is very general behavior no matter which loss function or error function you have.
Alright so in summary we talked about what happens if we add more complexity to our model.
We talked about polynomial regression.
and where we stop adding more terms to the model by monitoring train and tester error, and we also talked about the bias-variance trade-off principle.
Hi everyone.
In this video, we're going to talk about multilinear regression.
So previously, we talked about simple linear regression where we have only one variable, and now we're going to add more variables, whether it's a higher order terms for that single variable or other features into the model.
And then the key idea we're going to discuss is that when the model complexity increases by adding more features, it can fit the data better, but it can also introduce some other problems.
So we'll introduce a concept of bias-variance trade-off.
and then we'll talk about how to select the features that are most contributing to the model.
So last time we talked about single variable linear regression which takes the form of y equals a0 plus a1 x1.
So x1 is a one feature that we care about and a1 is a slope and a0 is a coefficient for intercept.
So the example we had was the You can predict the price of the house sales as a function of size of the house.
Size could be x1 and price is the y that we want to predict.
And now let's say we want to add another feature, say size of the lot.
So when it has a big lot, then maybe it's more expensive than the same.
small house that has a smaller lot.
So we can think about this and we can add the new term, new feature into our model a2x2.
And similarly, we can add more features such as a number of bedrooms and things like that.
Then it becomes more complex model and so on.
So this is also linear regression especially it's called multi linear regression because it has multiple features.
But we can also make some other model that has a higher order terms of the house size.
For example, we can have a square term of the house size.
So in that case, we'll have a1x1 plus a2x1 squared.
And that could be also a good model.
And if we want to add more complexity or higher order term to in this model with the same feature, We could add a third term, the cubic term of the house size like this and we can add more.
So in this case, it's called the polynomial regression.
This is multilinear regression.
We can also engineer some features.
Instead of having square term and cubic term and so on, we are not restricted to have just higher order terms.
But we can create some other variable or features using existing features.
So for example, if we are predicting some probability of getting diabetes based on height of a person and weight of the person.
and some other features that we measured from the lab and so on.
Instead of having this model height plus a2, weight plus and so on, we can construct another variable let's say called x prime and which is BMI which is proportional to weight divided by height squared.
So this BMI is a function of x1 and x2 and this becomes a new feature x' and we can have instead a0 plus a1 x' and the things that we wanted to add like lab test and things like that something like this instead of having a height and weight separate features.
So there are many different possibilities that we can engineer like relevant features depending on your domain knowledge or your intuition on the problem and so on.
So linear model can become really flexible in this case.
So we're going to talk about what happens if we start adding more complexity into model and then there are some things that we need to be careful.
So we'll talk about those.
So let's start by polynomial regression.
This M represents the order of the maximum term.
So M equals 1 represents the simple linear regression AX plus B.
then m equals 2 will be a0 plus a1x plus a2x squared and so on.
So these are the complexity of our model.
So when you look at the simple linear regression, it looks a straight line which is okay but it's still maybe a little too simple for this data.
So let's add another term, square term, and then maybe it fits a little bit better and we can add a cubic term.
And then you can see as you add more high-order terms, the line, the fitted line becomes a little more flexible and have different shapes of the curve.
At some point, the fitting fails actually.
And what happens here is that I wasn't very careful about scaling of the feature x.
So in my simple linear regression model, this was on the order of 1000.
and my y is going to be on the order of million, then this coefficient could be on the order of thousand or less and so on.
And then this square term would be on the order of million and by the time I have the size of the house of six power, this could be 10 to 18, which is a really big number and the coefficient to match this number should be very small.
That means the computer has a hard time to calculate all these coefficients.
Therefore, the fitting may not work very well.
In order to prevent this disaster, one way you can do it is just scale the feature to something on the order of 1 instead of 1000.
So if you just divide by 1000 of your features, then you could have 1 to 6, 7 something like that.
And then here you're gonna have 1 to 6 or 10 to 6.
So it's more manageable.
Therefore you can add more high order terms if you want to.
However you will see shortly that we don't want to add high order terms indefinitely.
So it leads to a question where do we want to stop adding high order terms.
Obviously when you see the model fitness The model fitness will go up and up as you add more model complexity.
So you have some data like this.
And your model could be a little crazy that it has a really high order and can fit everything like this.
This model is not very good.
First, it's not very interpretable, but second, it's more vulnerable to new data points, say this one.
It will have a huge error, or maybe like something like here, you'll have a huge error with this.
However, if you have a simpler model, it will have a smaller error at this new data point and things like that.
That's the motivation.
How do we determine where to stop when we add model complexity?
We want to monitor the error that's introduced when we introduce new data points.
So you remember we talked about how to measure the test data error and training data error.
So we had a data set that we have both feature and label and then we set aside some portion of this data and called it test data.
Another name for test data that's used while we are training is called validation.
So we can call them interchangeably.
in machine learning community validation error is more used term for the data set that's set aside for the purpose of testing while you're training the model.
But anyway, with this we can measure errors for the training and testing.
So let's say we picked MSC.
Then as we mentioned before, we have a trained model and measure the we can have the prediction from the training data and With the training label, we can calculate the mean squared error or any error metric of your choice.
So that becomes the error for the training.
And we can do the similar for the test data.
XTE prediction value and then YTE.
Then we can have the error for the test data.
And this F corresponds to each different model with the different...
high-order terms or different model complexity.
So this is M equals 1 and this is model with M equals 2 etc.
Then when you plot, the exact shape of the curve for training error and test error will be different depending on your number of data and data itself that you randomly sample.
Also, it will depend on your model.
Complexity and so on.
However, in general, you're gonna see this type of error curves.
So for training error, it will go down as you increase your model complexity.
However, the test error will go down in the beginning and then at some point it will start going up again as the model complexity is increased.
And we can find the sweet spot here that the test error is minimized.
So we can pick our best model.
complexity equals 2.
You can also see this model complexity M equals 3 model is also comparably good and in some cases depending on your data draw it can show you actually slightly better results than model complexity equals 2.
However, if they are similar, then you want to still choose the simpler model and this kind of principle is called Occam's razor.
It's essentially telling that if the model performance are similar, For simpler model and complex model, we prefer choosing simpler model.
Alright, so let's talk about how well my model fits.
So we're going to look at the numbers R squared value and adjusted R squared.
These are metrics for how well the model fits.
Adjusted R squared is actually same as R squared except that it also takes number of features in top count.
However, when the number of samples are much larger than number of features in the model, these two numbers are essentially the same.
So let's derive R squared as a measure of model fit.
When do we know that model has a good fit?
From the least squared method that we used to determine our coefficient values, we know that model has a good fit when we have a squared error is minimized.
So again, we can use MSC or RSS.
RSS is a residual thermal squares.
It's nothing but same as MSC without the averaging factor.
So we define this quantity and we know that if this quantity is minimized, we know the model has a good fit.
However, there is a little bit of problem with this metric.
One is that this value can be arbitrarily large depending on our unit of the target variable.
And also if we have a different set of data, this quantity will be different.
So we want to normalize by something similar error measure that has same kind of unit.
So what would be a good way to do that?
We can define a benchmark model, say y equals y mean, and then we can compare how good is my error from my model.
y equals beta 0 plus beta 1 x.
Compared to the error of my benchmark model, which is y equals y min, so we're gonna define another quantity called the TSS, total sum of squares, that actually quantifies the error between my null model and my training data points.
So with that, we can define a dimensionless quantity.
by dividing RSS by TSS.
So this is a quantity essentially telling that what's the ratio of the error from my model to the error from the null model or benchmark model.
So this can be a good quantity that measures how my model fits compared to my null model.
We also want our quantity or R-squared value to be higher when when my model fits better.
And if you see, RSS goes down when the model fits better.
So we're going to flip the sign.
We're going to just subtract this quantity from 1.
Then actually it becomes the definition of R squared.
So let's take a moment and think about what values R squared can take.
All right.
So we're gonna think about two extreme cases.
So one extreme case is that when my RSS is 0, that means my model fits perfectly all of the data points, which will never happen in practice.
But let's think that my model is so good that all the data points are on my model's line.
Then this term goes to 0, and my R-squared value will go 1.
So that's one extreme.
Can R-squared go 1?
larger than 1.
R-squared value cannot be larger than 1 because RSS cannot be negative, right?
The another extreme case is that my model is actually just as good as my null model y equals y mean.
In that case, my RSS value will be same as TSS.
So this goes to 1.
Then my R squared will go to zero.
Can R squared value go negative?
Yes, it can.
In practice, if you use a package to fit your regression line, it will almost never happen.
But in case your model is this bad, like this, the slope is totally wrong.
And then it might have RSS that's larger than TSS.
Then this R squared value can go negative.
For simple linear regression, this may not happen, but as you might see later, in a more complex model, sometimes the model can fit worse than the baseline.
So remember that R-squared can go negative as well.
Alright, so we saw that R-squared value could be a good measure of how my model fits.
However, you have to be careful when you interpret the value from your summary table.
Let's take an example where we might want a model that takes a form of ax and there is no intercept.
Why would we want to do that?
So let's have a look at the intercept value.
It's a negative value and that means my sales price will go negative when my living space is zero.
That doesn't make a lot of sense.
So maybe instead of having this uninterpretable intercept, maybe we want to have a model that has no intercept.
And then, yeah that sounds good.
My sales price of house should be zero when the living space is zero.
So let's take a fit and look at the summary table.
We have a square fit living coefficient which is similar to the previous value which is good.
But then we suddenly see R-squared value has gone up.
What does that mean?
Does it mean our new model y equals ax is better than our old model ax plus b?
Well, not necessarily.
If you look at carefully, you're going to see uncensored next to the r squared.
What does that mean?
It turns out that this r squared value is calculated such that RSS of our new model, this guy, and then divide by TSS of the new null model which is not y equals y mean but now our new null model is y equals 0.
So this goes to here and then the total sum of squares from y equals 0 will be way higher.
Therefore, the R-squared value ...
can be much larger than the previous one.
So if you want to compare apple to apple how my new model is doing in terms of the error, you can just directly calculate RSS for our new model.
Let's say y equals ax and then compare with the previous model RSS y equals ax plus b.
Then you're gonna see this RSS is larger than this one.
But the value that it gives here in the summary table is a little bit deceptive.
Okay, so let's talk about how significant the coefficient values are.
So when do you say the coefficient values are significant?
And conversely, when can we say that the coefficient value is not significant?
So think about this, when the coefficient value is not significant, that means we picked up some kind of noise from the data and assign some value for coefficients.
when in fact the coefficient value is zero.
So when you hear coefficient value is not significant, that means the coefficient value should be actually zero.
So let's look at this coefficient value.
It's minus 4000 something and it's a 280 something.
So that looks like that's very big number, big difference from zero.
So maybe we can, can you just say my coefficient values are all significant.
So the absolute value of the coefficient value is not enough to say whether it's significant or not, because consider we change the unit of this target variable, then we can suddenly have a very small coefficient number, and it's hard to tell then whether this number should be zero or not.
So we need some comparison.
We need some way to compare whether this number is good enough or not.
And usually the standard error is a good way to tell it.
So that means if my coefficient value is maybe here, let's say mu, average of my coefficient value is here and this is zero.
And we want to know how far away is my average coefficient value.
And also we want to know how much of spread I have.
So if the spread is pretty large like this, then maybe this mean value for my coefficient isn't very real.
However, if I have this sharp distribution that essentially says my spread of my values for the coefficients are this small and this far away from zero, then I can say with the confidence that my coefficient value is actually real.
So we're going to talk about this and all these values that shows here are good measure of those confidence or statistical significance.
So let's dive in.
So we mentioned that it is important to know the standard error or the spread of my coefficient value.
And there are different ways to get the standard error or the spread of my coefficient value.
One is using some theory or assumption that the residual is some normal distribution with the zero mean and certain spread or variance.
Another way to do that is we just resample the data multiple times and then fit onto that data and get the coefficient value and we do that experiment multiple times.
then we can get the standard error of my coefficients and all kinds of statistical values from there.
But let's briefly talk about what this model-based method is.
So VIRAR derivation, this is called the covariance matrix, which is variance of beta zero, variance of beta one, and covariance of beta zero and beta one.
So matrix looks like this.
But however, we don't have to remember all this math.
This is given by this formula and this leads to the standard error value for intercept and slope.
It looks like this.
However, we don't have to remember all this formula.
But important thing to remember is that all of this variance or standard error value is proportional to the variance of the residual.
So that means if I have data that has a large spread like this, then it's likely that my coefficient values also have a large spread.
And also you can see that the spread of the coefficients not only depends on this variance of the residuals, but also the variance of the data itself.
This model assumes a homoscedasticity.
That means the spread of the data is kind of homogeneous over the data.
However, if you look at our data, that looks like some kind of cone shape like this.
The residual, the spread of the residual is not homogeneous.
However, we can still assume this model and then derive the quantities that we need.
If you are not convinced by that, the model's assumption, then maybe we can use bootstrapping method.
So bootstrapping method is resampling method.
So let's say we have data point that looks like this originally.
Then we can sample some experiment that samples some of the data point like this.
And then we can have that and then draw another one that samples this data.
And so on.
And we can have multiple copies of this.
We can have many many samples that we want.
We can even sample the same data twice or multiple times.
It doesn't matter.
We can have some sampling with the replacement.
So let's say we have many data like that and then we can fit the coefficient values and this coefficient value will be different from this one slightly but they will be similar.
So we get all these values then we can get the mean value of this as well as the standard deviation or variance of that value.
Alright so that's how we get standard error for the coefficient values.
So let's talk about how we determine whether our coefficient values are statistically significant.
To do that, we're going to do the hypothesis testing.
With the two hypothesis, the null hypothesis say that our coefficient value is 0 and the alternate hypothesis saying that our coefficient value is not 0.
And to test that, we're going to construct a t-score which is given by this.
The t-score is standardized.
our coefficient value, estimate value, by subtracting the mean which is given by our hypothesis and the standard error of our estimated coefficient.
This is similar to G-score in normal distribution and actually when the number of samples is larger than 30, the t-distribution approximate to normal distribution so they are essentially the same most of cases.
We're going to calculate the p-value and We can briefly review.
So let's say we have a standard normal distribution like this.
This is zero mean and has a unit variance.
And then let's say we want to have a 5% of error rate.
So that gives us critical value which defines that this area within these critical values.
plus 1.96 and minus 1.96 for standard normal distribution.
This area is 0.95 and the rest, this region and that region, the combined area would be 0.05.
So that's our error rate and this value is also called alpha.
And in standard normal distribution, this shaded area are symmetric, so each of them is going to be 0.025 0.025 and then we're going to have p-value according to our t-score.
So whenever our t-score lies in the rejection region which is shaded in this red area, or maybe here, then we can reject the null hypothesis.
And what is the p-value here?
p-value is this area under the curve enclosed by this t-score or this area in case the t-score was negative.
In that case, in this particular example, our p-value is smaller than the half of the alpha.
So this green area is smaller than the rejection area and in that case, our t-score lies in the rejection region.
Therefore, we can reject the null hypothesis.
What if our t-score lied in here?
Then again, our p-value will be this big.
So when our p-value is bigger than the half of the alpha, we cannot reject the null hypothesis.
All right, so let's see if we can reject the null hypothesis from our regression result.
So that looks like this.
When we look at the t value, it's minus 10, I'm gonna change my color, it's minus 10 sigma away from the mean and for the intercept and for the slope, it's 145 sigma away from the mean.
So that's pretty significant and as you can expect the p value is fairly small, almost zero for two coefficients.
Therefore, we can safely reject the null hypothesis and we can conclude that our coefficient values are statistically significant.
Similarly, we can also define 95% CI, competency-inheritance for the coefficients.
To calculate that, the formula is given by this.
Mean of the coefficient plus minus two or actually it's 1.96 times the standard error and the standard error is given here.
We can also define 95% confidence interval for the regression line.
That means 95% of time my regression line will lie within this orange shaded region.
And 95% prediction interval, which is for the sample points.
That means 95% of time the sample points will be within this blue shaded region.
This analysis can be handy when you have some outliers and these outliers may be good to remove to have better regression.
Ok, so let's talk about how we measure the error from the test data and the training data and how to compare them.
So we talked about this popular error measure so we're going to use them or one of them and let's say we have original training data that we used to use to fit the model.
Instead of using all of them to fit the model, we're going to set aside some data.
Some portion of data is test data.
And the rest we're going to use it for training.
train set, test set, and each of them have feature and label.
So train set has feature x train and label y train.
and the test set has feature test and label test.
So using the train set, we're gonna fit the model.
So model initially had undefined coefficient values.
So we're gonna do fit and then supply our train data, X train and Y train.
And this fit function will determine the coefficient values.
And now this model internally will have optimal coefficient values.
So with that, we're gonna predict this time.
So dot predict.
And for prediction, we don't need a label.
So we're gonna put train data.
Then it becomes yprediction.
So I'm gonna put hat here, but from the training data.
We can do the similar.
with the already fitted model and dot predict and supply test data instead this time and this will give Y prediction from the test data.
So what do we do with this?
So this value and this value.
We can measure the error between the prediction value and and a label.
So for example, so training MSC or error for the training data is going to be MSC of Y true value for the training label, so which is this one, and the Y prediction value from the training data from this one.
So that's going to be our train error.
So for example this one.
And we can calculate similarly MSA for test data.
So MSA test TE is going to be MSA y test the true value and then the y prediction from the test data.
And this value is this for example.
So it's very common that the test error is a slightly larger than the train error.
Or if the data were pretty homogeneous and your model is doing well then train error and test error could be similar value.
We'll say later that if my model is overparameterized, then it doesn't do very well in the test data.
And it's an important way to figure out whether my model is overparameterized or not.
So we'll talk about that later.
In summary, we talked about how we determine the coefficients.
So we talked about least squares method.
method, which minimizes the residual sum of squares.
So we talked about what the RSS is, what the mean squared error is, and bunch of other error metrics.
And we also talked about the goodness of the model fit.
So we talked about R squared and how R squared is derived.
So we derived it using RSS and TSS.
And we also talked about some things that we need to be careful when we interpret the R squared value.
We also talked about significance of the coefficients.
So we talked about standard error of the coefficients, how they are derived or can be determined.
And then we talked about t-score and the p-value and hypothesis testing to say whether the coefficients values are significant or not.
And then we talked about confidence intervals as well.
And lastly, we talked about how to measure the error for training data and test data and how to compare them.
And that's it for the simple linear regression.
And in the next video, we're going to talk about what happens when we add more model complexity such as higher order terms or other features into the model.
Hi everyone, welcome to the class.
This video will talk about introduction to machine learning.
So before we talk about machine learning, let's talk about a several buzz word here.
You might have heard about data science, which some of you might be taking other courses in data science.
Surely you heard about machine learning because this course is going to be about machine learning.
Many of you might have heard about artificial intelligence, which is another buzzword, along with the machine learning and data science these days.
Maybe you also heard about deep learning, which is another hype word.
So let's briefly talk about what these are.
So for data science, it's a really big interdisciplinary field about data.
So you can think about it's actually anything to do with data, including data pipelining, even data collection and data munging and cleaning and data analysis.
which may include manual data analysis, including some simple checks or exploratory data analysis, or can also include machine learning techniques to analyze the data.
Since the data science has a really big spectrum, it can oftentimes be called soft and hard data science.
Soft data science means dealing with the techniques that doesn't require a lot of software engineering skills or a lot of math skills.
So something like data visualization and reporting, dashboard, those kind of things, as well as simple data analysis, can fall into that category.
Whereas the hard data science, those involve more mathematical and more technical skills, such as analyzing data or building systems using machine learning.
Also, data science can deal with the data that's a small size that can fit into your Excel file or something like that, or it's a big data that sits in the big data warehouse.
In industry, the job description looks like this.
So the data scientists, their job role can be varied.
They can do data collection, cleaning, and managing data or preparing data for whatever the company needs.
or they can build machine learning models and do the testing on those data or build a system.
As well as they can also do the visualization and stuff.
So usually data scientists have diverse backgrounds and they require interdisciplinary knowledge.
Machine learning, we mentioned that machine learning several times during the talk about data science.
So machine learning is part of data science and it is also a subfield of artificial intelligence.
It focuses on learning algorithms and building models and training them on the data.
Machine learning consists of different types of learning, such as supervised learning, unsupervised learning, or reinforcement learning.
Many machine learning models, they are coming from statistical learning.
So machine learning extends the statistical learning by including more complex algorithms, which deal with more complex data and bigger data, and more efficient algorithms.
In industry, machine learning engineers can develop and test machine learning models and design machine learning experiments.
and build machine learning systems.
Artificial intelligence has a long history in the CS, and it is about problem solving with intelligence.
That means an AI agent will make an optimal decision according to its algorithm, whether it has a learning component or not, to maximize the goal as a response to the environment.
You might think that AI field is very practical because you're seeing a lot of applications these days.
However, AI also has a lot of theoretical components in it.
And in industry, AI engineers and experts, they are more or less similar to ML engineers, but in broad set skills, including math and programming skills, as well as machine learning.
And they work on building AI system, building machine learning models, natural language processing, robotics, and computer vision and stuff.
And deep learning focuses on neural network models, so building neural network models and training them on data.
And it also deals with a lot of optimization algorithms and training techniques in order to deal with the complex neural network model training.
It is very suitable for complex data such as images, texts and voice, and graphs, and hybrid types of data.
It is a subfield of artificial intelligence and subfield of machine learning.
And in industry, deep learning engineers work on machine learning problems.
It deals with complex data such as images and texts and things like that, or even high performance computing.
We're going to show some summary diagram.
So there is a data science, which is a big interdisciplinary field.
And there is AI, also very big field.
Data science is about anything to do with data, including data analysis.
Whereas artificial intelligence is about solving problems using intelligent algorithms.
And in intersection, when the AI algorithm is learning from the data, it is called machine learning.
And particularly, if it deals with complex data with neural network architecture, it is called deep learning.
And here is the Google trend on the term on machine learning and software engineering.
Just to compare how the machine learning becomes popular for recent few years.
The term machine learning has been around for a long time, however, it became much more popular during the last five or more years.
This graph also indicates that the job growth in machine learning has grown up really much, about 350% during the past few years.
As you can see, machine learning is a top skill in the jobs that involves AI skills.
Alright, that already sounds like ML is very cool.
Let's talk about what ML can do.
So, machine learning is applied everywhere these days.
So, for example, when you do online shopping, you often see this product recommendation based on your browsing and shopping history.
And those use machine learning algorithms to predict the products that are more likely to be purchased by the customers.
Same goes for movie recommendation and music recommendations.
So sentiment analysis is very popular applications these days.
It is standard by now that data scientists analyze the texts such as news articles and social media articles.
to figure out citizens' sentiment on political events.
Similarly, the product review scales or restaurant review scales can be predicted by machine learning algorithms, which information can be important for businesses.
Machine learning is also used a lot in the financial industry.
So, for example, we can forecast the stock price using machine learning.
Machine learning is also used for algorithmic trading, as well as RoboAdvisor, which gives advice for people on how to allocate their assets.
Also, it can be used for forecasting housing price.
And as you can imagine, machine learning can be also useful in medical industry.
By applying machine learning models in the images or tables, it can help doctors to make a medical diagnosis or medical decisions.
It is also used in many science disciplines, such as the bioscience.
So for example, machine learning techniques can be applied to this graph data to inspect the protein interactions or this type of data for the study of genetics.
Machine learning is also a key component in Internet of Things.
Smart sensors and smart devices produce a lot of data, and also machine learning plays a key role in analyzing those data.
Machine learning is also used in self-driving cars.
Self-driving cars can use machine learning and deep learning to recognize images and make decisions.
Alright, so let's talk about what we will learn in this course.
So here is the data science project lifecycle.
So data should be collected and pipelined into data warehouse.
And there is a data governance that the data warehouse has to implement.
And there will be also data pulling and cleaning and maintaining the data.
And that part is called data engineering mostly.
Data science, on the other hand, focuses on using the data and analyzing those data that were prepared by the data engineering process.
So you can include the selection of those data from the warehouse and then cleaning those data and exploratory data analysis and data pre-processing, which means that we prepare data for the model to consume.
And after that, data scientists will build the models and then do the model training.
And there is a result.
And depending on the result, they have to go back to build models.
Or if the result is so strange, then they will have to collect more data or select more data and do this cycle again.
All right.
So among the steps that we mentioned in the data science project cycle, we're going to talk about a few things.
And we don't cover everything.
So, for example, this course is not about data collection and pipelining.
We'll talk about a little bit of data cleaning and EDA and data pre-processing but the main focus will be how to build a model, how to select models and how to do the training and do the testing and analyze those results.
Okay so let's talk about what is learning.
When these children learn alphabets they can learn to generalize.
So for example they can recognize this letter whether it's a small or large or it has a different font or it has a different color or the image looks angled, or the letter is in the word.
It seems so obvious for people, however, to make a machine to learn this, it is not trivial.
So here is some example of supervised learning.
There are images and the labels.
The labels are the names of these animals.
And a supervised learning model learns to predict the label given data.
Unsupervised learning actually resembles very much how humans learn.
So in baby stage, they don't know about the geometric shapes and colors.
But over time, they learn to recognize these visual properties and also recognize the similarities and dissimilarities between them, even before they learn about the names of these colors and shapes.
So those types of learning without labels are called unsupervised learning.
And unsupervised learning is about learning underlying features and extracting information, organizing patterns in the data, or clustering similar data points.
Another type of learning is reinforcement learning.
which the AI agent learns how to act from experience.
So experience is either reward or punishment.
It is very much similar to the animal training.
So we give treats or punishment to make the animal behave desired way.
So reinforcement learning is used a lot in AI and robotics.
All right, so let's change gears and then talk about some definitions that frequently show up in machine learning.
So data in machine learning can be any forms such as tables, images and text and sounds and graphs.
It can be any format.
However, we're going to talk about mostly the tabulated data format.
So let's take an example of this table.
Let's say the supervised learning task is to predict the house price.
So this one would be our labels.
And these labels also are called targets.
And all these columns, except the labels, they are called the features.
And also they are called predictors, which means that those features and predictors are used in the machine learning models to predict the labels.
And this row of the tables are called observations, or samples, which means that the instances of data.
Here are some few examples of machine learning tasks.
So for example, prediction includes classification regression, and these are in supervised learning.
And clustering, which groups the similar data points together, and anomaly detection and dimensionality reduction, they are in a category of unsupervised learning.
And there are other machine learning tasks, such as data generation and feature selection, which are typically not categorized as supervised learning or unsupervised learning.
However, these type of machine learning tasks can be used to enhance the performance of supervised learning tasks.
Alright, so let's talk about prediction tasks in supervised learning.
Prediction tasks can be either classification or regression, depending on the label's data type.
So, when the label is a categorical variable, which means 0 or 1, or 1, 2, 3, or a, b, c, these are called categorical variables, and in that case, it becomes classification.
If the categories are binary, it is called binary class classification.
And if it's multi-categories, call them multi-class classification.
Binary, multi-class.
On the other hand, if the label variable is a real value variable, so something like 0.1, 0.999, something like that, or 3.4.
So real value variables, to predict those labels given the data, is called a regression problem.
So with that in mind, let's talk about how supervised learning works briefly.
So here's the data.
Data consists of features and target.
So feature usually we call x and target is y.
And then we have a model and this feature is input to the model.
The model might have parameters inside or hyperparameters.
That means some settings the user get to choose.
And then after feeding the features into model, the model will make a prediction.
Initially, the model doesn't predict very well, so there will be some error between the prediction and target variable.
And this error can be used to tweak the model to have a better prediction next iteration, and over this iteration, the model becomes more accurate.
So that's how supervised learning works.
So here is a brief taxonomy of supervised learning models.
So for models that has internal parameters, it is called parametric models.
We are explaining what those are individually, and these are examples of parametric models.
Whereas non-parametric models doesn't have internal parameters.
And these are examples of non-parametric models.
Although we are not going to talk about every single model here, we'll talk about most of these models in this class.
Hi everyone.
In this video, we're going to talk about linear regression.
So we'll begin by the definition of linear regression, and we'll talk about how this model can optimize to get the best estimate value.
And then we're going to talk about important quantities for linear regression, such as a fitness performance metric, things like that.
And we'll talk about how statistically significant these estimate values are.
Alright, so let's begin by reviewing how supervised learning works.
So supervised learning needs training data that feeds to the model.
And this model has internal parameters.
And sometimes some models don't have parameters at all.
Some models have hyperparameters as well that users need to tweak.
But anyway, with that, the model can predict the value.
And if the parameters for the parametric model is not optimized, then this prediction value will be far away from the target.
And our goal is to tweak this parameter by optimization so that the model makes a prediction that's close to the target as much as possible.
So what is a linear regression?
It is one of the simplest kind of supervised learning model.
And it predicts a real value number which is regression.
And then it has the parameters.
inside and these parameters are often called coefficients.
And it does not have a hyperparameters.
That means the user doesn't need to figure out some design parameters in advance or during the training.
And importantly, linear regression model assumes a linear relationship between the features and the target variable.
Well, what does that mean?
It means the feature, let's say we have only one feature for now, has a linear relationship to the target variable.
So, let's say it's a house size and this is a house price and there could be some data like this that tells us that when the house size gets larger than the house price gets larger.
And another example could be maybe we want to predict the salary of a person as a function of their years of experience.
Then we might have some data like that.
That shows that in general, when the years of experience goes up, then the seller goes up.
It doesn't have to be a positive slope all the time.
There could be some other example like this.
Maybe the data looks like this and this is age.
and this is a survival rate from some disease such as cancer, then maybe there is a trend that looks like this.
As the age goes up, maybe survival rates goes down.
So these examples show some kind of linear relationship of the feature to the target variable.
When we have a multiple features, linear model also have some linear combination.
shape.
So what that means is this.
So if I have a feature 1 all the way to feature p and they are linearly combined to each other so 1x1 there is a coefficient a1 and then I add up with another coefficient times x2 plus etc.
Coefficient for feature p and then I can also add some free parameter a0 for the intercept.
this becomes my linear model.
And this is called linear combination.
So this type of model, whether we have many variables or one variable, that shows some linear relationship of the variable to the target and this type of model is called the linear regression.
Let's take an example.
This data is coming from Kaggle website.
Kaggle is a repository for machine learning data.
So, if you want to build a machine learning model and train to the data, this is a place to go.
And this website also hosts the ML competition.
That means a lot of competitors, they build their models that fits the data and then they will compare their model performance on this platform.
And this is super fun, so you should try.
Anyway, this data comes from there and this data is about predicting the house sales price in Washington state when there are a bunch of features that describes the house.
So, price is our target variable Y and all these other columns are features.
And because we want to build a simple regression model like this, we want to find out which feature could be a good predictor to predict the house sales price.
If you have a domain knowledge, you can think about what feature will be useful to predict the house price.
You can think about maybe number of bedrooms are important.
The more number of bedrooms, then maybe it's more expensive.
Or you can think about the size of the house matters.
Or you can think about the location of the house matters most.
Things like that.
However, to quantify and have some evidence that which features is most important or likely to important to predict the price, we can Have a look at the correlation matrix.
So, correlation matrix gives correlation values between the features.
So, diagonal elements shows the correlation to the cell.
It has the value of 1 all the time.
However, the other off-diagonal terms, they show the correlation between different features.
Because it's too many, 21 features, I'm going to select the first few and then look at it.
And as you can see...
from the first row, which is correlation values for all other features to the price, you can figure out the square foot living, which is the house size, is most correlated to the price.
And there are other features such as the grade of the house that's comparably good to predict the price.
You should be careful when you select multiple features based on correlation metrics because The order of correlation, that means a high correlation or absolute value of a correlation to lower ones, these orders are not directly related to how important the features are.
So, for example, this feature may have the same or comparable value, correlation value to the price with the skirt foot living.
However, skirt foot living and grades are highly correlated.
So, when I add this feature to my model on top of a square foot living, that doesn't add so much value because this is pretty similar to this one.
So, in that case, some other variables such as floors or something like that or maybe view would add better value to predict the price than this one that has a high correlation to the price.
So, you have to be a little bit careful.
We're gonna go through a method that actually helps to select the features.
right order.
But to select just one feature, correlation matrix gives a good information.
So, let's begin by that.
So, let's talk about univariate linear regression.
Univariate means the variable is only one and also for that same reason, univariate linear regression is called a simple linear regression and often takes this form.
that we have a coefficient beta 0 and beta 1, which represent the intercept and slope.
And then it has a residuals that measures the difference between the target value and the prediction value by our model.
So this residual is important to measure the error and this is for each data point.
So for example, if we have some data that looks like this.
and maybe this is my regression line, then this is going to be my intercept and the slope, the one, and each discrepancy of the data points to the regression line is called residuals.
And our goal is to minimize overall residuals of my model and make my model to produce a predictor value that's as close as possible to the target variable.
This can be done using a single line using stat model OLS package.
There are other packages such as sklons linear model.
However, this is widely used and it is useful because it generates some summary table like this.
So this summary table has a lot of information including the most interesting part.
What are the my coefficient values?
So with this coefficient value, I can determine what my slope and my intercept is for my simple linear regression model.
And beside of coefficient values, we can ask some other questions that are important to linear regression.
We'll begin by how do we determine the coefficients, in other words, how does the model training works under the hood of this.
package.
And we'll also discuss how well my model fits.
And from the summary table values, what gives an idea of how my model fits.
And then we'll also talk about how statistically significant my coefficients are.
That means how robust our estimation for the coefficient is.
And we're going to also talk about how well my model predicts on unseen data.
That means how well does it generalize, which is very important in machine learning.
Alright, so how do we find the coefficients?
So again, this beta zero, beta one are coefficients, and this is my model.
And the difference between the target value and the predicted value is called residual.
So here's a plot that plots the residuals.
So this is a residual that has positive value, and these are the residuals that have negative value.
So when we say how good my model is, that means how small is the error overall.
So we need to find an error measure that accounts all these residuals from all the points.
So how do we do that?
One way to do it is just sum them up.
However, it's going to be zero all the times if the regression line was fit.
So this is not very useful.
So we're going to define another error measure that measures the distance instead of just summing all of these residuals.
So we're gonna have absolute value and then sum them up to n samples.
And in case we have many many samples, this quantity can be very big, so we want to divide by n. Then it becomes mean absolute error, which is a one good way to measure error.
Another way we can do it is we can maybe square each residuals and then sum them up.
And also we can divide by n and this gives mean squared error.
So these two are very popular error measure in regression tasks.
There are some other error metrics that can be also useful.
So we talked about MAE, but MAE can be arbitrarily large depending on how large y's are.
Therefore, we can define percent absolute error instead.
So percent absolute error is a mean of absolute value of this.
This is a target variable value and this is a prediction value and that's divided by target variable value again and takes the absolute and sum them up and takes an average.
So that can be handy.
That can be useful metric.
We talked about mean squared error but mean squared error unit is different from Y unit.
So in case we want to compare in the same unit, we can take a square root Then it becomes root mean square error, which is also good metric in regression Alright, so let's talk about how the optimization in linear regression work there could be various method but this method called least squared method is most popular and almost all Python package that solves a linear regression uses this method so So as a reminder, the model takes the features and it has internal parameters and linear regression does not have hyper parameters and it makes a prediction and we want to find out the values of the parameters that will make this prediction accurate as possible.
And this is done by optimization.
And for this squared method that linear regression mostly use, takes the feature and target value and find a solution for the parameters.
And the name suggests least squares because it uses a squared error.
So we're going to use MSA and let's have a look what the error surface of MSA look like.
So in MSA, the error in the coefficient space where this axis is one of the coefficients and the other axis is the other coefficient, and this axis represents the error, the size of the error.
Then this takes a kind of bowl shape like this.
So if you look at from the top, the contour will look like an ellipsoid.
So, for example, partial derivative of MSE with respect to coefficient A and set it to 0.
And similarly, we can take a partial derivative of MSE with respect to B and set it to 0.
Why we do that?
If you think about the parabola shape, at the bottom, the slope or the gradient becomes 0.
So, we'll use that fact.
And if we do the algebra, we're gonna get the solution without derivation, but you can look at the supplemental note that has all the derivation.
Important thing to remember is that the slope is proportional to the covariance of the variable x and y and then inversely proportional to the variance of x.
And similarly, intercept has this relation.
And if you look at carefully, This suggests that actually the regression line passes through the center, which is mean of x and comma mean of y.
So regression line is centered around these mean values for the x and y.
What happens when we change the scale of the variables?
So for example, we have a big value for living space square foot and really big value for a sales price.
So we want to change the unit for example.
Makes it million dollar as a unit and use a small number and maybe we can divide by thousand for the square-fit living.
So in that case, if we change this by one over thousand of original value and this is 10 to the minus 6 of original value, what happens to my value?
for beta 1 and beta 0.
You can think about it for a while.
Alright, we're back.
So, we're gonna call it r and we're gonna call this as s. And as you know, covariance is calculated by this formula, x minus x mean times y minus y mean, an expectation of this value.
And then similarly, the expectation of x-xmean squared.
So this part is scaled by r and this part is scaled by s and this part scaled by r squared.
So as a result, we're gonna get s divided by r times original value of beta 1.
So if we plug these numbers, we're gonna get 10 to the minus 3 times original value for beta 1.
So my slope gets thousand times smaller if I make my X variable thousand times smaller and make my Y variable million times smaller.
What happens to beta0, my intercept?
So this is going to be S times original value of EY.
And this we already calculated.
It's going to be S over R times the original value of EY.
which I'm gonna just say square and then this quantity becomes r times the ex.
So this cancels out and then we're gonna get s times original value beta zero.
So my inner set doesn't change when I scale the x.
However, it's going to change when I scale the y and it only depends on the scaling of the y.
So let's talk about how we generalize the least squares method to multivariate case.
So when we have p number of features, this is a feature matrix, and we add a column that has ones so that it can take care of the intercept term.
So together with this, this torus matrix is called design matrix.
And this index 1 to n is for the sample index, and this 0 to p is for the feature index including the intercept.
So MSE in matrix form is going to look like this.
Y-Xβ and these are all matrices and then two norm of the matrices.
That is actually the Y-Xβ transpose and Y-Xβ.
So let me take a derivative with respect to beta, then we're going to get this equation and if we further simplify it will look like this.
And this is called a normal equation.
And then solving this equation for beta, it gives a solution like this.
So, it involves the inverse of this matrix inside and sometimes it can be a problem if the rank of this matrix xt and x are not equal to n. And when does it happen?
It happens when there are two or more variables or the features are linearly correlated.
So for example, if my x1 values were 1, 2, 3, and some of the other feature, let's say x5 was linearly dependent on x1, so for example, two times of this, something like that, then these two features are redundant.
Therefore, these metrics becomes non-invertible.
And then there is a problem when we try to get the solution beta.
It actually doesn't mean that we don't have solution.
It means that we have a solution that are not unique.
So we're going to have a hard time to determine unique solution.
But anyway, almost all Python packages that solves the ordinary least squares or less has some mechanism to find the inverse metrics of this.
called pseudo inverse and sometimes this is called Moore-Penrose inverse.
So, with this, we don't have to worry about non-invertible matrices.
