{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f5e851a",
   "metadata": {},
   "source": [
    "## Generate Questions from Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05582b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "qg_pipeline = pipeline(\"text2text-generation\", model=\"valhalla/t5-base-qg-hl\")\n",
    "\n",
    "input_file = \"Data/sentences.txt\"\n",
    "output_file = \"Data/generated_questions.txt\"\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    sentences = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as out_file:\n",
    "    for sentence in tqdm(sentences, desc=\"Generating Questions\", unit=\"sentence\"):\n",
    "        questions = qg_pipeline(sentence, max_length=128, num_return_sequences=1)\n",
    "        for q in questions:\n",
    "            out_file.write(q[\"generated_text\"] + \"\\n\")  # Write each question on a new line\n",
    "\n",
    "print(\"Question generation complete! Questions saved in 'Data/generated_questions.txt'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988ac5c9",
   "metadata": {},
   "source": [
    "## Create Embeddings for Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a86979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the model\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load the dataset\n",
    "questions_file = \"Data/generated_questions.txt\"\n",
    "questions = [line.strip() for line in open(questions_file, \"r\") if line.strip()]\n",
    "\n",
    "# Encode all questions\n",
    "question_embeddings = np.array(model.encode(questions)).astype(\"float32\")\n",
    "\n",
    "# Save embeddings and questions\n",
    "np.save(\"Data/questions_embeddings.npy\", question_embeddings)\n",
    "with open(\"questions_list.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(questions))\n",
    "\n",
    "print(\"Embeddings saved successfully to `questions_embeddings.npy` file!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30a4cfb",
   "metadata": {},
   "source": [
    "## Classify Question whether clear or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa182c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 1. 📂 Define local paths\n",
    "data_folder = \"Data\"\n",
    "embeddings_path = os.path.join(data_folder, \"questions_embeddings.npy\")\n",
    "questions_path = os.path.join(data_folder, \"questions_list.txt\")\n",
    "sentences_path = os.path.join(data_folder, \"sentences.txt\")\n",
    "faiss_index_path = os.path.join(data_folder, \"sentence_embeddings.index\")\n",
    "\n",
    "# 2. 🤖 Load Sentence-BERT model\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# 3. 📥 Load question embeddings and questions\n",
    "question_embeddings = np.load(embeddings_path)\n",
    "\n",
    "# Load lecture sentences\n",
    "with open(sentences_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lecture_sentences = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# Load FAISS index\n",
    "faiss_index = faiss.read_index(faiss_index_path)\n",
    "\n",
    "# 4. 🔍 Similarity classification function\n",
    "def classify_question(query, threshold=0.60):\n",
    "    query_embedding = model.encode([query]).astype(\"float32\")\n",
    "    similarities = cosine_similarity(query_embedding, question_embeddings)[0]\n",
    "    max_similarity = np.max(similarities)\n",
    "    is_clear = max_similarity >= threshold\n",
    "    return (\"Clear\" if is_clear else \"Vague\"), max_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec344a37",
   "metadata": {},
   "source": [
    "## Ask user Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9384abf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Related Sentences:\n",
      "- In this video, we're going to talk about support vector machine. - 0.4898\n",
      "- Whereas this SVM uses a libSVM algorithm which is specially made for SVM and this algorithm uses the kernels. - 0.5777\n",
      "- You might notice that I use the term SVC, which is a Supervector Classifier, versus SVM, Supervector Machine. - 0.6065\n",
      "- So for example, support vector machine can do both regression and classification. - 0.6068\n",
      "- In this video, we're going to talk about support vector machine with the kernel tricks. - 0.6180\n",
      "- Whereas SVM, it can handle more or less similarly to categorical variables. - 0.6271\n",
      "- In this video, we're going to compare support vector machines' performance with other models. - 0.6687\n",
      "- So let's briefly talk about properties of SVMs. - 0.6798\n",
      "- And support vector machine which uses distance between the points and the decision boundary or hyperplane. - 0.6922\n",
      "\n",
      "Related Sentences with Metadata:\n",
      "- [23.srt] [00:00:06.400 --> 00:00:09.550] In this video, we're going to talk about support vector machine. - Distance: 0.4898\n",
      "- [25.srt] [00:02:40.930 --> 00:02:52.409] Whereas this SVM uses a libSVM algorithm which is specially made for SVM and this algorithm uses the kernels. - Distance: 0.5777\n",
      "- [25.srt] [00:02:06.450 --> 00:02:15.140] You might notice that I use the term SVC, which is a Supervector Classifier, versus SVM, Supervector Machine. - Distance: 0.6065\n",
      "- [10.srt] [00:01:14.640 --> 00:01:20.250] So for example, support vector machine can do both regression and classification. - Distance: 0.6068\n",
      "- [25.srt] [00:00:05.809 --> 00:00:09.660] In this video, we're going to talk about support vector machine with the kernel tricks. - Distance: 0.6180\n",
      "- [26.srt] [00:02:27.619 --> 00:02:33.389] Whereas SVM, it can handle more or less similarly to categorical variables. - Distance: 0.6271\n",
      "- [26.srt] [00:00:06.000 --> 00:00:10.960] In this video, we're going to compare support vector machines' performance with other models. - Distance: 0.6687\n",
      "- [26.srt] [00:01:25.789 --> 00:01:28.709] So let's briefly talk about properties of SVMs. - Distance: 0.6798\n",
      "- [14.srt] [00:01:05.540 --> 00:01:13.769] And support vector machine which uses distance between the points and the decision boundary or hyperplane. - Distance: 0.6922\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import csv\n",
    "\n",
    "# Load the model and FAISS index\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "faiss_index = faiss.read_index(\"Data/sentence_embeddings.index\")\n",
    "\n",
    "# Load lecture sentences\n",
    "with open('Data/sentences.txt', 'r') as file:\n",
    "    lecture_sentences = file.readlines()\n",
    "lecture_sentences = [line.strip() for line in lecture_sentences if line.strip()]\n",
    "\n",
    "lecture_data = []\n",
    "with open('Data/srt-embedding-metadata.tsv', 'r', encoding='utf-8') as file:\n",
    "    tsv_reader = csv.reader(file, delimiter='\\t')\n",
    "    for row in tsv_reader:\n",
    "        if len(row) == 3:\n",
    "            filename, timestamp, sentence = row\n",
    "            lecture_data.append((filename.strip(), timestamp.strip(), sentence.strip()))\n",
    "\n",
    "# Get student's question\n",
    "student_question = input(\"Enter your question: \")\n",
    "question_embedding = np.array(model.encode([student_question])).astype('float32')\n",
    "\n",
    "# Search all sentences (max number can be total sentences in the index)\n",
    "distances, indices = faiss_index.search(question_embedding, len(lecture_sentences))\n",
    "\n",
    "# Define a distance threshold (lower means more similar)\n",
    "distance_threshold = 0.7\n",
    "\n",
    "related_sentences = []\n",
    "related_results = []\n",
    "for j in range(len(indices[0])):\n",
    "    i = indices[0][j]\n",
    "    distance = distances[0][j]\n",
    "    sentence = lecture_sentences[i]\n",
    "    \n",
    "    # Check if the sentence is below the distance threshold and is not a question\n",
    "    if distance > 0 and distance <= distance_threshold and not sentence.strip().endswith('?'):\n",
    "        related_sentences.append((sentence, distance))\n",
    "        filename, timestamp, _ = lecture_data[i+1]\n",
    "        related_results.append((filename, timestamp, sentence, distance))\n",
    "\n",
    "\n",
    "# Display related sentences with distances\n",
    "print(\"\\n Related Sentences:\")\n",
    "for sentence, distance in related_sentences:\n",
    "    print(f\"- {sentence} - {distance:.4f}\")\n",
    "\n",
    "print(\"\\nRelated Sentences with Metadata:\")\n",
    "for filename, timestamp, sentence, distance in related_results:\n",
    "    print(f\"- [{filename}] [{timestamp}] {sentence} - Distance: {distance:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
