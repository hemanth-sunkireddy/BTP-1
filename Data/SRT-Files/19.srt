1
00:00:05,589 --> 00:00:10,570
Hello everyone, in this video we're gonna talk about ensemble method and especially random forest.

2
00:00:12,449 --> 00:00:13,779
So what is an ensemble?

3
00:00:14,080 --> 00:00:17,440
If you hear ensemble, you might imagine this kind of image.

4
00:00:18,219 --> 00:00:22,149
So individual instrument players can make some sound in the music.

5
00:00:22,679 --> 00:00:29,109
However, the sound characteristic and spectrum can be limited by one instrument alone.

6
00:00:30,820 --> 00:00:39,090
But if you have a collection of these different types of instruments, you can make very rich and flavorable musical sound.

7
00:00:39,369 --> 00:00:40,549
And that's the ensemble.

8
00:00:42,159 --> 00:00:45,420
So this kind of analogy can apply to machine learning model.

9
00:00:46,250 --> 00:00:48,920
So for example, decision tree can be a weak learner.

10
00:00:49,700 --> 00:00:54,230
However, if they are aggregated in certain ways, they can be much better.

11
00:00:57,109 --> 00:01:02,369
So here is an intuition why the collection of machine learning model can be better.

12
00:01:02,869 --> 00:01:07,819
Well, let's say we have some problem to solve in the general public community.

13
00:01:08,179 --> 00:01:21,430
If you sample people that has the same race, same gender, same age group, and same kind of background, it's likely to have only represent those kind of people.

14
00:01:21,590 --> 00:01:37,460
The other hand, if you have a diverse people that has a different gender, different age group, and race, and background, it's likely to have more representative of the different groups and then therefore we are likely to make a better decision.

15
00:01:38,500 --> 00:01:40,219
Okay, so diversity is great.

16
00:01:40,879 --> 00:01:43,179
Then how can we make our models diverse?

17
00:01:44,619 --> 00:01:49,229
One idea might be maybe we can train our models on different subsets of data.

18
00:01:51,219 --> 00:01:55,560
So training model on different random subset of data is called bagging.

19
00:01:56,199 --> 00:02:03,379
You can think about like putting different data set into bag and then make the model trained on this bag of the data.

20
00:02:04,519 --> 00:02:08,579
Well, but actually the name is not because they put the data into bags, but...

21
00:02:09,959 --> 00:02:12,089
The full name is a bootstrap aggregation.

22
00:02:12,899 --> 00:02:15,019
So bootstrap aggregation is like this.

23
00:02:15,189 --> 00:02:20,699
You have different ways to random sample the data.

24
00:02:20,729 --> 00:02:26,889
So first step would be a randomly sample subset of training data with the replacement.

25
00:02:26,889 --> 00:02:29,229
So we can use the replacement.

26
00:02:29,229 --> 00:02:33,349
That means we can sample the same data that we already sampled.

27
00:02:33,659 --> 00:02:40,019
So let's say you can sample this yellow sections out of this whole data.

28
00:02:40,659 --> 00:02:43,120
and you can also have this overlaps like this.

29
00:02:43,659 --> 00:02:45,259
So that's a bootstrap process.

30
00:02:46,519 --> 00:02:50,469
And then we can grow a tree on this selected data.

31
00:02:50,930 --> 00:02:56,519
So tree number 1 and tree number 2, tree number 3, tree number 4, etc.

32
00:02:58,229 --> 00:03:05,650
And in general, we don't let this tree prune because they may become similar to each other.

33
00:03:05,650 --> 00:03:11,729
However, it is also possible in practice that we can grow prune the tree and then ensemble them.

34
00:03:13,319 --> 00:03:20,699
Alright, and then after growing this tree to each different subset of data, we can ensemble them.

35
00:03:21,250 --> 00:03:29,379
So ensemble method in regression, the famous method is just averaging the result and for classification we can use the voting method.

36
00:03:32,139 --> 00:03:36,479
Another bonus by doing bagging is we can use the auto bag error.

37
00:03:37,149 --> 00:03:43,569
So auto bag error is kind of validation error that we can test our fitted tree.

38
00:03:44,150 --> 00:03:46,129
that was trained on these yellow chunks.

39
00:03:46,530 --> 00:03:52,680
Then we can test on this the rest of the data that we didn't select to train on.

40
00:03:53,259 --> 00:03:55,930
So let's talk about random forest.

41
00:03:58,060 --> 00:04:02,250
So random forest has another added idea to the bagging.

42
00:04:03,150 --> 00:04:11,959
So bagging classifier alone can give some performance boost because we can diversify our models by training on the random sample data subsets.

43
00:04:13,469 --> 00:04:15,539
And in random forest on top of that

44
00:04:15,849 --> 00:04:19,600
We also have some process of decorrelation.

45
00:04:19,769 --> 00:04:29,550
That means we random sample the features and have the model fitted to this subset of the features instead of whole features.

46
00:04:30,949 --> 00:04:32,759
So why is it called the decorrelation?

47
00:04:33,230 --> 00:04:47,269
Because if we have the same features all the time to grow the trees, even though the data subsets are slightly different, the individual tree might have the same structure of splitting in the same orders of

48
00:04:47,490 --> 00:04:47,620
so on.

49
00:04:47,620 --> 00:05:00,069
So if you have a random sampling of features, individual trees grown on the subset of the data and subset of features will be likely to have a different structure from each other.

50
00:05:00,569 --> 00:05:03,360
So that is why it is called a decorrelation.

51
00:05:04,310 --> 00:05:10,830
So having this bagging and decorrelation together, the whole algorithm is called the random forest.

52
00:05:11,720 --> 00:05:18,160
Okay great I get that why the random sampling features might help, then how do I randomly sample these features?

53
00:05:19,569 --> 00:05:22,480
A rule of thumb is the square root method.

54
00:05:23,060 --> 00:05:28,860
So when we have 100 features in the data, we will select 10 features in the data subset.

55
00:05:30,710 --> 00:05:33,829
Here are the results of random forest classifiers.

56
00:05:34,490 --> 00:05:44,699
The green line shows that we had a square root method for selecting features versus the red curve means a random forest using all samples.

57
00:05:44,699 --> 00:05:46,850
So essentially that's the bagging.

58
00:05:47,600 --> 00:05:51,509
So you can see some increased performance when we de-correlate the trees.

59
00:05:54,310 --> 00:05:56,320
or use a smaller number of features.

60
00:05:59,720 --> 00:06:02,520
Here is another result showing the power of angsangbol.

61
00:06:03,330 --> 00:06:07,250
So this green star point is actually a single tree test performance.

62
00:06:08,100 --> 00:06:21,010
And then as you can see, as we increase the number of trees in the angsangbol, it generally goes up and then at a certain point, they kind of behave similar.

63
00:06:21,060 --> 00:06:26,150
So this blue curve, for example, is a random forest test error.

64
00:06:28,230 --> 00:06:30,630
And this red curve is a bagging test error.

65
00:06:32,320 --> 00:06:42,090
So both the random forest and the bagging method, they are ensembleing methods and they increase the performance a lot compared to just a single tree.

66
00:06:42,420 --> 00:06:53,270
However, as you can see, decolonizing trees make it a little better than just bagging, just random sampling the data.

67
00:06:55,030 --> 00:06:56,560
You can also see the out-of-bag test error.

68
00:06:56,560 --> 00:07:00,300
So these are kind of validation error during the training process.

69
00:07:03,910 --> 00:07:06,880
And random forests also have a cool feature.

70
00:07:07,050 --> 00:07:08,460
It has a built-in feature importance.

71
00:07:08,460 --> 00:07:15,440
So in SQL library you can pull out feature importance after fitting the random forest model in the data.

72
00:07:15,440 --> 00:07:24,230
Oftentimes this is useful because you can figure out some feature importance and then use it as a feature selection.

73
00:07:24,230 --> 00:07:35,830
So even if you want to use some different model you can still use random forest to do the feature selection and then you can build some more serious model on top of it.

74
00:07:35,830 --> 00:07:37,770
So that can be a handy tool.

75
00:07:39,950 --> 00:07:51,850
Alright, so far we talked about some basics of random forest, what their definition is, and why they are useful, and what kind of mechanism they work on.

76
00:07:52,290 --> 00:07:58,020
And next video, we're going to talk about another angsangbuk method called boosting.

