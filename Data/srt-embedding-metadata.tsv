filename	timestamp	sentence
8.srt	00:00:06.169 --> 00:00:06.910	Hi everyone.
8.srt	00:00:06.910 --> 00:00:09.970	In this video, we're going to talk about multilinear regression.
8.srt	00:00:11.320 --> 00:00:21.269	So last time we talked about multilinear regression with the higher order terms of a single variable, and this time we're going to talk about multilinear regression model when there are multiple variables.
8.srt	00:00:22.239 --> 00:00:29.300	So we're going to see how to interpret these coefficients and then how to inspect whether these coefficients are significant.
8.srt	00:00:30.780 --> 00:00:32.969	And we're going to talk about how to select features.
8.srt	00:00:33.179 --> 00:00:35.310	and what to consider when we select features.
8.srt	00:00:36.340 --> 00:00:39.770	And we'll talk about highly correlated features and multicollinearity.
8.srt	00:00:40.230 --> 00:00:40.770	What are they?
8.srt	00:00:40.770 --> 00:00:42.359	How does it affect the model?
8.srt	00:00:42.469 --> 00:00:45.109	And what can we do when we select features?
8.srt	00:00:47.070 --> 00:00:50.549	And we're going to talk about what other things to consider when we select features.
8.srt	00:00:51.600 --> 00:00:55.799	And lastly, we'll talk about what to do when there are interactions between the features.
8.srt	00:00:57.740 --> 00:01:01.939	So, as you know, the multilinear regression model can be formulated by this.
8.srt	00:01:02.359 --> 00:01:07.450	So, all the variables are linearly combined to represent the model.
8.srt	00:01:07.930 --> 00:01:09.700	to predict the target variable Y.
8.srt	00:01:11.469 --> 00:01:17.200	And the coefficient, each coefficient is an average effect of that variable to Y target variable.
8.srt	00:01:17.799 --> 00:01:21.409	When we consider all other variables are independent and fixed.
8.srt	00:01:22.629 --> 00:01:24.670	This assumption may not be true in general.
8.srt	00:01:25.280 --> 00:01:29.849	So variables or the predictors might be correlated in real world scenario.
8.srt	00:01:30.840 --> 00:01:34.939	And also, we also assume these coefficients are constant.
8.srt	00:01:35.079 --> 00:01:37.129	However, that might not be true in general.
8.srt	00:01:41.890 --> 00:01:48.109	So if there is an interaction between two variables or more, This constants or coefficient may not be true constant but is a function of some other variables.
8.srt	00:01:48.890 --> 00:01:51.450	So we'll talk about what to do when it happens.
8.srt	00:01:52.980 --> 00:01:56.040	Let's take an example that we saw previously.
8.srt	00:01:56.040 --> 00:01:58.180	It's a house price prediction.
8.srt	00:01:58.930 --> 00:02:05.520	So house price is a Y target variable and all other variables are the features.
8.srt	00:02:06.430 --> 00:02:12.219	We're going to inspect the types of variables and see if they are suitable for linear regression model.
8.srt	00:02:13.380 --> 00:02:15.450	So what are the types of variables?
8.srt	00:02:20.810 --> 00:02:27.000	There can be real value number and there could be categorical variable.
8.srt	00:02:30.660 --> 00:02:39.000	And categorical variable can have ordinal and non-ordinal categorical variable.
8.srt	00:02:39.000 --> 00:02:43.110	What is ordinal categorical variable?
8.srt	00:02:43.740 --> 00:02:48.560	These are categories that have meaning in their order.
8.srt	00:02:48.560 --> 00:03:02.500	So something like age group or grade, abc or 1234, those can be ordinary categorical variable because they have a meaning in their order.
8.srt	00:03:03.670 --> 00:03:12.420	The examples of non-ordinary categorical variables are male, female, race, ethnicity, and so on.
8.srt	00:03:12.420 --> 00:03:15.500	Some classes they don't have any meaning in their order.
8.srt	00:03:16.250 --> 00:03:22.570	So we can permute the orders of categories and they don't have an effect.
8.srt	00:03:23.470 --> 00:03:41.580	So non-ordinal categories are difficult to use in linear regression model because in linear regression model, a variable value times the coefficient present how much of the value in target variable is contributed by that variable.
8.srt	00:03:46.900 --> 00:03:49.600	Therefore, if the variable can be permuted arbitrarily, It's not easy to use in the linear regression.
8.srt	00:03:50.240 --> 00:03:55.160	However, there are ways to use these non-ordinary categorical variables in the linear regression.
8.srt	00:03:55.160 --> 00:04:04.540	For example, we can code male, female into 0 or 1 or 1 or 0 or sometimes minus 1 to 1.
8.srt	00:04:04.540 --> 00:04:07.980	So if we choose one of these, it will work.
8.srt	00:04:07.980 --> 00:04:10.250	And then how about race?
8.srt	00:04:10.250 --> 00:04:15.600	Let's say race had only three categories, Asian, Black, and Caucasian.
8.srt	00:04:17.579 --> 00:04:21.210	So this variable has three categorical values.
8.srt	00:04:21.710 --> 00:04:26.759	So we can convert this into individual three binary categorical variables.
8.srt	00:04:26.829 --> 00:04:29.560	So is the person Asian or not?
8.srt	00:04:29.650 --> 00:04:31.720	Is the person black or not?
8.srt	00:04:32.189 --> 00:04:33.879	Is the person Caucasian or not?
8.srt	00:04:35.720 --> 00:04:42.610	However, we don't need all three of them because if the two are known, the other one can be known as well.
8.srt	00:04:42.610 --> 00:04:43.699	So they are dependent.
8.srt	00:04:44.150 --> 00:04:46.750	So if we just get rid of one of them.
8.srt	00:04:47.970 --> 00:04:52.620	and use only 2 into the model, then it works better.
8.srt	00:04:53.780 --> 00:05:02.270	So in general, if the non- ordinary categorical variable had n categories, we could convert them into binary categorical variables.
8.srt	00:05:02.460 --> 00:05:10.379	But you have to also consider whether you want to include n-1 new features into your model.
8.srt	00:05:10.379 --> 00:05:12.730	So what if you had a really large n?
8.srt	00:05:12.759 --> 00:05:17.030	Do you want to add large n-1 features into your model?
8.srt	00:05:18.250 --> 00:05:18.990	Probably not.
8.srt	00:05:19.590 --> 00:05:21.870	That is an example of this zip code.
8.srt	00:05:21.939 --> 00:05:33.180	So zip code had 70 something categories and I didn't want to add 70 something new binary variables into my model because my model then becomes too big.
8.srt	00:05:34.090 --> 00:05:39.970	So hopefully the other variables such as latitude and longitude can capture some information about the location of the house.
8.srt	00:05:40.360 --> 00:05:44.410	So I'm gonna just get rid of zip code and then use other variables.
8.srt	00:05:46.790 --> 00:05:50.560	Before we build a model, let's have a qualitative inspection.
8.srt	00:05:51.400 --> 00:06:01.820	So, in this case, we're gonna see the correlation between the price and all other variables and see if which variables might be useful to predict the price.
8.srt	00:06:02.430 --> 00:06:13.610	So, square foot living could be useful, grade could be useful, and some other variables such as square foot above or square foot living 15 could be useful.
8.srt	00:06:14.200 --> 00:06:20.310	By the way, this square foot living 15 is a square foot living of similar 15 houses.
8.srt	00:06:22.940 --> 00:06:34.050	So therefore, This must have some redundant information as square foot living and also the square foot living is a square foot above plus the square foot basement so they are linearly dependent.
8.srt	00:06:34.520 --> 00:06:42.940	So they must have redundant information and this redundant information shows the high correlation between the features.
8.srt	00:06:43.160 --> 00:06:56.550	So these two variables have really high correlation because they are linearly dependent and this square foot living and square foot living 15 because they are similar in the definition they are also highly correlated.
8.srt	00:06:57.660 --> 00:07:06.510	So we identified a few variables that are correlated to each other and some variables are linearly dependent to each other.
8.srt	00:07:08.880 --> 00:07:11.440	This can be also visually inspected in the pair plot.
8.srt	00:07:11.690 --> 00:07:15.580	So pair plot is distribution plot between two features.
8.srt	00:07:16.010 --> 00:07:20.500	So in the diagonal element, it shows the distribution of itself, the feature itself.
8.srt	00:07:21.750 --> 00:07:27.640	And the off-diagonal element shows the distribution between one feature to the another.
8.srt	00:07:28.590 --> 00:07:36.650	So for example, square foot above and square foot leaving had a really high correlation and you can see very skinny distribution of the data.
8.srt	00:07:37.430 --> 00:07:49.580	Actually, this is a very good indication of a collinearity, which we will talk about later, but essentially that happens because these two features are dependent each other or nearly dependent each other.
8.srt	00:07:54.500 --> 00:08:04.890	Okay, so we've qualitatively inspected variables and their correlations and we found that some features may have some redundant information and they may cause some problems.
8.srt	00:08:05.600 --> 00:08:11.440	So with that in mind, let's see what happens if we throw all the features into the model and fit to the data.
8.srt	00:08:13.480 --> 00:08:21.780	So here are the results summary table and we can see that model fit with the R squared is almost 0.7, which is good value.
8.srt	00:08:21.780 --> 00:08:29.340	And previously we didn't explain what this f-statistic value is and what the p-value for the f-static is.
8.srt	00:08:29.340 --> 00:08:36.230	And this actually shows whether there is at least one significant variable in the model.
8.srt	00:08:36.230 --> 00:08:36.700	So...
8.srt	00:08:39.460 --> 00:08:43.610	The null hypothesis for F-test would be all the coefficient values are 0.
8.srt	00:08:43.610 --> 00:08:47.580	So the F-value is defined by this formula.
8.srt	00:08:48.310 --> 00:08:59.900	So TSS minus RSS divided by number of features and divided by RSS times n minus p minus 1.
8.srt	00:09:00.130 --> 00:09:02.500	So this is a formula for F-test.
8.srt	00:09:02.500 --> 00:09:08.590	And then the bigger this number, we are more sure about there is at least one.
8.srt	00:09:09.970 --> 00:09:11.900	significant variable in the model.
8.srt	00:09:14.020 --> 00:09:19.590	So in our case, the F statistic value is a big and the p-value for that is almost a zero.
8.srt	00:09:19.590 --> 00:09:23.790	That means it's smaller than certain threshold of error rate.
8.srt	00:09:23.790 --> 00:09:29.750	Therefore, we can conclude that our model has at least one significant variable.
8.srt	00:09:40.420 --> 00:09:42.750	So let's have a look at the p-values for individual variables and we can see immediately that some variables have insignificant coefficient values.
8.srt	00:09:42.980 --> 00:09:45.910	So, square floors have a really high p-value.
8.srt	00:09:45.910 --> 00:09:49.080	As well as sales months have high p-value.
8.srt	00:09:49.080 --> 00:09:54.080	So, we can reject these features because their coefficient values are essentially zero.
8.srt	00:09:54.160 --> 00:10:04.270	Alright, so after removing the features that has a high p-values, we get this result.
8.srt	00:10:04.270 --> 00:10:05.800	So, R-squared value is similar.
8.srt	00:10:05.800 --> 00:10:07.800	f-statics value is still large.
8.srt	00:10:10.220 --> 00:10:19.590	and let's inspect the t-score and the p-values of each individual coefficients and all of them looks statistically significant and that's good.
8.srt	00:10:19.590 --> 00:10:34.520	However, this is not the complete story because we still see some features such as these three linearly dependent each other still exist in the model and they still have a high correlation value.
8.srt	00:10:34.520 --> 00:10:41.120	So we're going to talk about some better ways to automatically add or remove the features in the next video.
9.srt	00:00:05.830 --> 00:00:10.619	In this video, we're going to talk about feature selection method and things to consider when we select features.
9.srt	00:00:11.050 --> 00:00:18.500	So last time, we tried to fit the model that has all the features inside and found that some of the coefficients were not significant.
9.srt	00:00:19.019 --> 00:00:27.129	So after removing those, we tried again and found that the coefficients are significant, but still we had some linearly dependent features in it.
9.srt	00:00:28.019 --> 00:00:31.480	So it's a lot of manual process to figure out which features to select.
9.srt	00:00:31.739 --> 00:00:36.759	So instead of doing that, we're going to introduce some method that automatically selects the features.
9.srt	00:00:37.269 --> 00:00:46.140	So the first method is called the forward selection, which add the feature one by one by looking at the one that maximizes the R-squared value.
9.srt	00:00:46.140 --> 00:00:54.879	So the add feature that maximizes the R-squared value, and that's the forward selection.
9.srt	00:00:54.879 --> 00:01:01.019	And another method is called the backward selection, which starts from the full model.
9.srt	00:01:01.019 --> 00:01:06.599	So by full model, I mean there are all the features inside the model already.
9.srt	00:01:06.599 --> 00:01:09.560	And remove the one feature that has maximum p-value.
9.srt	00:01:11.250 --> 00:01:25.269	So remove xj that has maximum p-value and we repeat this process until we reach the tolerance of the p-value or stop character.
9.srt	00:01:25.269 --> 00:01:32.229	Another good method is called a mixed selection which combines the forward selection and backward selection.
9.srt	00:01:44.649 --> 00:01:51.019	Which means that we add some feature that maximize the R-squared first and then fit the new model and inspect the result and see if there are coefficients that are insignificant and if there are insignificant coefficients, just remove the features.
9.srt	00:01:51.729 --> 00:01:59.629	And then we add another feature again and then inspect the result and remove all the features that have large p-values and so on.
9.srt	00:01:59.629 --> 00:02:04.019	So let's compare the result.
9.srt	00:02:04.019 --> 00:02:13.269	So for selection gives this result that it adds a square for living first and then latitude and view and grade and so on.
9.srt	00:02:13.269 --> 00:02:18.329	And it doesn't have a stock criteria so we're gonna just fit all of them to the last feature.
9.srt	00:02:21.159 --> 00:02:24.439	And then this is result from the backward selection.
9.srt	00:02:26.119 --> 00:02:30.119	It starts from the full model and then it removes one by one.
9.srt	00:02:30.119 --> 00:02:37.679	So it first removed the floors and then secure-floor lot, second and sales months removed and so on all the way to the top.
9.srt	00:02:37.679 --> 00:02:41.379	And the year built was the last one that was removed.
9.srt	00:02:41.379 --> 00:02:53.939	But as you can see, the feature importance are the orders are very different from the floor selection because the floor selection cares about the R-squared value whereas the backward selection cares about the .
9.srt	00:02:54.419 --> 00:03:01.729	p-value and as you can imagine the mix selection resembles the four selection result.
9.srt	00:03:01.729 --> 00:03:03.929	However, it stops at some point.
9.srt	00:03:03.929 --> 00:03:22.689	So because it also look at the maximum r squared, the order orders are pretty similar to four selection, but then at some point adding another feature will lead always have a p value that's larger than certain criteria, so it stops there.
9.srt	00:03:22.689 --> 00:03:26.299	So actually practically mix selection is a good way to use.
9.srt	00:03:27.659 --> 00:03:28.769	as a feature selection.
9.srt	00:03:33.059 --> 00:03:38.479	So here is the result of the correlation matrix after we select the features from mixed selection.
9.srt	00:03:38.989 --> 00:03:46.479	So good news is that now we don't have the linearly dependent feature such as such as a square foot above.
9.srt	00:03:46.979 --> 00:03:51.989	These are gone, but still we see large correlation values between the features.
9.srt	00:03:53.349 --> 00:03:55.349	So let's talk about correlated features.
9.srt	00:03:57.109 --> 00:03:58.139	Why do they occur?
9.srt	00:03:58.229 --> 00:04:02.049	So high correlation among features may occur from different regions.
9.srt	00:04:02.539 --> 00:04:04.619	One of them would be redundant information.
9.srt	00:04:04.939 --> 00:04:10.939	So when the features are linearly dependent on each other, the information is redundant and they may have a high correlation.
9.srt	00:04:12.169 --> 00:04:18.129	And when there is an underlying effect such as confounding or causality, the features may be highly correlated.
9.srt	00:04:18.829 --> 00:04:24.139	So for example, ice cream sales and the sharks attack.
9.srt	00:04:27.169 --> 00:04:28.729	They have nothing to do with each other.
9.srt	00:04:28.729 --> 00:04:32.429	However, they can be caused by hot weather.
9.srt	00:04:35.579 --> 00:04:45.669	So, hot weather here is called confounding and then because of this confounding, ice cream sales and then sharks attack, they will have a high correlation in the data.
9.srt	00:04:47.809 --> 00:05:02.269	And the example of causality is heart disease can lead to heart attack and diabetes doesn't cause heart attack directly, but it can cause a heart disease, some type of heart disease and then cause a heart attack.
9.srt	00:05:02.779 --> 00:05:07.239	So when we look at the diabetes and then heart attack, they may look highly correlated.
9.srt	00:05:08.969 --> 00:05:12.729	And in some cases, just the variables are correlated in nature.
9.srt	00:05:12.929 --> 00:05:17.639	So for example, number of bedrooms and the size of house, they don't cause each other.
9.srt	00:05:17.639 --> 00:05:18.939	They don't have confounding.
9.srt	00:05:18.999 --> 00:05:20.659	However, they are just correlated.
9.srt	00:05:21.229 --> 00:05:23.069	So they may have high correlation.
9.srt	00:05:25.509 --> 00:05:29.519	So we mentioned that it is problematic when there are highly correlated features.
9.srt	00:05:29.649 --> 00:05:30.439	And why is that?
9.srt	00:05:31.489 --> 00:05:36.269	When the predictors are highly correlated, the coefficient estimate becomes very inaccurate.
9.srt	00:05:36.719 --> 00:05:39.319	And also, the interpretation of the coefficient.
9.srt	00:05:39.589 --> 00:05:42.939	as a variable contribution to the response becomes inaccurate.
9.srt	00:05:44.829 --> 00:05:49.839	So when there is a high correlation between features more than 0.7, we consider it's problematic.
9.srt	00:05:49.839 --> 00:05:57.899	And this is especially called collinearity when two features are very similar to each other.
9.srt	00:05:57.899 --> 00:06:08.079	Like we saw previously in the pair plot that the distribution of the data was very skinny between feature 1 and feature 2, for example, then they are very collinear.
9.srt	00:06:08.079 --> 00:06:13.629	Well, it's not always possible to detect the collinearity using correlation metrics.
9.srt	00:06:14.610 --> 00:06:22.539	Because if there are multiple variables that are involved in the collinearity, they may look like okay in the correlation matrix.
9.srt	00:06:22.659 --> 00:06:25.240	However, they could be still collinear.
9.srt	00:06:25.729 --> 00:06:28.430	And this special case is called the multicollinearity.
9.srt	00:06:31.560 --> 00:06:37.949	So beside the correlation matrix, variance inflation factor is a better way to detect this multicollinearity.
9.srt	00:06:38.750 --> 00:06:42.680	So VIF is defined by this formula.
9.srt	00:06:43.079 --> 00:06:45.610	The VIF of a coefficient beta i.
9.srt	00:06:46.500 --> 00:07:01.469	is 1 over 1 minus R squared value of this fitting and this fitting is actually not fitting the target variable Y but fitting that variable Xi using all other variables.
9.srt	00:07:02.289 --> 00:07:14.919	So using other variables that are not this one and we are fitting the model and we're gonna get the R squared value and we can get the VIF value.
9.srt	00:07:20.909 --> 00:07:25.919	So if the VIF value is larger than 5 in general, or sometimes 10, it means that there is strong multicollinearity.
9.srt	00:07:26.389 --> 00:07:33.379	So let's have a look which variables had multicollinearity in the original model that we had all the features in it.
9.srt	00:07:34.039 --> 00:07:40.419	So clearly, square foot living, square foot above, square foot basement, they are linearly dependent each other.
9.srt	00:07:41.339 --> 00:07:43.819	So they show strong multicollinearity.
9.srt	00:07:46.119 --> 00:07:51.519	And then after we have a mixed selection, some of the redundant features are gone.
9.srt	00:07:51.519 --> 00:07:52.959	For example, this one is gone.
9.srt	00:07:53.279 --> 00:07:54.439	And let's inspect.
9.srt	00:07:54.479 --> 00:08:02.809	Square-foot living has still high VIF value, but it's much better than previous one because one of the dependent feature was gone.
9.srt	00:08:02.809 --> 00:08:09.079	And I think that was the only one that was bad here.
9.srt	00:08:09.189 --> 00:08:17.339	All right, but when we see the correlation matrix, there are still some highly correlated features.
9.srt	00:08:17.339 --> 00:08:19.799	Like for example this one.
9.srt	00:08:19.799 --> 00:08:24.599	So let's remove these highly correlated features.
9.srt	00:08:26.089 --> 00:08:27.439	after the mixed selection.
9.srt	00:08:27.469 --> 00:08:40.230	And when we remove those variables with a very high correlation to the square foot living, we end up with a much lower VIF value for square foot living because the collinearity is gone.
9.srt	00:08:42.629 --> 00:08:45.139	Here are some things to consider when we select features.
9.srt	00:08:45.230 --> 00:08:47.039	So we talked about model fitness.
9.srt	00:08:47.500 --> 00:08:52.889	Four selection gives a maximum model fitness by adding one features at a time.
9.srt	00:08:53.549 --> 00:08:58.500	And also we talked about removing variables with the insignificant coefficients.
9.srt	00:08:58.669 --> 00:09:01.279	So backward selection was good at this.
9.srt	00:09:02.149 --> 00:09:06.350	And if we combine this, we can have mixed selection.
9.srt	00:09:08.429 --> 00:09:11.980	We also talked about some problems that may occur when you have a multi-core linearity.
9.srt	00:09:11.980 --> 00:09:17.159	And we haven't talked about this, so let's have a look.
9.srt	00:09:17.159 --> 00:09:22.029	So here is a graph that shows the performance of each model.
9.srt	00:09:22.419 --> 00:09:24.329	This is including intercepts.
9.srt	00:09:24.590 --> 00:09:28.409	So this is just an intercept and this model is intercept 1.
9.srt	00:09:32.109 --> 00:09:36.539	feature and so on all the way to 14 feature plus intercept.
9.srt	00:09:36.779 --> 00:09:37.989	So it shows here.
9.srt	00:09:38.749 --> 00:09:47.129	And this star represents the model performance after we removing the variables with the high correlation.
9.srt	00:09:47.419 --> 00:09:55.169	So when we remove the highly correlated features, they may have a better estimation of the coefficient value and then better interpretation.
9.srt	00:09:55.169 --> 00:09:57.919	However, it can have some less performance.
9.srt	00:09:59.239 --> 00:10:04.169	And another thing that we can think about is that do we need all these 14 features?
9.srt	00:10:04.879 --> 00:10:09.759	It seems that the model complexity 6 or 7 gives a efficient result.
9.srt	00:10:09.959 --> 00:10:14.189	This is still a less performance than having 14 features.
9.srt	00:10:14.189 --> 00:10:16.120	However, it's good enough.
9.srt	00:10:16.609 --> 00:10:18.289	So we can consider that as well.
9.srt	00:10:18.399 --> 00:10:26.899	And by looking at the VIF of this six feature model, it has a pretty good VIF as well.
9.srt	00:10:28.719 --> 00:10:33.370	So here is all the results from the models that we considered so far.
9.srt	00:10:33.549 --> 00:10:35.969	So again, this number of features include the inner set.
9.srt	00:10:36.099 --> 00:10:38.870	So it's actually 19 feature model plus one inner set.
9.srt	00:10:39.509 --> 00:10:45.379	And mix selection gives a 14 number of features selected and so on.
9.srt	00:10:45.470 --> 00:10:53.069	And then if we look at just the feature coefficient for square foot living, the coefficient values are all different.
9.srt	00:10:53.899 --> 00:10:57.600	All of them are statistically significant.
9.srt	00:10:57.709 --> 00:11:02.519	However, if you can see the coefficient values, they are very different.
9.srt	00:11:03.959 --> 00:11:10.629	In particular, this model removed all the features that are highly correlated to the square foot living.
9.srt	00:11:11.009 --> 00:11:15.899	Therefore, the coefficient value for square-foot living is more accurate and more interpretable.
9.srt	00:11:15.899 --> 00:11:23.339	So this means that when we increase square-foot living by 1, then the house price goes up by $313.
9.srt	00:11:23.339 --> 00:11:30.079	On the other hand, the coefficient value for square-foot living is lower in other models.
9.srt	00:11:31.419 --> 00:11:38.129	And that's because the other models still had other variables that were highly correlated to the square-foot living.
9.srt	00:11:38.129 --> 00:11:40.399	Therefore, the coefficient values are not accurate.
9.srt	00:11:42.289 --> 00:11:48.149	and all these correlated features, they kind of share the contribution to the house price.
9.srt	00:11:52.269 --> 00:11:55.809	Okay, so lastly, let's talk about what to do when there are interactions.
9.srt	00:11:55.989 --> 00:11:57.139	So what is interactions?
9.srt	00:11:57.679 --> 00:12:05.089	Interactions can happen when this coefficient is not constant, but is a function of some other variable, say x3.
9.srt	00:12:05.789 --> 00:12:16.709	So in that case, what we want to do is that we're going to have interaction term, so x1 times x3, and then assign another coefficient.
9.srt	00:12:17.089 --> 00:12:20.299	Let's say beta and then add to the model.
9.srt	00:12:21.469 --> 00:12:42.479	And not only this, we can also do all the combinations such as adding x1 times x2, x2 times x3 and all combinations.
9.srt	00:12:43.099 --> 00:12:46.029	And we can also have a higher order terms, something like that.
9.srt	00:12:46.689 --> 00:12:51.189	In that case, we have infinite menu of features and we don't want to do that.
9.srt	00:12:51.289 --> 00:12:54.139	But however, we can just choose the order.
9.srt	00:12:54.179 --> 00:12:57.849	Maybe the maximum order is just one feature times another.
9.srt	00:12:58.299 --> 00:12:59.429	and then we can add them up.
9.srt	00:13:00.559 --> 00:13:04.299	Then we have a problem of how to select all these many combination features.
9.srt	00:13:04.889 --> 00:13:08.329	Again, we're going to apply the same method that we talked about before.
9.srt	00:13:09.259 --> 00:13:12.129	So, mixed selection method is a good way to do that.
9.srt	00:13:29.120 --> 00:13:34.139	One thing that is a little different from the previous case is that when we have this interaction term, then we must include also β1x1 plus β3x3 And sometimes you might see these coefficient values may not be significant.
9.srt	00:13:34.139 --> 00:13:37.409	However, we should still include these terms in order to have this term.
9.srt	00:13:37.409 --> 00:13:46.649	So with that difference, having interaction terms in the model is the same as having multiple features in the model.
17.srt	00:00:05.150 --> 00:00:09.679	Hi everyone, in this video we're going to talk about hyperparameters of decision trees.
17.srt	00:00:10.529 --> 00:00:13.740	So as a quick review, here is how decision tree splitting works.
17.srt	00:00:13.859 --> 00:00:23.190	So from the root node, it has samples and it's gonna pick a feature and its threshold value to minimize the sum of the MSE of the splitted node.
17.srt	00:00:24.929 --> 00:00:30.850	So like this, and then it will further split and pick another feature and threshold value.
17.srt	00:00:32.740 --> 00:00:33.329	like this.
17.srt	00:00:35.170 --> 00:00:38.460	And we also talked about different metrics for different tasks.
17.srt	00:00:38.519 --> 00:00:45.039	So for the regression trees, we use MSC, MAE, or RSS to split the node.
17.srt	00:00:45.519 --> 00:00:54.030	And for classification tasks, the tree uses Gini and entropy, or information gain sometimes, to split the node.
17.srt	00:00:55.799 --> 00:01:01.159	And in this video, we're going to talk about some usage in SKLang, how to fit the models.
17.srt	00:01:01.849 --> 00:01:10.109	some useful functions and we'll talk about hyperparameters of the decision trees that we need to pick values such that we minimize overfitting.
17.srt	00:01:11.989 --> 00:01:19.079	So here are some references that you can look at the document and they have useful stuff.
17.srt	00:01:20.159 --> 00:01:35.909	So we simply import decision tree regressor and classifier from the sklon tree module and for example if it was classification task we can construct a model by just simply calling this decision tree classifier.
17.srt	00:01:37.819 --> 00:01:48.379	and then fit the data, the features and the labels, and here are the snapshots from the document that shows that it has many many other options.
17.srt	00:01:49.980 --> 00:01:52.329	Alright, so we'll talk about some of them.
17.srt	00:01:53.280 --> 00:02:00.840	Another useful function that is also contained in that escapelontree module is the plot tree.
17.srt	00:02:10.770 --> 00:02:12.819	When we pass the fitted object to the plot tree function, it's going to return some list of text objects and then also the visualization of this.
17.srt	00:02:15.990 --> 00:02:22.879	We can also use export graphics function from sk1 tree to make a fancier visualization.
17.srt	00:02:22.990 --> 00:02:29.050	To do that we're gonna use graphics and some other modules and the usage will look like this.
17.srt	00:02:41.310 --> 00:02:41.949	We just pass this object fitted object and then it's going to convert this text object to a graph object and then the image function will create an image out of this.
17.srt	00:02:42.840 --> 00:02:44.300	So it will look like this.
17.srt	00:02:44.379 --> 00:02:54.590	So if you see more red and more blue it means that the node is more pure and if you see white node that means it's kind of 50-50 or very mixed there.
17.srt	00:02:55.310 --> 00:02:58.530	So it's a little bit fancier but essentially kind of the same.
17.srt	00:02:59.590 --> 00:03:06.039	Decision trees, while they are easy and useful to understand, they have some drawbacks.
17.srt	00:03:11.150 --> 00:03:13.060	They are very easy to overfit so we're gonna talk about some strategies to prevent overfitting.
17.srt	00:03:13.590 --> 00:03:16.789	So first strategy is stopping the tree to grow.
17.srt	00:03:17.509 --> 00:03:18.449	It's called all-stopping.
17.srt	00:03:18.449 --> 00:03:21.349	And second strategy is called pruning.
17.srt	00:03:21.349 --> 00:03:22.689	We'll talk about that later.
17.srt	00:03:22.689 --> 00:03:26.259	And another good strategy is ensembleing the trees.
17.srt	00:03:26.259 --> 00:03:31.810	All right, so how do we stop the tree grow only?
17.srt	00:03:31.849 --> 00:03:42.120	We have a bunch of hyper parameters listed here and we can pick some values such that we can stop the tree grow.
17.srt	00:03:42.750 --> 00:03:51.370	So for example, maxDepth will limit the tree, the depth of the tree, so that it can stop growing when it reaches certain depths.
17.srt	00:03:52.439 --> 00:04:01.519	And meanSampleSplit will make the node stop splitting when it has a less number of samples arrived in that node.
17.srt	00:04:02.789 --> 00:04:12.719	And meanSampleSleep also can stop tree grow further or node split further when it has a certain number of samples in the leaf node.
17.srt	00:04:13.120 --> 00:04:14.479	So they are kind of similar.
17.srt	00:04:15.409 --> 00:04:18.189	mean weight fraction lift are also similar.
17.srt	00:04:18.359 --> 00:04:26.379	It is a continuous version of mean samples lift, so instead of number of samples, we'll look for the weight fraction of the node.
17.srt	00:04:27.659 --> 00:04:41.009	And mean impurity decrease also stops splitting at that node if the impurity decrease from that node is negligible or less than certain number.
17.srt	00:04:46.569 --> 00:04:52.469	And max features also can help with the overfitting because it can make the model less flexible by looking at the less number of features when we have so many features.
17.srt	00:04:53.729 --> 00:05:02.859	And there are more design parameters in the sklearn implementation of decision trees, which you can also look at the documentation, but we'll focus on just a few.
17.srt	00:05:03.099 --> 00:05:07.829	So the most direct way to prevent overfitting in the decision tree is a max depth.
17.srt	00:05:08.099 --> 00:05:12.099	So by just limiting the depth, we can directly make the tree not grow.
17.srt	00:05:12.099 --> 00:05:16.339	And the minimum sample width is also very useful.
17.srt	00:05:17.019 --> 00:05:23.159	So the smaller the number of the sample of the leaf, that means the model is more flexible.
17.srt	00:05:23.509 --> 00:05:30.089	So if you want to make the model less flexible, so less overfit, then increase this number.
17.srt	00:05:30.899 --> 00:05:33.310	Another one to try is an impurity decrease.
17.srt	00:05:33.310 --> 00:05:38.489	However, you will have to know some values, so you will have to give some trial and error.
17.srt	00:05:40.859 --> 00:05:44.439	An impurity decrease is calculated as this one.
17.srt	00:05:44.439 --> 00:05:49.829	So when there are n samples in the parent node and it splits to an L and an R.
17.srt	00:05:50.839 --> 00:06:17.910	the impurity decrease or information gain is given by the impurity of the original node minus the weighted sum of the impurity of the children node so the weights will be the fraction of the sample numbers times the impurity of the left box and the weight of the right box times impurity of the right box.
17.srt	00:06:18.209 --> 00:06:19.730	So that's the impurity decrease.
17.srt	00:06:20.579 --> 00:06:22.590	So you will pick some value threshold.
17.srt	00:06:23.540 --> 00:06:25.050	and see what happens.
17.srt	00:06:26.660 --> 00:06:30.629	Other useful options that you can use when you build a model is the max features.
17.srt	00:06:30.829 --> 00:06:38.470	So it's going to limit the feature number and usually square root or log options are popular.
17.srt	00:06:38.470 --> 00:06:42.519	Square root is more popular by the way.
17.srt	00:06:43.160 --> 00:06:53.720	And class weight by default is none, but if you use a balance, it usually gives a better performance, especially true when you have imbalanced labels.
17.srt	00:06:55.889 --> 00:06:59.569	And CCP-alpha is used when you use minimal complexity pruning.
17.srt	00:06:59.569 --> 00:07:03.230	So we'll talk about this more in detail in the pruning video.
17.srt	00:07:03.230 --> 00:07:09.110	So how do we choose its hyperparameter values?
17.srt	00:07:09.310 --> 00:07:14.340	We might have some heuristic values or just try a few values.
17.srt	00:07:14.340 --> 00:07:18.830	However, we can also do a pragmatic approach like grid search.
17.srt	00:07:18.830 --> 00:07:24.220	Unfortunately, sklearn library also have a very convenient tool.
17.srt	00:07:25.089 --> 00:07:26.920	called gridSearchCV.
17.srt	00:07:27.680 --> 00:07:52.910	It does grid search as well as cross-validation so that it makes sure it's not just a one-pick value that was out of luck, but it does cross-validation, which will split the data into by default five chunks and then and it's going to fit the model and then get the accuracy from this chunk and this chunk and then it will average the result.
17.srt	00:07:55.920 --> 00:08:00.770	And it will give the result that which model hyperparameter gave the best result.
17.srt	00:08:01.410 --> 00:08:06.500	So from model selection module, we can call the gridSearchCV.
17.srt	00:08:06.780 --> 00:08:09.870	And this is individual decision tree classifier.
17.srt	00:08:09.900 --> 00:08:12.830	I just happen to call RF, but you can call whatever.
17.srt	00:08:13.050 --> 00:08:21.120	And then these parameters are dictionary that shows that which hyperparameters and which values you want to change to.
17.srt	00:08:21.490 --> 00:08:23.210	So I gave some different options.
17.srt	00:08:23.970 --> 00:08:25.800	And then...
17.srt	00:08:27.210 --> 00:08:41.170	I put these two objects in the grid search CVE and after fitting the grid search object with the data, we get some we can we can call the result by dot best estimator.
17.srt	00:08:41.170 --> 00:08:55.019	It will return what was the best estimator and gives the hyper parameter values here and dot best score will give the what was the accuracy value for the classification when we use these hyper parameters.
17.srt	00:08:56.389 --> 00:08:58.440	Alright so these are some handy tools.
17.srt	00:09:01.920 --> 00:09:09.639	So we showed how to use sklearn library for constructing decision trees and how to use grid search to find the hyperparameter values.
17.srt	00:09:10.889 --> 00:09:16.850	In the next video, we're going to talk about pruning the decision trees as a part of strategies of preventing overfitting.
16.srt	00:00:05.129 --> 00:00:10.500	Hey everyone, in this video we're going to talk about decision tree classifier and their split criteria.
16.srt	00:00:14.070 --> 00:00:18.789	So decision tree classifier look exactly like decision tree regressor.
16.srt	00:00:19.589 --> 00:00:25.250	This is a representation of the decision tree classifier in the HERT dataset.
16.srt	00:00:33.390 --> 00:00:34.369	It's binary class classification, so at the end of the day in the terminal node we'll have a few samples.
16.srt	00:00:34.630 --> 00:00:40.039	It seems like it has only one or two samples or just a few at each terminal node.
16.srt	00:00:40.619 --> 00:00:49.870	When we don't stop growing tree in the middle, it will just fully grow until it has a pure node, everything pure in the terminal node.
16.srt	00:00:51.019 --> 00:00:55.489	Alright, so if we zoom in some first a few nodes, it will look like this.
16.srt	00:01:03.429 --> 00:01:11.310	So like Decision Tree Regressor, it will pick a a criteria So which feature to split on and which feature value on that feature to split on.
16.srt	00:01:11.400 --> 00:01:20.099	So for example, out of these 13 features, it chose a thal and less than equal to the value of 4.5.
16.srt	00:01:20.099 --> 00:01:24.209	It will split whether it's true, satisfy this condition or not.
16.srt	00:01:24.209 --> 00:01:26.959	And then it will lead to children rules.
16.srt	00:01:33.570 --> 00:01:36.640	So then the next question we can ask is how does decision tree classifier pick this split criteria.
16.srt	00:01:38.760 --> 00:01:41.170	So it works very similar to decision tree regressor.
16.srt	00:01:41.300 --> 00:01:56.620	So in decision tree regressor, there was some samples in the original boxes and then we picked the criteria such that the splitted box, so true and false, we measure MSC here.
16.srt	00:01:57.250 --> 00:01:58.490	It doesn't have to be MSC.
16.srt	00:01:58.490 --> 00:02:00.200	It could be RSS or MAE.
16.srt	00:02:00.670 --> 00:02:04.890	So MSC of the left box and MSC of the right box.
16.srt	00:02:05.769 --> 00:02:12.030	we have different you know choices of how to split the box, the original box.
16.srt	00:02:12.159 --> 00:02:38.560	So we'll go through this is feature one, this is feature two, then it will try to split everything in every possible way like this and then measure the resulting left and right, left and right, left and right, and then pick the one, pick one split that actually gave the best result.
16.srt	00:02:38.560 --> 00:02:42.449	By best I mean the minimize the total MSC.
16.srt	00:02:44.030 --> 00:02:57.319	So, it similarly works that way, except that now the metric that we use to calculate this left box and right box result is Gini instead of MSE.
16.srt	00:02:57.979 --> 00:03:00.310	So Gini is a measure of impurity.
16.srt	00:03:00.969 --> 00:03:13.050	So decision tree classifier measures the impurity of the left box and then the impurity of the right box and then it will also inspect all the split possibilities like this.
16.srt	00:03:13.359 --> 00:03:18.519	every combination and it will pick the one that gives the minimum total impurity.
16.srt	00:03:20.389 --> 00:03:34.959	All right, okay, so again the regression decision tree regressor has MSC or RSS, same as RSS, and MAE as a metric that helps the finding split criteria.
16.srt	00:03:36.479 --> 00:03:38.889	And for classification, we have three choices.
16.srt	00:03:38.889 --> 00:03:43.229	It could be more but you know these three are most popular.
16.srt	00:03:43.259 --> 00:03:45.539	So Gini is a measure of impurity.
16.srt	00:03:46.189 --> 00:03:48.430	and it looks like this.
16.srt	00:03:49.049 --> 00:04:05.639	Actually when you look at the RSS which is a measure of variance of that box, Gini is somewhat similar because when you just think about coin flip problem, this is a variance of the Bernoulli probability distribution function.
16.srt	00:04:05.639 --> 00:04:15.289	So Gini somehow it's measuring the variance like RSS does or MSC does, but Gini is a measure of impurity.
16.srt	00:04:16.250 --> 00:04:20.649	So I just wanted to mention that they have some similarity.
16.srt	00:04:21.529 --> 00:04:24.009	An entropy is a measure of uncertainty.
16.srt	00:04:24.680 --> 00:04:35.689	So uncertainty in the information theory means that when there is a packet, we don't know the value of the packet, whether it's a 0 or 1.
16.srt	00:04:35.939 --> 00:04:41.490	If we don't know fully, then the uncertainty is maximized, therefore the entropy is maximized.
16.srt	00:04:52.319 --> 00:04:56.879	However if we have certain information that maybe 80% of chance that it's a 0 and maybe 20% of chance it's 1, then we have certain information and less uncertainty than 50-50 chance.
16.srt	00:04:56.879 --> 00:05:03.050	So the entropy measures that and the formula looks like this.
16.srt	00:05:03.779 --> 00:05:15.459	So there is minus sign over here and the information gain is a difference in the entropy of the one parent node and it's a binary split the children node.
16.srt	00:05:15.500 --> 00:05:24.949	So some of these two entropies left box and right box or it could have some weight as well if they have different number of samples.
16.srt	00:05:25.129 --> 00:05:26.750	So we'll see them in more detail.
16.srt	00:05:31.150 --> 00:05:40.960	Alright, so Gini and entropy, they measure the same kind of property, purity or impurity or uncertainty, they are similar concepts.
16.srt	00:05:41.500 --> 00:05:56.949	So you can kind of intuitively think about this case, you have some bag and if everything is kind of fully mixed, you have blue marble and red marble kind of mixed 50-50 like this.
16.srt	00:06:30.680 --> 00:06:33.920	and our goal is to separate them to everything pure in the two small bags and if everything is perfectly separated we'll have all blues in one bag and all red in the other bag so we have full visibility and we are like 100% sure that which one is which.
16.srt	00:06:34.660 --> 00:06:55.340	If they are not separated well enough, then maybe they will have 80% of blue marbles in one bag and 20% of red marble in the same bag and the other one has 80% of red marbles and then 20% of blue marble.
16.srt	00:07:01.080 --> 00:07:04.670	And if the separation was bad, then everything will be just 50-50.
16.srt	00:07:07.200 --> 00:07:08.090	Something like this.
16.srt	00:07:11.820 --> 00:07:14.150	So there is no useful information in this case.
16.srt	00:07:14.240 --> 00:07:16.430	We didn't gain any information.
16.srt	00:07:16.890 --> 00:07:22.360	Everything is just mixed together so we don't know which bag contains which color.
16.srt	00:07:23.610 --> 00:07:28.120	However, there are certain information here.
16.srt	00:07:28.220 --> 00:07:31.900	We have some certainty and this is...
16.srt	00:07:32.420 --> 00:07:35.030	100% certain that we know which one is which.
16.srt	00:07:37.990 --> 00:07:50.620	So this is pure and this is, we can say maximally impure and somewhat impure.
16.srt	00:07:52.769 --> 00:07:55.540	And this is uncertain.
16.srt	00:07:57.870 --> 00:07:58.710	This is certain.
16.srt	00:08:05.000 --> 00:08:11.110	So our goal is to make the node split such that the splitted nodes are as pure as possible.
16.srt	00:08:11.710 --> 00:08:29.629	So to do that, we're gonna use Gini and entropy as a metric and the decision tree split algorithm will inspect every possible split along one feature and will also scan every features and will pick the split criteria.
16.srt	00:08:29.629 --> 00:08:36.879	Alright, so split criteria using Gini index.
16.srt	00:08:37.259 --> 00:08:39.450	So Gini function look like this.
16.srt	00:08:40.269 --> 00:08:43.419	So in the binary case, it's symmetric.
16.srt	00:08:44.190 --> 00:08:55.820	So we can draw the function like this and it's a maximum and 50-50 mixture and then it's a zero at the pure node.
16.srt	00:09:00.019 --> 00:09:01.909	So let's have some example.
16.srt	00:09:02.029 --> 00:09:04.710	Let's practice calculating Gini.
16.srt	00:09:04.710 --> 00:09:07.070	So what's the Gini of this entire box?
16.srt	00:09:09.850 --> 00:09:15.670	Well, as a burr puck, it's a fully mixed five cats and five tigers.
16.srt	00:09:16.269 --> 00:09:19.830	So the genie would be one half.
16.srt	00:09:20.710 --> 00:09:25.710	So genie for the binary case, one half is the maximum value.
16.srt	00:09:28.269 --> 00:09:29.450	We can calculate the two.
16.srt	00:09:29.450 --> 00:09:35.850	So in this box, five out of ten sample is a cat.
16.srt	00:09:35.850 --> 00:09:37.529	So one half.
16.srt	00:09:37.800 --> 00:09:40.490	times 1 minus 1 half is 1 half as well.
16.srt	00:09:41.019 --> 00:09:52.409	Plus, when you switch the label as a tiger, the probability of having tiger in this box is 1 half, same as 1 minus 1 half.
16.srt	00:09:52.970 --> 00:09:55.480	So altogether it's going to be 1 half.
16.srt	00:09:56.490 --> 00:09:58.560	So that's the genio of this entire box.
16.srt	00:09:59.360 --> 00:10:02.370	Now, let's say we had some split like this.
16.srt	00:10:03.360 --> 00:10:05.480	And what is the genio of this left box?
16.srt	00:10:06.840 --> 00:10:08.889	Well, the left box is pure.
16.srt	00:10:09.029 --> 00:10:10.180	Everything is cat.
16.srt	00:10:11.300 --> 00:10:16.350	So we're gonna have 0, Ginny, in the left box.
16.srt	00:10:16.910 --> 00:10:32.200	In the right box, we have a 1 cat out of 6 samples, and the 5 are tigers, so the probability of cat is 1 6, and the probability of tiger is 5 6.
16.srt	00:10:33.630 --> 00:10:40.720	So according to this formula, it's symmetric, so 1 6 times 1 minus 1 6 is 5 6.
16.srt	00:11:11.250 --> 00:11:21.570	plus 5 6 1 minus of that is 1 6 again so it's going to be 5 18th so that's the genie of the right box all right so let's talk about entropy so entropy is again measure of uncertainty if it's a 50-50 mixture in the binary class it's the most uncertain and This is when we use a natural log, but it's also common to use a base of 2, log2, when we have a binary class classification.
16.srt	00:11:22.130 --> 00:11:27.270	Actually, you can use anything like log of base 2 or 10 or natural log.
16.srt	00:11:28.430 --> 00:11:32.130	Doesn't matter, but I think the natural log is the most popular choice.
16.srt	00:11:33.270 --> 00:11:42.400	But anyway, if we use log base of 2 when the binary class classification, the maximum entropy value becomes 1.
16.srt	00:11:42.790 --> 00:11:47.290	but in this case because I used the natural log, it's some weird value here.
16.srt	00:11:47.680 --> 00:11:58.220	But anyway, that is the same as that it's the max at 50-50 mixture and it's zero at pure nodes.
16.srt	00:11:58.220 --> 00:12:04.820	All right, and information gain is a reduction in entropy.
16.srt	00:12:13.830 --> 00:12:18.910	So reduction means the entropy of the original box and minus the summed or total entropy of the split as a result of the split.
16.srt	00:12:20.530 --> 00:12:21.400	So let's have a look.
16.srt	00:12:21.400 --> 00:12:26.750	So total entropy of unsplited box is 1.
16.srt	00:12:26.940 --> 00:12:32.790	When we use the log base 2, everything is like 50-50 so we get entropy value of 1.
16.srt	00:12:33.450 --> 00:12:36.500	And let's say we split to this part again.
16.srt	00:12:37.180 --> 00:12:43.780	So the left would be 0 because everything is pure as a cat and the right box would be non-zero.
16.srt	00:13:14.730 --> 00:13:16.720	again and we use the formula then we get this so 1 6 is the probability of having cat and 5 6 is the probability of having tiger so when we calculate that the entropy of the right box is 0.65 so information gain in this case would be the original entropy which is 1 and then minus the weighted sum of those left box and right boxes.
16.srt	00:13:18.050 --> 00:13:23.500	So for left box, so we give the weight as a number of fraction of the sample.
16.srt	00:13:24.040 --> 00:13:28.710	So originally there were 10 samples, but then the left box only has a 4.
16.srt	00:13:29.250 --> 00:13:33.350	So the fraction or the weight of the left box would be 0.4.
16.srt	00:13:33.670 --> 00:13:41.320	And then the entropy of the left box is 0, so times 0 here.
16.srt	00:13:41.320 --> 00:13:41.640	Minus...
16.srt	00:13:44.430 --> 00:14:03.820	the fraction or weight of the right box is 0.6 and then the entropy of the right box is 0.65. so have this and when you calculate the information gain then we get 0.61. okay as an exercise let's have a look at this example.
16.srt	00:14:03.820 --> 00:14:11.540	so you are asked to find the width split among these three choices gives the maximum information gain.
16.srt	00:14:11.540 --> 00:14:16.590	so first choice is red split.
16.srt	00:14:17.860 --> 00:14:19.850	it will give this split.
16.srt	00:14:20.470 --> 00:14:27.759	The second choice would be green split and the third choice would be the blue split.
16.srt	00:14:28.330 --> 00:14:31.280	So which one do you think will give the maximum information gain?
16.srt	00:14:32.250 --> 00:14:35.240	You can use eyeball or you can calculate it.
16.srt	00:14:51.860 --> 00:15:02.560	Alright the answer is red and the hand waving way to explaining is that it's going to give the most number in the pure node in the left box and then also the right box is the most pure So for the red split, left box and right box, the probability of having cat is 1 and the probability of having tiger is 0, so it's very pure.
16.srt	00:15:03.100 --> 00:15:12.110	For the right box, the probability of having cat is 1, 6 and the probability of having tiger is 5, 6.
16.srt	00:15:22.520 --> 00:15:44.460	For the green split, we had one tiger in the left box and then So it's 1, 0, and then the impure box give not much gain in terms of information because you know 4 out of 9 is tiger and 5 out of 9 is cat.
16.srt	00:15:55.100 --> 00:16:02.440	So before the split they were 50-50 so 10 samples 5 cat 5 tigers so it was 5 out of 10, 5 out of 10, so in terms of changes it didn't change much.
16.srt	00:16:03.780 --> 00:16:08.260	And then the number of a pure sample is only one here.
16.srt	00:16:09.230 --> 00:16:21.840	For the blue, it has a three tigers versus everything else, so in the pure node the probability is like this, but only had the three numbers in the sample.
16.srt	00:16:22.040 --> 00:16:24.060	This is 4, this is 1.
16.srt	00:16:25.200 --> 00:16:33.510	And as a result, out of 7, 2 are tigers and 5 of them are cats.
16.srt	00:16:35.150 --> 00:16:38.510	So when you just eyeball these numbers, what do you think?
16.srt	00:16:39.610 --> 00:16:52.270	It seems like the red split gave the most pure result on the pure node and also more pure among these 3 choices.
16.srt	00:16:53.760 --> 00:16:56.870	We can be more quantitative and use an entropy formula.
16.srt	00:16:57.560 --> 00:17:24.590	So using entropy formula, we can do the red is going to be 1 minus 0.4 times 0 minus 0.6 times minus 1 times 1 6 log 1 6 plus 5 6 times log 5 6.
16.srt	00:18:03.020 --> 00:18:14.780	And for green, 1 minus 0.1 times 0 minus 0.9 times 1 times 59 log 59 plus 49 times log 49 and for blue minus 0.3 times 0 minus 0.3 7 times minus 1 times 2 sevenths log 2 sevenths plus 5 sevenths plus log 5 sevenths.
16.srt	00:18:15.880 --> 00:18:29.340	Well so yeah if we calculate that with the calculator we're gonna get a result like so this one gives a reduction of 0.61 and this one gives a reduction of 0.11 only.
16.srt	00:18:35.690 --> 00:18:36.780	It's very small reduction and then for blue it's gonna give a reduction of approximately 0.4.
16.srt	00:18:36.780 --> 00:18:44.070	So yeah, you can see quantitatively that this one gave the most reduction in entropy.
16.srt	00:18:44.070 --> 00:18:46.660	So most information gained too.
16.srt	00:18:46.660 --> 00:19:01.290	Alright, so that was it for the practice and as a summary, we talked about different metrics for regression task and classification task in the decision tree split.
16.srt	00:19:01.290 --> 00:19:05.670	So for the decision tree regressor, use MSC or RSS.
16.srt	00:19:07.750 --> 00:19:11.540	or MAE to choose the split character.
16.srt	00:19:11.540 --> 00:19:17.730	And for decision tree classifier, it uses a Gini, Entropy, and Information Gain.
16.srt	00:19:18.070 --> 00:19:22.200	And we talked about what their formula is and how to calculate them.
16.srt	00:19:22.990 --> 00:19:28.260	Alright, so in the next video, we're going to talk about how to prune the decision tree.
14.srt	00:00:00.530 --> 00:00:10.710	this error will optimize the parameter values so that this prediction value will be as close as possible to the target value.
14.srt	00:00:13.710 --> 00:00:17.089	In non-parametric models, the parameter doesn't exist.
14.srt	00:00:17.089 --> 00:00:26.949	Therefore, the question is, well, how do we optimize the model such that this prediction value gets as close as possible to the target value?
14.srt	00:00:26.949 --> 00:00:29.780	The model has hyperparameters.
14.srt	00:00:31.290 --> 00:00:31.700	usually.
14.srt	00:00:31.700 --> 00:00:34.789	They may not have, but usually they should have.
14.srt	00:00:35.329 --> 00:00:45.160	And then these non-parametric models sometimes use this error or sometimes they don't, but uses some other quantity to optimize the model.
14.srt	00:00:45.160 --> 00:00:46.789	So we'll get to that.
14.srt	00:00:46.789 --> 00:00:57.480	So examples of non-parametric models are KNN, K-nearest neighbor, which is the simplest machine learning algorithm, and then decision trees.
14.srt	00:01:01.500 --> 00:01:03.689	that uses a tree-like model.
14.srt	00:01:03.850 --> 00:01:05.540	We'll get to that later.
14.srt	00:01:05.540 --> 00:01:13.769	And support vector machine which uses distance between the points and the decision boundary or hyperplane.
14.srt	00:01:13.890 --> 00:01:18.640	So k-nearest neighbor works like this.
14.srt	00:01:18.640 --> 00:01:21.700	So imagine I have training data that looks like this.
14.srt	00:01:21.700 --> 00:01:28.810	Red dots and blue dots and the task is to classify my data points whether it's red or blue.
14.srt	00:01:28.810 --> 00:01:34.740	And let's say I have data points to classify here, and I don't know whether it's red or blue.
14.srt	00:01:36.280 --> 00:01:43.969	And k-nearest neighbors says that just take k numbers of nearest neighbors and classify to the majority of them.
14.srt	00:01:43.969 --> 00:01:56.150	So let's say if I have k equals 1, I take the closest one, I take one nearest neighbor which is red, so my green point is going to be red in this case.
14.srt	00:01:56.150 --> 00:02:07.420	If I said, if I had three neighbors, then I have two blues and one red, therefore my green points will be classified as 2.
14.srt	00:02:08.219 --> 00:02:10.180	by the majority rule, a voting rule.
14.srt	00:02:11.189 --> 00:02:23.349	If I had 5, if I had a k equals 5, then now I have a 3 red neighbors and 2 blue neighbors, so it's going to be classified as red now.
14.srt	00:02:24.640 --> 00:02:26.110	You might have noticed two things.
14.srt	00:02:26.550 --> 00:02:40.250	First, this green point kind of flips between red and blue, and second, the choice of k number is odd number, why is that?
14.srt	00:02:41.120 --> 00:02:43.759	Because If I have an even number, then I might have tie.
14.srt	00:02:44.740 --> 00:02:49.719	I might have just two red and two blues, and I don't know what to choose then.
14.srt	00:02:50.310 --> 00:02:55.590	So that's why we usually use odd number for the k values for KNN model.
14.srt	00:02:57.170 --> 00:03:02.449	Another thing you might have noticed is that why is this green swing between red and blue?
14.srt	00:03:03.300 --> 00:03:09.530	It is just happened to be that this green sits on the decision boundary.
14.srt	00:03:15.990 --> 00:03:18.770	For example, let's say this side is red and this side is blue and this green just at the right in between so it can swing.
14.srt	00:03:19.939 --> 00:03:21.560	But that's not very important.
14.srt	00:03:21.700 --> 00:03:23.670	I just wanted to show it can happen.
14.srt	00:03:24.450 --> 00:03:33.210	And another question you might ask is that can KNN do other than classification?
14.srt	00:03:34.030 --> 00:03:34.569	Yes, it can.
14.srt	00:03:34.569 --> 00:03:37.189	You can also do the regression.
14.srt	00:03:37.670 --> 00:03:48.120	The difference would be that instead of taking the majority rule here, if it's a regression, it's going to take the average of these five values for example when the k equals five.
14.srt	00:03:49.950 --> 00:03:54.520	Kaelin uses distance metric, for example, Manhattan distance and Euclidean distance.
14.srt	00:03:54.810 --> 00:04:05.470	Euclidean distance is a simple distance between these two points, whereas Manhattan distance would be the delta x plus delta y, for example.
14.srt	00:04:05.900 --> 00:04:09.900	There are more distance metrics that you can use, but these two are pretty popular.
14.srt	00:04:13.420 --> 00:04:15.490	So let's have an example.
14.srt	00:04:15.840 --> 00:04:18.750	This is from famous iris dataset.
14.srt	00:04:20.700 --> 00:04:26.460	To display more conveniently, I only use two features and then two classes of iris.
14.srt	00:04:26.460 --> 00:04:35.300	So you can see some blue points and red points are kind of mixed in some area.
14.srt	00:04:35.680 --> 00:04:37.020	So it's hard to separate.
14.srt	00:04:37.940 --> 00:04:43.320	So this two graph shows that the decision boundary, I can model.
14.srt	00:04:43.430 --> 00:04:47.530	In each case, k values are different.
14.srt	00:04:47.930 --> 00:04:49.670	And now I have a question for you.
14.srt	00:04:49.750 --> 00:04:52.220	Which of these k's have a smaller k number?
14.srt	00:04:54.870 --> 00:04:56.470	Okay, we'll see the answer here.
14.srt	00:04:56.680 --> 00:05:00.389	The answer was the left one has a smaller k value.
14.srt	00:05:00.460 --> 00:05:02.389	In fact, it was k cos 1.
14.srt	00:05:03.389 --> 00:05:09.009	And as you can see here, as the k increases, the decision boundary becomes smoother and smoother.
14.srt	00:05:10.389 --> 00:05:16.689	When the k is small, let's say 1, then I only have to consider just one neighbor.
14.srt	00:05:16.689 --> 00:05:24.220	So if my data points here, I only consider this one, and the next time my data points here, then I consider this one.
14.srt	00:05:24.689 --> 00:05:28.080	Therefore, the decision boundary can be very granular.
14.srt	00:05:28.670 --> 00:05:31.810	Therefore, it can fit to the very complex data like this.
14.srt	00:05:32.830 --> 00:05:47.580	Whereas if I have to consider many neighbors, when I'm here, I will have to consider 61 neighbors and then count red versus blue and decide which one is more dominant here, in this case red.
14.srt	00:05:47.580 --> 00:05:57.810	So the decision boundary can be very smooth in this way because I'm kind of averaging out a lot of data points.
14.srt	00:05:58.990 --> 00:06:03.600	All right, so this might remind you the concept of bias and variance.
14.srt	00:06:03.959 --> 00:06:06.589	So how's the bias and variance in K and N?
14.srt	00:06:08.430 --> 00:06:09.500	So here are some quiz.
14.srt	00:06:09.850 --> 00:06:11.459	Which model has a larger bias?
14.srt	00:06:12.600 --> 00:06:14.930	When the K is small or when the K is large?
14.srt	00:06:17.430 --> 00:06:21.160	The answer is when we have a larger K, we have a larger bias.
14.srt	00:06:21.730 --> 00:06:22.360	Why is that?
14.srt	00:06:22.360 --> 00:06:29.269	Because a K and N model with the larger K is a simpler model and it's less flexible.
14.srt	00:06:29.600 --> 00:06:36.470	you saw in the previous slides that the decision boundaries are much smoother for when K is larger.
14.srt	00:06:38.009 --> 00:06:50.019	The simpler model, which is a less flexible model, has a larger bias because it simplifies the real-world data, therefore it introduces more bias and more assumption about data.
14.srt	00:06:50.019 --> 00:06:53.470	Alright, another question.
14.srt	00:06:53.470 --> 00:07:00.840	Which model has a larger variance when the K is small or when the K is large?
14.srt	00:07:03.780 --> 00:07:13.710	So larger variance happens when the model is more flexible, therefore we can guess that the small k, k and n should have a larger variance.
14.srt	00:07:17.290 --> 00:07:19.850	So how do we determine the optimal k value?
14.srt	00:07:20.490 --> 00:07:36.380	As you saw previously that the training error goes down as the model complexity increases, test error goes down in the beginning but then it has an optimal value and it goes up again as the model complexity increases.
14.srt	00:07:36.590 --> 00:07:41.430	because the model is too complex to the data, therefore it's overfitting.
14.srt	00:07:41.500 --> 00:07:43.110	It's not generalizing very well.
14.srt	00:07:43.110 --> 00:07:45.720	So that point happens here.
14.srt	00:07:45.780 --> 00:07:59.250	So around k equals 21, the optimal value happens and the test error is minimized, whereas it can go up if we keep increasing the model complexity.
14.srt	00:08:00.460 --> 00:08:03.260	So you can see that this side is more complex model.
14.srt	00:08:07.920 --> 00:08:26.640	When the k value gets smaller, the model gets more complex, more flexible, and the other side becomes simpler and has a larger bias, larger variance.
14.srt	00:08:30.420 --> 00:08:36.020	So that's the relationship between the k and then, k of the k and n and the bias and variance.
14.srt	00:08:37.470 --> 00:08:37.810	All right.
14.srt	00:08:39.360 --> 00:08:40.830	So more KNN properties.
14.srt	00:08:41.549 --> 00:08:44.830	As we saw, it's a simple and memory-based algorithm.
14.srt	00:08:44.830 --> 00:08:51.210	Memory-based means that it just needs all the training data in order to inference.
14.srt	00:08:51.210 --> 00:08:58.690	And its time complexity is the order of number of samples times the number of features.
14.srt	00:08:58.690 --> 00:09:05.190	There can be K here as well, but if you had to rank K neighbors anyway, then there are some clever algorithms.
14.srt	00:09:09.060 --> 00:09:33.009	It's not, when you measure the time actually, it doesn't go very linearly because there are some better sorting algorithms, but anyway Time complexity is roughly n times m where this is number of samples and this is number of features and we can just support that by doing some, a few experiments.
14.srt	00:09:39.529 --> 00:09:45.129	So this data comes from OpenML and this has a 90 More than 90,000 samples with the 100 features and the task is to classify binary class.
14.srt	00:09:45.129 --> 00:09:54.919	So at k equals 9, which was the optimal value, the train time for k and n linearly increases as the number of samples increases.
14.srt	00:09:54.919 --> 00:09:59.590	Another data supports that as well.
14.srt	00:09:59.590 --> 00:10:08.049	Instead of increasing the number of samples, this time by increasing number of features, the train time also goes linearly.
14.srt	00:10:08.049 --> 00:10:11.269	So this data was...
14.srt	00:10:14.370 --> 00:10:21.929	classifying three different boundary types of the gene sequences and has 180 binary features.
14.srt	00:10:26.579 --> 00:10:33.349	Training the logistic regression on the same data set and measuring the training time can give some comparison.
14.srt	00:10:33.389 --> 00:10:38.509	Surprisingly, this KNN model is very efficient.
14.srt	00:10:40.039 --> 00:10:43.409	Well, it is usually said that KNN is slow.
14.srt	00:10:43.649 --> 00:10:48.909	because it has to measure all the distance between the points in the training data set.
14.srt	00:10:48.909 --> 00:10:55.409	So it's said to be slow, but with this number of samples, it's not terribly bad.
14.srt	00:10:55.509 --> 00:11:01.100	It has a training time very small compared to the logistic regression.
14.srt	00:11:01.549 --> 00:11:03.220	It's surprisingly fast.
14.srt	00:11:04.049 --> 00:11:12.649	And logistic regression might be slow just because it uses a fancy second derivative optimization algorithm that can run many times as well.
14.srt	00:11:16.189 --> 00:11:20.529	This graph shows that the there is an optimal k-value for the KNN model.
14.srt	00:11:20.529 --> 00:11:25.529	The test accuracy has some optimal value at certain k-value, which is 7 in this case.
14.srt	00:11:25.529 --> 00:11:35.539	So another property that KNN has is that it suffers severely from curse of dimensionality.
14.srt	00:11:35.539 --> 00:11:38.339	What is the curse of dimensionality?
14.srt	00:11:38.339 --> 00:11:45.139	Curse of dimensionality is that the model performs very poorly when we have a lot of features.
14.srt	00:11:46.689 --> 00:11:49.370	so that there is a curse when the dimension is high.
14.srt	00:11:50.399 --> 00:11:52.529	To see that, we're going to do some experiments.
14.srt	00:11:52.970 --> 00:11:57.230	I just plotted explained variance ratio from PCA.
14.srt	00:11:58.679 --> 00:12:11.569	By transforming our 180 features using principal component analysis, it's going to rank the combination of these 180 features in order of importance.
14.srt	00:12:12.539 --> 00:12:16.159	That is called explained variance ratio.
14.srt	00:12:17.159 --> 00:12:29.539	And this gradual increase of this explained variance ratio tells that a lot of these features are all important.
14.srt	00:12:29.759 --> 00:12:38.659	If only a few of these features were important, then this explained variance ratio graph would look like this.
14.srt	00:12:39.870 --> 00:12:48.409	Like very sharply increase up until point, that means more than 90% of variance would be explained by just only few features.
14.srt	00:12:49.340 --> 00:12:53.549	However, this graph shows that it gradually increases.
14.srt	00:12:53.549 --> 00:12:57.740	That means all features are kind of important.
14.srt	00:12:58.440 --> 00:13:03.149	So with that in mind, let's have a look and compare with the logistic regression.
14.srt	00:13:03.840 --> 00:13:17.320	So because most of these features are important, in logistic regression you can see that the test accuracy still increases as the number of features increases.
14.srt	00:13:23.559 --> 00:13:24.940	However, in KNN, as you can see, with the various values of K, for the various k values.
14.srt	00:13:25.039 --> 00:13:32.860	It has some peak value at very small dimension of features and then it sharply decrease the performance.
14.srt	00:13:33.289 --> 00:13:35.350	So that's the curse of high dimensionality.
14.srt	00:13:37.080 --> 00:13:38.049	So let's fix it.
14.srt	00:13:38.129 --> 00:13:48.230	Our optimal value k equals 7 and as you can see the test accuracy dropped very sharply as we increase the number of features that are included in the model.
14.srt	00:13:52.639 --> 00:13:55.700	So why does curse of dimensionality happen here?
14.srt	00:13:56.759 --> 00:14:08.789	It happens because intuitively the number of data points in the given volume of this height dimension sharply decreases when this dimension becomes high.
14.srt	00:14:09.740 --> 00:14:14.309	Therefore, we need more data points in order to have the same level of accuracy.
14.srt	00:14:15.479 --> 00:14:23.829	However, with the fixed data size, the concentration of data decreases dramatically.
14.srt	00:14:23.829 --> 00:14:29.899	Therefore, we have degradation of performance in accuracy when the dimension is too high.
14.srt	00:14:31.669 --> 00:14:33.350	But it's not that simple.
14.srt	00:14:33.450 --> 00:14:45.399	Researchers have found that if the features are highly correlated to each other, it may suffer less because the effective dimension is less than the number of features.
14.srt	00:14:46.190 --> 00:14:49.450	But anyway, still, K-NN suffers from curse of dimensionality.
14.srt	00:14:49.450 --> 00:14:58.210	So when this happens, you want to use smaller number of features and avoid from being high dimension.
14.srt	00:15:01.169 --> 00:15:02.500	when you are using KNN.
14.srt	00:15:05.409 --> 00:15:14.759	Also, not only the KNN, other machine learning models that use the distance metric in their algorithm can suffer from curse of dimensionality.
14.srt	00:15:14.759 --> 00:15:24.149	So you might choose wisely which model to use when your dimension is too high, unless you can or you want to reduce the number of features.
14.srt	00:15:33.000 --> 00:15:49.039	Alright, so in this video we talked about KNN as an example of non-parametric model, which is the simplest machine learning model and we talked about its property, its bias variance, its hyperparameter k, and how it behaves when the k increases or k decreases, and its properties such as curse of dimensionality.
14.srt	00:15:50.750 --> 00:15:55.219	We'll talk about more sophisticated models, non-parametric models, in the next videos.
15.srt	00:00:05.629 --> 00:00:06.049	All right.
15.srt	00:00:06.339 --> 00:00:07.049	Hey everyone.
15.srt	00:00:07.049 --> 00:00:09.449	In this video, we're going to talk about decision trees.
15.srt	00:00:12.050 --> 00:00:25.690	So, so far we talked about some examples of parametric models such as linear regression and logistic regression, which they have parameters or coefficients inside, and we used different metrics to optimize those.
15.srt	00:00:26.170 --> 00:00:32.219	And then we had the KNN for example of non-parametric model, which does not have parameter inside.
15.srt	00:00:32.219 --> 00:00:33.929	However, we use this metric.
15.srt	00:00:35.140 --> 00:00:36.140	to make a decision.
15.srt	00:00:36.929 --> 00:00:42.210	And decision tree is another non-parametric method which is a little bit more complex than KNN.
15.srt	00:00:42.210 --> 00:00:44.700	So let's have a look.
15.srt	00:00:44.700 --> 00:00:46.929	So what is a decision tree?
15.srt	00:00:46.929 --> 00:00:48.549	Let's take an example.
15.srt	00:00:48.549 --> 00:00:55.950	These are the photos of two different kinds of mushroom and one of them is edible and the other one is deadly poisonous.
15.srt	00:00:55.950 --> 00:00:58.439	So which one do you think is edible?
15.srt	00:00:58.439 --> 00:01:04.400	It is difficult to tell because they look very similar.
15.srt	00:01:05.749 --> 00:01:10.419	And in fact, the upper one is edible and the lower one is called a death cat.
15.srt	00:01:12.929 --> 00:01:14.759	The decision tree may look like this.
15.srt	00:01:15.929 --> 00:01:24.789	Let's say we have different samples of mushroom data and then from this first node it's asking some criteria whether it's large or not.
15.srt	00:01:25.299 --> 00:01:31.549	So let's say this one is large and then classify to large equals yes.
15.srt	00:01:36.099 --> 00:01:38.799	This one as well and these two are not large so we'll arrive to this node.
15.srt	00:01:38.980 --> 00:01:41.849	Let's say we call this node 2 and this node 3.
15.srt	00:01:42.739 --> 00:01:50.090	Alright from the node 2 there is another criteria or ask questions whether it's yellow or not.
15.srt	00:01:50.840 --> 00:01:59.719	So this one is not yellow so end up the edible and this one is yellow so therefore it's poisonous.
15.srt	00:02:06.189 --> 00:02:10.159	From the node 3 both of them are spotted so we'll go to node4 asking whether they have fall smell or not.
15.srt	00:02:10.159 --> 00:02:18.949	Let's say this one had a fall smell therefore it's poisonous and this one did not so therefore it is edible.
15.srt	00:02:19.530 --> 00:02:22.050	So decision tree works like this.
15.srt	00:02:22.050 --> 00:02:27.670	It splits the samples from each node depending on their criteria.
15.srt	00:02:27.670 --> 00:02:33.230	Let's talk about some terminology here.
15.srt	00:02:33.330 --> 00:02:39.340	So node at the top is called the root node and contains all the samples to begin with.
15.srt	00:02:41.740 --> 00:02:55.360	And then as the samples travel through these different node splits, when it arrives at the terminal nodes that doesn't split anymore, those nodes are called the leaf nodes and they are highlighted as green here.
15.srt	00:02:56.840 --> 00:03:06.840	And all other nodes between, they're called intermediate nodes and including root node, they also have decision criteria.
15.srt	00:03:06.930 --> 00:03:08.270	Therefore they are decision nodes.
15.srt	00:03:10.340 --> 00:03:12.870	So how the model learns to make a decision?
15.srt	00:03:18.549 --> 00:03:21.689	So as we mentioned, linear regression minimizes the MSE to learn to make a decision by optimizing their parameter values.
15.srt	00:03:22.599 --> 00:03:27.709	Same goes for logistic regression except that the criteria is now cross entropy.
15.srt	00:03:29.030 --> 00:03:35.169	KNN has no parameters, therefore no optimization, however uses a distance metric to make a decision.
15.srt	00:03:36.199 --> 00:03:47.099	Decision tree similarly does not have parameters, however uses other metrics such as MSA for regression task and entropy or Gini for classification task.
15.srt	00:03:47.870 --> 00:03:50.759	And they split nodes as we've seen before.
15.srt	00:03:52.809 --> 00:03:55.519	So decision tree regressor works like this.
15.srt	00:03:56.379 --> 00:04:06.269	So the goal is to split the samples into two boxes such that the MSA is minimized as a result of this split.
15.srt	00:04:07.739 --> 00:04:15.169	So let's say I have different options to split among these different features.
15.srt	00:04:15.569 --> 00:04:19.719	Let's say I have data that has two features only and six data points.
15.srt	00:04:22.079 --> 00:04:30.999	And then I want to split this into two boxes and I don't know how yet that will minimize the total sum of MSE.
15.srt	00:04:31.969 --> 00:04:40.509	So I have a choice of splitting along x1 or I have a choice of splitting along x2.
15.srt	00:04:41.579 --> 00:04:51.919	So another choice that we should make is that okay let's say I chose x2 to split then which value of x2 should I split?
15.srt	00:04:52.399 --> 00:04:57.739	Should I split here or here, here or here?
15.srt	00:04:59.369 --> 00:05:06.179	So these all decisions will be made by looking at the MSC of each split.
15.srt	00:05:09.189 --> 00:05:13.989	So again, we have different choices for making splits along X1.
15.srt	00:05:57.319 --> 00:06:06.739	For example, I can split this way and maybe left and right and measure the MSC and let's say this split criteria was A, then I can see if split by this split criteria and measure the MSC here and then sum them up and this total MSC I record it and then now I'm gonna move different split criteria so let's say this is called B then X1 is less than or equal to B And then I'm gonna measure this MSE for left and right boxes and then record the total MSE and I keep this procedure.
15.srt	00:06:07.379 --> 00:06:18.659	Let's say this one is C, D, E. Then I have five different split criteria along X1 feature.
15.srt	00:06:21.099 --> 00:06:23.089	Then I also record this MSE.
15.srt	00:06:27.969 --> 00:06:40.769	So that's for X1 and I can do the same for x2 so again it also have five different split criteria and then we can call it like big A, big B, C, D, something like that along the feature x2.
15.srt	00:06:42.269 --> 00:06:55.349	So as a result we have ten different values for MSC as a result of ten different split options and what we want to do is to inspect this MSC values and then pick the one that makes the minimize MSC.
15.srt	00:06:59.369 --> 00:07:07.709	So let's say this split criterion gave the smallest MSE among these 10 different choices, then now it becomes my split criterion for my root node.
15.srt	00:07:08.739 --> 00:07:12.649	So by root node, I mean the first box that we're given.
15.srt	00:07:14.259 --> 00:07:15.689	So this is my root node.
15.srt	00:07:16.959 --> 00:07:29.199	And then as we just saw, let's say this was the best split that minimized MSE, then now these two boxes becomes the split node.
15.srt	00:07:29.329 --> 00:07:33.569	So this node at the root node had six data points, and now we have...
15.srt	00:07:34.439 --> 00:07:58.009	split into two boxes left and right and each of them has three samples and the decision criteria at the root node was x1 less than equal to c. So if we keep doing this procedure, we're gonna reach to some terminal node or stopping criteria then the tree stops there.
15.srt	00:07:58.069 --> 00:08:06.369	So this is how the decision tree regressor works and the decision tree classifier works similar way.
15.srt	00:08:06.629 --> 00:08:09.859	except that it's not MSC but uses some other metric.
15.srt	00:08:10.839 --> 00:08:12.159	So we'll talk about that later.
15.srt	00:08:14.379 --> 00:08:16.479	Okay so let's have a look at the real data.
15.srt	00:08:16.539 --> 00:08:25.779	This data set is called faculty salary data set recording faculty salary at all the 90s and the task is to predict the assistant professor salary.
15.srt	00:08:26.369 --> 00:08:29.239	And it has four features and 50 samples.
15.srt	00:08:29.749 --> 00:08:37.289	But for simplicity to visualize I only use the two features and then depth equals two.
15.srt	00:08:40.769 --> 00:08:43.789	So depth in the tree means that how many levels that we go to grow the tree.
15.srt	00:08:43.789 --> 00:08:48.999	So this is depth equals zero at the root node, and this is steps one, and this is steps two.
15.srt	00:08:49.929 --> 00:09:00.789	If you don't specify the depth limit, the tree will grow until it has only one sample at the leaf node, or if there is another stopping criteria, it will stop there.
15.srt	00:09:02.009 --> 00:09:10.069	Anyway, this is the original data that had a salary mean value of 43,000, and then it had a 50...
15.srt	00:09:10.969 --> 00:09:11.639	samples.
15.srt	00:09:12.000 --> 00:09:15.549	So we have 50 samples and a value here at the root node.
15.srt	00:09:16.359 --> 00:09:20.659	And as we saw before, we will find all the split criteria.
15.srt	00:09:20.689 --> 00:09:31.209	That means that the decision tree will inspect all these split points along x0 and then along x1.
15.srt	00:09:31.859 --> 00:09:36.129	So it's going to split at the in the middle between the two samples.
15.srt	00:09:42.319 --> 00:09:47.379	So in the middle here, middle here, all the way to here and then it's gonna measure MSC's and then you will figure out which one to split.
15.srt	00:09:48.689 --> 00:10:04.799	So that's how it measures MSC here and then as a result it found that the splitting X1 54.95 at this point will make the MSC the lowest from the root node.
15.srt	00:10:13.489 --> 00:10:20.799	So depending on the answer to this criteria, we'll have these two And then each, from the each node, we'll also find another split criteria for next split.
15.srt	00:10:21.589 --> 00:10:31.289	And for example, this R1F box split criteria was splitting at feature x0 at the value 84.25.
15.srt	00:10:31.289 --> 00:10:40.189	And then it's gonna give this split, and then it leads to these two boxes.
15.srt	00:10:40.339 --> 00:10:45.679	Each of them have a mean value of 46,000 and 42,000.
15.srt	00:10:48.279 --> 00:10:55.749	Alright, if we further do that, the same procedure for this node will end up with this result.
15.srt	00:10:56.349 --> 00:11:00.029	So this was a simple example for how decision tree regressor works.
15.srt	00:11:00.649 --> 00:11:04.049	And in the next video, we'll talk about decision tree classifier.
11.srt	00:00:05.700 --> 00:00:06.370	Hello everyone.
11.srt	00:00:06.450 --> 00:00:11.960	We're going to talk about optimization in logistic regression to determine the values of the coefficients.
11.srt	00:00:13.410 --> 00:00:17.239	Alright so we're going to start by introducing a new concept called the maximum likelihood.
11.srt	00:00:17.550 --> 00:00:19.010	So what is the likelihood function?
11.srt	00:00:19.089 --> 00:00:24.000	Likelihood function is a product of all the probabilities that correspond to labels.
11.srt	00:00:30.940 --> 00:00:35.609	So for each sample the probability of classifying the label correctly And multiplying all these probabilities for each sample is called the likelihood function.
11.srt	00:00:36.060 --> 00:00:42.179	And by maximizing this likelihood, we can determine the coefficient values for the logit in the logistic regression.
11.srt	00:00:43.490 --> 00:00:51.240	By the way, this likelihood function is not only for the logistic regression, but it occurs again and again in machine learning, and it's a common theme.
11.srt	00:00:51.679 --> 00:00:54.450	This principle applies to all parametric models.
11.srt	00:00:54.570 --> 00:00:57.900	So if we maximize the likelihood, the parameters get determined.
11.srt	00:00:58.900 --> 00:01:01.350	And we're going to derive from this maximum likelihood.
11.srt	00:01:01.740 --> 00:01:08.219	This is especially for the binary class classification because we only delete the label equals 1 or 0.
11.srt	00:01:08.300 --> 00:01:16.180	So we're going to start by this likelihood function for binary class classification, and then we're going to derive the loss function from it.
11.srt	00:01:17.360 --> 00:01:18.270	So let's have a look.
11.srt	00:01:18.860 --> 00:01:31.510	So here is some example where we have y1 and we set it to 1 and y2 equals 1 and y true value for third example would be 0.
11.srt	00:01:32.560 --> 00:01:36.380	4 is 0 and y5 is 1.
11.srt	00:01:48.420 --> 00:01:51.020	And let's say we are using logistic regression and feed our features to get the predicted value I had but actually what logistic regression model produced at the output is the probability.
11.srt	00:01:51.430 --> 00:01:54.040	So this is a sigmoid function that we saw before.
11.srt	00:01:54.190 --> 00:02:04.200	So sigmoid function takes this form and this represents the probability of the label becomes 1.
11.srt	00:02:05.170 --> 00:02:14.740	So with that we can have a probability for sample number 1 and probability for sample number 2, probability for sample number 3 and so on.
11.srt	00:02:54.300 --> 00:03:00.180	And we'd like to construct the likelihood that means when the probability represent the probability of the label being 1 we're gonna have to flip some of them like this one so that it has the probability that represent the y equals 0 so to do that we're gonna change the sign and multiply all of the probabilities together and this quantity becomes a total probability of having all of these labels classified correctly.
11.srt	00:03:00.850 --> 00:03:04.630	So all y1, y2, etc.
11.srt	00:03:06.050 --> 00:03:06.870	are correct.
11.srt	00:03:09.250 --> 00:03:20.690	So this says that the correct probability that means we would like to maximize this probability so that our model can correctly classify all the examples.
11.srt	00:03:21.610 --> 00:03:22.910	So let's maximize this.
11.srt	00:03:23.900 --> 00:03:26.060	That's why this is called a maximum likelihood.
11.srt	00:03:29.060 --> 00:03:32.900	So likelihood function is the probability the total probability of classifying everything correctly.
11.srt	00:03:34.020 --> 00:03:36.810	So it takes this form and we would like to maximize.
11.srt	00:03:37.890 --> 00:03:42.380	And now we have some trouble because there are so many multiplications here.
11.srt	00:03:43.010 --> 00:03:52.720	This is not very easy to you know calculate so we're gonna take a log to the entire term so that we change this to the summation.
11.srt	00:03:59.290 --> 00:03:59.730	So summing up all the examples that has yi label 1.
11.srt	00:04:00.100 --> 00:04:14.950	Actually this has to be log pi and sum when the case is y equals 0.
11.srt	00:04:20.340 --> 00:04:25.610	And we can even make it more clean by having this one summation instead of two.
11.srt	00:05:04.860 --> 00:05:09.319	So instead of having when the case is 0 or 1, I'm gonna just set to everything has to be 1 and then I'm going to add yi here so it becomes the multiplication plus if I change the variable here to be 1 minus yi then when this is 1 this quantity becomes 0 so we can combine these two terms together so make it a little clean So this is our final form for the log likelihood.
11.srt	00:05:09.470 --> 00:05:16.060	So this is log likelihood and we want to maximize this quantity.
11.srt	00:05:18.259 --> 00:05:23.170	So actually maximizing log likelihood is the same as minimizing the loss function.
11.srt	00:05:23.720 --> 00:05:28.170	So we can define a loss function as just inverse of this.
11.srt	00:05:28.389 --> 00:05:36.019	So take a minus sign here and the same formula.
11.srt	00:05:38.050 --> 00:05:45.719	So that's our loss function and this is called binary cross entropy.
11.srt	00:05:45.719 --> 00:06:02.039	And this binary cross entropy is very common in binary class classification so we'll use this cross entropy loss function very often.
11.srt	00:06:03.569 --> 00:06:08.189	Alright so cross entropy again is a generalized form as this.
11.srt	00:06:08.699 --> 00:06:27.500	So these two are probability distribution and usually the one that's here means that the probability distribution that's kind of a label or true value So it can come from the data or you can come from the labels Where is this one?
11.srt	00:06:40.800 --> 00:06:53.240	The probability that goes into the log is predicted value So essentially we are measuring the difference between the true labels and the predicted probability So that's the meaning of the cross entropy and I omitted a category but if you have more than two categories, they have index for the category.
11.srt	00:06:53.639 --> 00:07:02.180	Alright so that's a cross entropy and for binary case where the category is only 0 and 1, we'll have this formula.
11.srt	00:07:02.970 --> 00:07:05.009	We derived from the maximum likelihood.
11.srt	00:07:06.920 --> 00:07:09.910	So searching parameters involves optimization.
11.srt	00:07:10.000 --> 00:07:14.800	So again the feature goes into model and the model has parameters.
11.srt	00:07:15.430 --> 00:07:25.539	the model will predict the value and with the target value, the loss function will compare this prediction and target and produce some error.
11.srt	00:07:26.800 --> 00:07:30.860	So if the error is bigger, then it's going to change the parameter value more.
11.srt	00:07:31.259 --> 00:07:37.409	And when we do this cycle multiple times, we're going to get the parameter values optimal value.
11.srt	00:07:37.969 --> 00:07:45.250	So that's optimization and this is parameter update procedure and we're going to use a gradient descent to update the parameters.
11.srt	00:07:47.079 --> 00:07:49.000	Alright, so let's talk about gradient descent.
11.srt	00:07:49.489 --> 00:08:00.599	So this error surface is actually from MSC loss, which is from linear regression, but the reason why I just draw here is that it's easier to draw than cross entropy.
11.srt	00:08:01.569 --> 00:08:13.000	So the loss looks like this, and let's say you're a skier and you're an advanced skier that you're not afraid to go to steep slope, and let's say you want to get to the base as soon as possible.
11.srt	00:08:13.310 --> 00:08:14.859	So what is your strategy here?
11.srt	00:08:17.139 --> 00:08:20.099	So you're gonna follow steepest slope, right?
11.srt	00:08:20.169 --> 00:08:35.099	So let's measure a gradient along the coefficient a, and let's measure the gradient along the coefficient b, and we're gonna update our weight, which is the parameter values for a and b, according to this gradient.
11.srt	00:08:37.229 --> 00:08:43.159	So that effectively makes this skier to go to this direction and follow the steepest slope.
11.srt	00:08:43.779 --> 00:08:51.329	Alright, so that's the intuition for gradient descent, and to do that mechanically, we'll have to make a derivative for loss function.
11.srt	00:08:51.569 --> 00:08:54.069	So loss function for MSC looks like this.
11.srt	00:08:54.399 --> 00:08:55.799	It has a residual squared.
11.srt	00:09:31.009 --> 00:09:42.029	and then the gradient along a coefficient is a partial derivative of loss function with respect to a and the function here is takes a form of f squared so it's like df squared of dx and it will be 2f times df dx so this is called a chain rule if you have a function that's a function of some other function which is a function of something else like this you can take a chain so making derivative of this, let's say this is x and again g is of another function of x.
11.srt	00:10:00.379 --> 00:10:00.939	Then what we will do is take a derivative f with respect to its argument which is g and then times dg, dg and then finally we can do dg dx.
11.srt	00:10:01.549 --> 00:10:05.539	So it looks complicated but actually when you look at it carefully it's not.
11.srt	00:10:05.809 --> 00:10:15.620	So when there is a multiple nested function you take the derivative conveniently as this and take the chain rule.
11.srt	00:10:30.549 --> 00:10:32.439	So using that if you take the chain rule here, it's 2f so there is a 2 here and therefore this 2 is gone and this is f and this is the fd w which is df dA.
11.srt	00:10:32.439 --> 00:10:40.709	So there is x here and for derivative of loss function with respect to b coefficient.
11.srt	00:10:41.879 --> 00:10:46.669	Then also takes a f here and then df db which is 1.
11.srt	00:10:46.840 --> 00:10:48.009	So there is nothing here.
11.srt	00:11:02.019 --> 00:11:07.189	So this is the formula for gradient descent for MSA loss function and weight update rule says the weight is updated such that it's the old value of the weight minus some constant alpha times the gradient of the loss function with respect to that weight.
11.srt	00:11:09.569 --> 00:11:11.429	This is called the learning rate by the way.
11.srt	00:11:15.500 --> 00:11:18.009	And the bigger the value, the bigger the step size.
11.srt	00:11:18.009 --> 00:11:26.509	So if the learning rate is big, then the step is bigger.
11.srt	00:11:26.789 --> 00:11:30.669	And be careful if it's too big, then it can pass the solution like this.
11.srt	00:11:35.289 --> 00:11:41.000	And if the learning rate is very small, we're gonna take a small step toward the goal like this.
11.srt	00:11:41.980 --> 00:11:47.460	So if the learning rate is too small, it's going to take a lot of steps and longer time.
11.srt	00:11:48.269 --> 00:11:54.580	So usually when you do the gradient descent optimization, you will have to choose this learning rate.
11.srt	00:11:54.580 --> 00:11:56.970	So therefore, this learning rate is a hyperparameter.
11.srt	00:12:02.149 --> 00:12:05.080	Hyperparameter means that some kind of...
11.srt	00:12:05.509 --> 00:12:09.050	parameter that you will have to, the user will have to choose.
11.srt	00:12:09.550 --> 00:12:11.170	So learning rate is one of them.
11.srt	00:12:11.910 --> 00:12:18.740	We don't have to worry about it for the logistic regression because the logistic regression uses another form of gradient descent.
11.srt	00:12:19.040 --> 00:12:26.550	Actually that uses second derivatives rather than first derivative like in the gradient descent and that is called the Newton method.
11.srt	00:12:26.550 --> 00:12:27.910	We'll talk about it very soon.
11.srt	00:12:29.200 --> 00:12:32.590	Alright so gradient descent for binary cross entropy laws.
11.srt	00:12:33.160 --> 00:12:34.640	Let's calculate this.
11.srt	00:12:35.150 --> 00:12:36.840	So BC law looks like this.
11.srt	00:14:10.779 --> 00:14:27.709	derived and then this is a sigmoid function and we're gonna take a derivative of loss function with respect to w and g is a function of w and x plus b so d loss d w is going to be d loss d sigma because loss is a function of sigma and then it's going to be d sigma d g because Sigma is a function of G and then DG DW okay so one at a time this value is going to be I'm gonna remove this for now because it's clean that way okay Y divided by Sigma so this is Sigma by the way this is Sigma derivative of log x is 1 over x so minus sign is from here and then minus 1 so that's my d log d sigma and then d sigma dg is sigma times 1 sigma times 1 minus sigma I'm not gonna show here but you can prove this easily and this is very good formula that sigma-weight function is very convenient as the derivative is itself times 1 minus itself and that's why it's used in the many gradient descent application.
11.srt	00:14:27.819 --> 00:14:35.819	Alright so we're gonna use that and then dG dW is simply X.
11.srt	00:14:41.389 --> 00:14:57.469	So combining all this together we're gonna get dL d w is going to be minus times sigma times 1 minus sigma and x.
11.srt	00:14:58.629 --> 00:15:04.049	Similarly, we can do the derivative for the bias.
11.srt	00:15:04.939 --> 00:15:10.149	And it will take the same thing except this part.
11.srt	00:15:10.649 --> 00:15:12.209	So there is just 1 here.
11.srt	00:15:14.959 --> 00:15:21.219	So that's the gradient along a coefficient w and bias B.
11.srt	00:15:21.870 --> 00:15:27.509	And we're gonna apply the same principle to update our weights.
11.srt	00:15:28.259 --> 00:15:31.229	So this could be either w or bias.
11.srt	00:15:32.159 --> 00:15:40.009	They're the coefficients and then dLows times the learning rate.
11.srt	00:15:41.139 --> 00:15:43.099	Okay so that was it.
11.srt	00:15:43.919 --> 00:15:47.779	Alright, so Newton's method, it's an extension to gradient descent method.
11.srt	00:15:47.779 --> 00:15:55.209	So gradient descent method only used the first derivative of loss function and its update rule was this.
11.srt	00:15:56.179 --> 00:16:00.979	Gradient with respect to w of the loss function.
11.srt	00:16:01.529 --> 00:16:06.889	Whereas Newton's method will use both the first and second derivative.
11.srt	00:16:08.109 --> 00:16:13.589	So first derivative here and then second derivative here.
11.srt	00:16:16.439 --> 00:16:18.759	And by the way this term is called a Hessian.
11.srt	00:16:19.909 --> 00:16:33.449	So in a matrix form it will look like a Hessian inverse and the gradient matrix times alpha minus omega like that.
11.srt	00:16:33.549 --> 00:16:43.899	And then the reason why Newton's method can be good is when we have a very flat gradient it can be very slowly converging.
11.srt	00:16:47.610 --> 00:16:54.730	However if there's a Hessian that's dividing this small gradient then Hessian is also small, then this can boost the speed of the convergence when the gradient is very small.
11.srt	00:16:56.439 --> 00:17:06.569	So the Newton's method converges faster, that means it requires a less number of iterations given the same learning rate for the gradient and the Newton's method.
11.srt	00:17:07.019 --> 00:17:08.279	However, it has a drawback.
11.srt	00:17:20.049 --> 00:17:20.629	So the memory that requires for one iteration, the Newton's method scales as n squared, whereas Newton.
11.srt	00:17:21.639 --> 00:17:26.289	Where is the gradient method when it takes O ?
11.srt	00:17:26.470 --> 00:17:44.289	And for the time complexity, Newton's method per iteration is going to be n cubed, whereas gradient method is n. So this is more expensive per iteration.
11.srt	00:17:44.289 --> 00:17:54.019	It's great that it can require less iteration, but given the same number of iterations, gradient descent requires less memory and less time.
11.srt	00:17:54.399 --> 00:17:56.119	This n is the number of parameters.
11.srt	00:17:57.029 --> 00:18:11.629	So if you have a lot of parameters like in the neural network, neural network typically have millions or billions of parameters, Newton's method or similarly second derivative method will be very slow.
11.srt	00:18:11.969 --> 00:18:19.659	So usually the neural network optimization utilize a gradient descent method rather than the second derivative method.
11.srt	00:18:20.169 --> 00:18:28.579	But in logistic regression and other parametric models in machine learning, where the number of parameters are smaller, we don't have to worry about that.
11.srt	00:18:29.000 --> 00:18:30.649	So that's why I.S.K.
11.srt	00:18:32.829 --> 00:18:38.990	Lohn and other similar packages using Newton's method or similar method to optimize the parameters.
11.srt	00:18:39.329 --> 00:18:42.409	Alright so let's have a look at some simulation here.
11.srt	00:18:43.419 --> 00:19:02.109	So this is a gradient descent and this is a Newton method and then they start from the same place and then you're gonna see shortly that they shoot very fast to the bottom and then it will go toward the goal.
11.srt	00:19:03.579 --> 00:19:11.189	Compared to the gradient descent method which goes very slowly when the gradient is small at the bottom, Newton's method is faster.
11.srt	00:19:11.339 --> 00:19:23.559	And that's because again, because this small gradient is divided by small h so it gains more boost when the gradient is flat here.
11.srt	00:19:26.669 --> 00:19:31.449	So that's some kind of intuition and then that's it for this video.
11.srt	00:19:32.569 --> 00:19:35.299	So in next video, we'll talk about performance matrix.
10.srt	00:00:06.120 --> 00:00:07.269	Hi everyone.
10.srt	00:00:07.309 --> 00:00:11.119	We're going to talk about introduction to logistic regression.
10.srt	00:00:13.339 --> 00:00:16.280	So brief review of machine learning problems.
10.srt	00:00:16.280 --> 00:00:25.149	So in machine learning, we have supervised learning with labels and unsupervised learning, which doesn't have label and reinforcement learning with feedback signals.
10.srt	00:00:25.679 --> 00:00:28.609	And we're going to focus on supervised learning.
10.srt	00:00:29.089 --> 00:00:31.239	And largely, it has two tasks.
10.srt	00:00:31.580 --> 00:00:33.200	regression and classification.
10.srt	00:00:33.770 --> 00:00:40.030	And in classification, binary class and multi-class classification we're going to talk about.
10.srt	00:00:40.030 --> 00:00:44.439	And previously we talked about linear regression can do the regression task.
10.srt	00:00:44.439 --> 00:00:48.049	And logistic regression we're going to talk about in this video.
10.srt	00:00:48.049 --> 00:00:52.299	Although its name says regression, it's actually for classification.
10.srt	00:00:52.299 --> 00:00:55.679	Especially it's useful for binary class classification.
10.srt	00:00:55.679 --> 00:01:01.090	There are some ways to do the multi-class classification with the logistic regression method.
10.srt	00:01:03.289 --> 00:01:12.340	But it's going to require some engineering to do that and other models that we're going to talk about it later and some of them will not talk about it in this course.
10.srt	00:01:12.820 --> 00:01:14.640	They can do different things.
10.srt	00:01:14.640 --> 00:01:20.250	So for example, support vector machine can do both regression and classification.
10.srt	00:01:20.250 --> 00:01:27.240	But similar to logistic regression, it's usually good for binary class rather than multi-class.
10.srt	00:01:27.240 --> 00:01:28.790	But it can work on multi-class.
10.srt	00:01:33.450 --> 00:01:36.010	If you engineer the label and some algorithms inside of the model correctly.
10.srt	00:01:37.010 --> 00:01:40.540	And then decision trees can do everything.
10.srt	00:01:40.540 --> 00:01:46.070	So you can do regression and binary class, multi-class without any problem.
10.srt	00:01:46.070 --> 00:01:50.099	Also, it's nice that you can take categorical variable very efficiently.
10.srt	00:01:50.189 --> 00:02:01.379	Neural network, same thing, can do everything and many other models that we may not introduce in this course can do different things.
10.srt	00:02:05.270 --> 00:02:10.539	So with this high-level, High-level introduction, let's dive in and let's talk about what is the binary class classification.
10.srt	00:02:10.539 --> 00:02:14.340	It is essentially yes or no problem.
10.srt	00:02:14.340 --> 00:02:17.210	So the label is binary.
10.srt	00:02:17.210 --> 00:02:27.849	So for example, credit card default, whether this customer that uses a credit card will likely to default or not given like some historic data.
10.srt	00:02:27.849 --> 00:02:33.340	And maybe there is insurance claims and some insurance claim can be fraudulent.
10.srt	00:02:33.340 --> 00:02:36.349	So that can be a binary class classification.
10.srt	00:02:38.240 --> 00:02:42.180	Spam filtering, given this email text, is this spam or not?
10.srt	00:02:43.990 --> 00:02:51.520	Medical diagnosis, given this patient information and lab tests and data, is this person have a disease or not?
10.srt	00:02:53.250 --> 00:03:03.159	Survivor prediction, given this patient's information and history and things like that, whether this patient will survive for next five years or not.
10.srt	00:03:04.800 --> 00:03:06.090	How about customer retention?
10.srt	00:03:06.420 --> 00:03:08.170	Is this customer behavior?
10.srt	00:03:08.949 --> 00:03:14.639	is likely to charm or not, so that marketing action can be taken.
10.srt	00:03:14.639 --> 00:03:22.609	Image recognition, various kinds, can also be binary class classification.
10.srt	00:03:23.469 --> 00:03:26.529	For example, is this animal dog or cat?
10.srt	00:03:27.899 --> 00:03:33.939	And sentiment analysis, given this text or Twitter sentences, what is the sentiment?
10.srt	00:03:33.939 --> 00:03:36.049	Is this negative or positive?
10.srt	00:03:36.049 --> 00:03:37.060	Things like that.
10.srt	00:03:38.909 --> 00:03:45.259	So as you can see, binary class classification can have a variety of different types of data input.
10.srt	00:03:45.929 --> 00:03:52.689	It could be tabulated data, it could be image, it could be text, it could be even speeches.
10.srt	00:03:53.219 --> 00:04:05.129	So that determines the binary class or not is actually entirely for the label instead of the data itself or the features itself.
10.srt	00:04:07.079 --> 00:04:09.869	So a brief example, we can talk about some...
10.srt	00:04:09.979 --> 00:04:12.089	breast cancer diagnosis problem.
10.srt	00:04:12.609 --> 00:04:19.209	So this is one of the features that can determine whether this tumor is malignant or not.
10.srt	00:04:19.319 --> 00:04:24.060	So it can be a binary classification problem.
10.srt	00:04:25.019 --> 00:04:36.519	And we want to have some kind of threshold or some decision value that above this value, maybe we are more sure that this is going to be malignant.
10.srt	00:04:37.219 --> 00:04:41.129	And maybe below this certain value, maybe it's less likely to be malignant.
10.srt	00:04:41.759 --> 00:04:49.740	So building a logistic regression model will help us to find this threshold value, which is called the decision boundary, by the way.
10.srt	00:04:50.389 --> 00:05:04.250	And if you have more than one feature, let's say we have two features, it can be shown as a 2D diagram like here, and our decision boundary will be likely to be a line instead of a threshold value.
10.srt	00:05:04.959 --> 00:05:11.300	So maybe this side is malignant and this side is likely to be benign.
10.srt	00:05:12.680 --> 00:05:20.509	Alright, so logistic function provides some convenient way to construct model like this.
10.srt	00:05:21.009 --> 00:05:22.720	So logistic function look like this.
10.srt	00:05:23.480 --> 00:05:31.050	It's between 0 and 1 and it smoothly connect the line between 0 and 1.
10.srt	00:05:31.879 --> 00:05:35.110	And there is a sharp transition around the certain threshold value.
10.srt	00:05:35.110 --> 00:05:38.230	Let's say this is 0, but it could be any other value.
10.srt	00:05:39.689 --> 00:05:43.509	And this represent, because it's between 0 and 1.
10.srt	00:05:44.439 --> 00:05:46.560	logistic function can be a probability function.
10.srt	00:05:48.360 --> 00:05:52.230	And actually the logistic function has another name called the sigmoid.
10.srt	00:05:52.500 --> 00:05:57.510	So this is also called a sigmoid function and the form takes this one.
10.srt	00:05:58.150 --> 00:06:08.010	So the G is the linear combination of the features with this weight and bias like we did in the linear regression.
10.srt	00:06:08.430 --> 00:06:14.120	And then this G goes through a nonlinear function 1 over 1 plus e to the minus G.
10.srt	00:06:14.629 --> 00:06:24.290	and then this entire function as a function of z is called a sigmoid and takes the shape of this curve here.
10.srt	00:06:26.589 --> 00:06:31.110	By the way, this g is called logit and this is a relative decision boundary.
10.srt	00:06:46.139 --> 00:06:48.569	So when it's set to zero, that means this is our threshold value and the probability here is going to be this one and this g is zero then it's 1 half, so it's going to meet the 0.5.
10.srt	00:06:48.680 --> 00:07:00.699	So with the 0.5 threshold, so above 0.5, we can say this is going to be malignant, and below 0.5 probability, we can say it's going to be benign.
10.srt	00:07:00.699 --> 00:07:09.610	Well, some people might ask why don't we use linear regression instead, and maybe we can fit it like here.
10.srt	00:07:09.730 --> 00:07:15.079	We can fit this and then maybe find some threshold, and it can also fit the probability of 0.5.
10.srt	00:07:20.220 --> 00:07:22.410	We can try to do that.
10.srt	00:07:22.410 --> 00:07:23.420	It's not easy.
10.srt	00:07:23.420 --> 00:07:37.530	First of all, we will have to find out where this threshold is and maybe we can just fit the line first and then just figure out which value will give 0.5 threshold.
10.srt	00:07:37.530 --> 00:07:44.570	But if we do that, it gives a different threshold value to the logistic regression.
10.srt	00:07:51.220 --> 00:07:58.550	So the one problem with the linear regression model if it fitted and then find the threshold where the probability value becomes 0.5, is that it's not very interpretable.
10.srt	00:07:58.580 --> 00:08:02.550	Where is the logistic regression with the sigmoid function?
10.srt	00:08:02.550 --> 00:08:14.180	It is a well-defined probability function, so it's very interpretable that we can find where probability becomes 0.5 and this gives the right threshold for us.
10.srt	00:08:14.180 --> 00:08:18.360	So let's talk about decision boundary more.
10.srt	00:08:21.530 --> 00:08:24.650	So in univariate case, where we have only one feature.
10.srt	00:08:26.199 --> 00:08:34.570	The decision boundary is a point where it meets the probability equals 0.5 so the equation looks like this and you can get the value out of it.
10.srt	00:08:34.570 --> 00:08:49.840	If we have two features the data will lie in the two-dimensional space and then the decision boundary becomes a line so we can find the line equation here which will draw this line.
10.srt	00:08:49.840 --> 00:08:56.820	If it's a multivariate have a multidimension more than 3, the decision boundary will be a hyperplane.
10.srt	00:08:59.309 --> 00:09:02.299	Okay, let's talk about what if we have multiple categories.
10.srt	00:09:02.409 --> 00:09:13.289	So instead of having yes or no problem, maybe we can have multiple categories, which is maybe we would like to predict whether this animal is cat or dog or maybe cow.
10.srt	00:09:13.909 --> 00:09:20.149	So for the logistic regression, the logit, which is a decision boundary, takes this form.
10.srt	00:09:20.240 --> 00:09:30.569	And then for softmax, which is multinomial, this has another name, multinomial logistic regression, has this form.
10.srt	00:09:32.850 --> 00:09:38.919	So they are very similar except that there is now an index for K category.
10.srt	00:09:39.460 --> 00:09:41.450	So this is index for category.
10.srt	00:09:43.919 --> 00:09:48.250	So for example for category number one, we can construct this model.
10.srt	00:09:48.250 --> 00:09:53.549	So there will be different weights assigned to each category and for each feature.
10.srt	00:09:54.360 --> 00:09:55.669	And now we did this logit.
10.srt	00:09:55.950 --> 00:10:05.789	So for logistic regression, we use the sigmoid function as a probability and we show this form but it can be rewritten as this form as well.
10.srt	00:10:06.220 --> 00:10:09.179	and this is very similar to softmax.
10.srt	00:10:10.620 --> 00:10:19.110	The softmax function takes the same form as this one, except that it now has an index for the category.
10.srt	00:10:19.949 --> 00:10:29.519	And then instead of this, now it has all the summation over all the possible exponents of these corresponding categories.
10.srt	00:10:30.659 --> 00:10:36.009	Alright, so softmax is called multinomial logistic regression.
10.srt	00:10:36.079 --> 00:10:41.610	However, there is another similar way that we can use the original logistic regression for multi-categories.
10.srt	00:10:43.360 --> 00:11:00.110	So maybe category A, B, C. We can construct such that it is a binary class classification for A versus not A, which we will have to combine these two cases.
10.srt	00:11:00.769 --> 00:11:06.610	So this is maybe logistic regression model 1, and this is logistic regression model 2.
10.srt	00:11:06.709 --> 00:11:16.740	We're going to do B versus not B, and then we're going to construct third model that says C versus not C.
10.srt	00:11:18.019 --> 00:11:29.240	And this approach is called one versus the rest, an OVR problem.
10.srt	00:11:29.439 --> 00:11:34.649	So there are, you know, different ways to get the multi-category classification done.
10.srt	00:11:34.649 --> 00:11:39.829	So one is, like we mentioned, we use a multinomial approach which is a softmax.
10.srt	00:11:39.829 --> 00:11:43.360	And another way to do is using OVR.
10.srt	00:11:48.669 --> 00:12:10.059	So you can find a Sklearn library that uses utilize these two, but I think Softmax or Multinomial is more common and you will see later other classification models such as SVM and decision trees, they have a preferred way of being Multinomial versus logistic or maybe some model is more convenient to use one versus the other, so we'll talk about that later.
10.srt	00:12:22.329 --> 00:12:29.719	By the way, both OVR and Softmax, their probabilities for categories they sum to one, so for example probability for A plus probability for B plus probability for being C category for the sample number 1, they sum to 1.
10.srt	00:12:30.769 --> 00:12:34.509	So that's the same for logistic and the softmax regression.
10.srt	00:12:35.199 --> 00:12:48.189	However, there could be some problem where maybe there are A, B, C category and we don't necessarily need to pick one of them, but maybe the category doesn't exist at all.
10.srt	00:12:55.449 --> 00:13:01.209	So neither cat nor dog nor cow but something else, then this should be 0, 0, 0, then In that case, it's called multi-label problem.
10.srt	00:13:01.209 --> 00:13:11.479	I know it sounds strange because label and categories, what's the difference?
10.srt	00:13:27.289 --> 00:13:30.620	But this type of problem where we don't necessarily have to pick one of them in the categories are called multi-label problem versus if we have to pick one of the the categories, then it's a multi-class problem.
10.srt	00:13:31.009 --> 00:13:36.649	And both the logistic and softmax models, they are for multi-class classification.
10.srt	00:13:36.649 --> 00:13:52.299	And then there can be some other ways to treat the multi-label problem, but we can still use the same models, but we will have to construct the labels differently and construct the training process a little differently.
10.srt	00:13:52.299 --> 00:14:01.210	So that's a little bit of difference, but you will see more often the multi-class classification problems than multi-level problems, but just keep in mind that they exist.
10.srt	00:14:03.110 --> 00:14:03.470	Alright?
10.srt	00:14:05.889 --> 00:14:11.210	But anyway, Softmax regression can give this kind of visualization.
10.srt	00:14:11.210 --> 00:14:22.440	So let's say we had only two features in the dataset and the data will lie in the 2D plane and this is going to be the decision boundary that Softmax will give us.
10.srt	00:14:23.460 --> 00:14:25.090	You can see more examples here.
10.srt	00:14:26.049 --> 00:14:30.019	Alright, and this ends our video.
10.srt	00:14:30.139 --> 00:14:36.909	And then in the next video, we're going to talk about how optimization works in logistic regression and how the coefficients are determined.
12.srt	00:00:05.179 --> 00:00:05.870	Hello everyone.
12.srt	00:00:05.929 --> 00:00:09.720	In this video, we're going to talk about performance metrics in classification.
12.srt	00:00:12.060 --> 00:00:23.399	So here is the example for binary class classification where the label is 1 when the tumor is malignant and the label is 0 when the tumor is not malignant.
12.srt	00:00:24.260 --> 00:00:33.049	So we created a logistic regression model based on this feature and let's talk about some terminology here.
12.srt	00:00:33.870 --> 00:00:40.230	So this region where both the labels and the predictions are positive is called a true positive.
12.srt	00:00:41.380 --> 00:00:47.109	And this region where the labels and the predictions are both negative, we call true negative.
12.srt	00:00:49.010 --> 00:00:56.230	And this region is called false positive because the prediction says it's positive, but actually the label says it's negative.
12.srt	00:00:57.689 --> 00:01:05.629	On the other hand, this region is called false negative because the prediction says negative, whereas the label says it's positive.
12.srt	00:01:06.519 --> 00:01:15.379	So we would like to build a model that maximizes the number of true positives and true negatives, whereas we want to minimize false positives and false negatives.
12.srt	00:01:18.210 --> 00:01:21.710	This false negative and false positive have different names as well.
12.srt	00:01:22.430 --> 00:01:27.520	And false positive is called type 1 error, whereas false negative is called type 2 error.
12.srt	00:01:28.680 --> 00:01:33.340	If those terminology confuses you, then you can remember this funny picture.
12.srt	00:01:33.760 --> 00:01:36.659	Type 1 error is like telling a man that he is pregnant.
12.srt	00:01:37.130 --> 00:01:41.490	Whereas type II error is like telling a pregnant woman that she is not pregnant.
12.srt	00:01:42.460 --> 00:01:44.930	So both are bad and sometimes they are in trade-off.
12.srt	00:01:44.930 --> 00:01:52.310	So depending on the situation and what are important to us in the problem, we'll have to consider one more seriously than the other.
12.srt	00:01:52.310 --> 00:02:00.180	So we talked about true positive, true negative, and then false positive and false negative cases.
12.srt	00:02:00.180 --> 00:02:05.630	A formal way to express that in a table is called confusion matrix.
12.srt	00:02:05.630 --> 00:02:08.840	So confusion matrix is like this.
12.srt	00:02:08.840 --> 00:02:10.220	There is a prediction label.
12.srt	00:02:10.220 --> 00:02:12.070	There is a target label.
12.srt	00:03:14.890 --> 00:03:18.250	and they are in the table that this is true positive and this is true negative and this is false positive this is false negative and sometimes depending on the notation they may be the row and the column may be exchanged and this can be calculated by the scalar matrix confusion matrix module and when you use a confusion matrix module it will give the labels as row and then prediction as column, but they don't display this so Sometimes it's confusing but you can figure out by looking at the data Okay, so let's say we have all the numbers that we collected from this confusion matrix and now let's calculate some performance metrics So the most popular one in classification is accuracy Accuracy is number of correct answers divided by all the data points So it's a measure of how many are accurate out of all the data points.
12.srt	00:03:20.720 --> 00:03:31.620	And true positive rate, in other words, recall or sensitivity, is a measure of how many are truly positive out of all the positive cases in the data.
12.srt	00:03:31.720 --> 00:03:33.410	So this is all the positives.
12.srt	00:03:34.570 --> 00:03:38.540	And this is true positive in the data.
12.srt	00:03:41.130 --> 00:03:43.280	Okay, and another metric...
12.srt	00:03:44.670 --> 00:03:49.319	True negative rate, which is similar to true positive rate except that they are kind of flipped.
12.srt	00:03:50.129 --> 00:03:54.040	Another name for it is specificity or selectivity.
12.srt	00:03:54.560 --> 00:03:58.550	Measures how many are true negative out of all the negative cases in the data.
12.srt	00:03:59.939 --> 00:04:03.290	So it's negative cases in the data.
12.srt	00:04:05.150 --> 00:04:06.610	By data, I mean the labels.
12.srt	00:04:09.960 --> 00:04:14.030	Another good measure that we often use is a positive predictive value.
12.srt	00:04:14.480 --> 00:04:16.120	Or in other words, precision.
12.srt	00:04:16.439 --> 00:04:24.699	measures how many are correctly classified as true positive out of prediction from the prediction.
12.srt	00:04:28.310 --> 00:04:38.810	And false positive rate, in other words, fallout rate tells us how many are false positive out of all the negative cases.
12.srt	00:04:39.519 --> 00:04:45.480	So how many were falsely classified as positive when it was actually negative.
12.srt	00:04:47.999 --> 00:04:54.879	So it's although negatives from the data and how many of them are falsely classified as positive.
12.srt	00:05:19.719 --> 00:05:26.349	So actually as you can see it is 1 minus right here and similarly false negative rate, in other words miss rate is also how many of positive in the data are falsely classified as negative and this is actually 1-tPr.
12.srt	00:05:26.349 --> 00:05:33.069	So they are related to each other.
12.srt	00:05:33.069 --> 00:05:38.579	F1 score is a good metric because oftentimes there is a trade-off between recall and precision.
12.srt	00:05:50.109 --> 00:05:56.089	So recall and precision and in some cases recall is a good metric but we want to see both of them together so in the case we want to use F1 score because it has both of the precision and recall inside.
12.srt	00:05:58.579 --> 00:06:02.689	So F1 score is usually robust metric so it's good to use.
12.srt	00:06:05.569 --> 00:06:12.909	All right so not only the F1 score here are also good metrics that are robust in you know different kinds of situations.
12.srt	00:06:13.279 --> 00:06:20.419	One of them is called ROC curve which is a receiver operating characteristic curve and it shows like this.
12.srt	00:06:21.329 --> 00:06:25.669	So in the x-axis it has a false positive rate and its y-axis it has a true positive rate.
12.srt	00:06:25.669 --> 00:06:30.589	And this red dotted line represents the random guess.
12.srt	00:06:30.589 --> 00:06:37.719	So if the curve goes this way, closer to this left top corner, this means it's good.
12.srt	00:06:37.719 --> 00:06:44.199	We have small false positive rate and large true positive rate, so that's good.
12.srt	00:06:44.199 --> 00:06:49.669	However, if the curve is below this random guess, then it's bad.
12.srt	00:06:53.709 --> 00:06:55.239	That means we have a high false positive rate and small true positive rate.
12.srt	00:06:55.939 --> 00:06:57.999	So this side is good and this side is bad.
12.srt	00:06:58.849 --> 00:07:02.759	So that's ROC curve and this is a graphic way to tell.
12.srt	00:07:03.319 --> 00:07:11.329	However, if you want to see a number then we can use area under the curve AUC.
12.srt	00:07:11.839 --> 00:07:13.899	So we measure the area under the curve.
12.srt	00:07:14.289 --> 00:07:22.749	So for example this curve the example the area under the curve will be this value.
12.srt	00:07:24.389 --> 00:07:26.109	So usually between 0 and 1.
12.srt	00:07:26.379 --> 00:07:28.429	the bigger the value, it's better.
12.srt	00:07:30.249 --> 00:07:32.169	So that was ROC and AUC.
12.srt	00:07:32.849 --> 00:07:39.889	Okay so when to use these different kinds of metric?
12.srt	00:07:40.729 --> 00:07:50.859	So we have a number of choices, accuracy, sensitivity, specificity, precision, fallout rate, miss rate, F1 score, AUC, and confusion metrics.
12.srt	00:07:51.109 --> 00:07:54.389	So AUC, ROC.
12.srt	00:08:29.379 --> 00:08:33.609	In general rule of thumb when the data is when the label is very imbalanced accuracy might be really bad so for example if your data is 99.9% negative and maybe 0.1% positive and maybe your model says 100% negative then it's going to give a fantastic accuracy 99.9% 0.9% correct if it says everything is negative.
12.srt	00:08:33.609 --> 00:08:35.309	So that's not good.
12.srt	00:08:35.309 --> 00:08:39.289	So accuracy may have some pitfall there.
12.srt	00:08:39.289 --> 00:08:43.449	So usually it's a good idea to use accuracy when you have a balanced data.
12.srt	00:08:43.449 --> 00:08:50.899	And recall, which is true positive divided by all the positive cases in the data.
12.srt	00:08:50.899 --> 00:08:57.349	They are used mostly when we want to capture as many positive cases as possible.
12.srt	00:08:57.349 --> 00:09:00.769	So even though we kind of sacrifice false positives.
12.srt	00:09:02.349 --> 00:09:15.699	So when it's good for, so for example cancer detection, cancer detection by missing someone having cancer, if we miss the cancer of a patient then the patient is at risk.
12.srt	00:09:16.139 --> 00:09:19.929	So there is a high cost associated with missing.
12.srt	00:09:20.129 --> 00:09:26.769	So in that case we want to use recall because we want to capture as as much of post cases as possible.
12.srt	00:09:28.019 --> 00:09:31.449	And in that case also we want to look at false negative rate.
12.srt	00:09:32.479 --> 00:09:36.649	If you have too much false negatives in the data, then we are in trouble.
12.srt	00:09:36.649 --> 00:09:39.249	So we want to look at both of them at the same time.
12.srt	00:09:39.829 --> 00:09:41.949	If we have a high cost of missing something.
12.srt	00:09:42.879 --> 00:09:51.319	And on the other hand, the false post rate or false alarm rate can be used when the cost of false alarm is high.
12.srt	00:09:51.919 --> 00:09:54.149	So for example, spam mail.
12.srt	00:09:55.969 --> 00:09:58.799	So having spam mail is fine.
12.srt	00:09:58.799 --> 00:09:59.849	It's just annoying.
12.srt	00:09:59.849 --> 00:10:05.289	However, if you have a false alarm, that means it's going to erase the important mail.
12.srt	00:10:05.709 --> 00:10:07.079	then it's problematic.
12.srt	00:10:07.079 --> 00:10:11.719	So we want to avoid these false alarms, then we want to look at the false positive rate.
12.srt	00:10:12.719 --> 00:10:18.189	Which is also similar to specificity or sensitivity, so we want to look at it as well.
12.srt	00:10:19.359 --> 00:10:23.849	Precision is used when we want to be very sure about the action.
12.srt	00:10:37.469 --> 00:10:41.969	So for example, when we identify scammers in PayPal or Venmo, and we'd like to inactivate their account, then we want to be very sure because otherwise we can just delete innocent user's account and it'll make the customer unhappy.
12.srt	00:10:43.599 --> 00:10:52.809	All right, so in summary, some performance metrics can be considered more important than the other depending on the situation and what's important in your problem.
12.srt	00:10:53.569 --> 00:10:58.389	However, these performance metrics are robust and can be used in almost any cases.
12.srt	00:10:59.669 --> 00:11:05.279	All right, so let's talk about cross entropy as a performance metric.
12.srt	00:11:05.679 --> 00:11:09.599	So, why do we want to use cross entropy and not accuracy?
12.srt	00:11:10.039 --> 00:11:17.139	Because accuracy, although it doesn't work very well in imbalanced data, if the data is balanced, it's pretty good.
12.srt	00:11:17.539 --> 00:11:19.539	And it's intuitive and interpretable.
12.srt	00:11:20.279 --> 00:11:22.109	But why do we want to use cross-entropy?
12.srt	00:11:22.109 --> 00:11:35.559	In a nutshell, cross-entropy can use more granular information about how the prediction is more confident, whereas accuracy only says whether it's correct or not.
12.srt	00:11:35.559 --> 00:11:37.979	Why is that?
12.srt	00:11:37.979 --> 00:11:38.719	Let's have a look.
12.srt	00:11:38.719 --> 00:11:40.029	So, for example, this case.
12.srt	00:11:41.839 --> 00:11:43.059	This is model A.
12.srt	00:11:43.139 --> 00:11:53.699	Model A's have accuracy of 2 3rd because it's correct two times and incorrect one time for these three samples.
12.srt	00:11:54.859 --> 00:11:59.609	And when you see, although it's correct here, the confidence is not that great.
12.srt	00:12:00.359 --> 00:12:07.339	Whereas the incorrect ones, the confidence is too confident for the incorrect answer.
12.srt	00:12:07.599 --> 00:12:10.889	So we can say this model A is not very good model.
12.srt	00:12:12.409 --> 00:12:17.949	Maybe we can compare to model B, which has the same accuracy to third.
12.srt	00:12:18.559 --> 00:12:23.489	However, when it's correct, it's pretty confident for the correct answers.
12.srt	00:12:24.179 --> 00:12:28.039	And when it's not correct, maybe it's not sure.
12.srt	00:12:28.169 --> 00:12:29.809	So maybe it makes sense.
12.srt	00:12:31.909 --> 00:12:38.569	So in this case, if we used cross entropy, it can discern these two different cases.
12.srt	00:12:39.259 --> 00:12:43.319	So it will give a better score, which is a lower cross entropy value.
12.srt	00:12:43.789 --> 00:12:50.419	for the better model and higher cross entropy value for the less working model.
12.srt	00:12:50.939 --> 00:12:58.099	So that's some intuition behind why you might want cross entropy and not accuracy, although accuracy might be more intuitive.
13.srt	00:00:05.129 --> 00:00:06.320	Okay, hello everyone.
13.srt	00:00:06.320 --> 00:00:11.480	In this video, we're going to talk about sklearn library usage for logistic regression.
13.srt	00:00:13.009 --> 00:00:17.170	Okay, so logistic regression module is inside of sklearn.linear model.
13.srt	00:00:17.170 --> 00:00:21.730	And it has a bunch of options here.
13.srt	00:00:21.730 --> 00:00:27.059	And interestingly, it already has regularization terms.
13.srt	00:00:27.059 --> 00:00:29.890	And they actually depend on the type of the solar.
13.srt	00:00:29.890 --> 00:00:32.079	So by default, solar is LBFGS.
13.srt	00:00:32.119 --> 00:00:36.210	And then in that case, by default, it uses L2 regularization.
13.srt	00:01:16.460 --> 00:01:19.579	and it already does the fit intercept equals true which is better to have in linear model and you don't have to worry about a lot of things here but you might want to change class weight equals balanced if you have an imbalance labels then it will automatically weight your class labels so you have a better a slightly better performance and in case you have a more than binary class it's going to automatically apply some multi-class and usually most of time it will apply the softmax which is multinomial And then, so there are several solver types.
13.srt	00:01:19.820 --> 00:01:24.980	Usually you don't have to worry about it, but if you want to try out different solvers, you can try.
13.srt	00:01:26.010 --> 00:01:31.070	All of them uses some sophisticated second derivative or similar method.
13.srt	00:01:32.540 --> 00:01:42.490	So for njobs, if you have a multiple core CPU, then you can utilize it so you can have less computation time with the parallelization.
13.srt	00:01:43.099 --> 00:01:47.969	If you do the njobs equals minus one, it's going to use all the CPU cores in your computer.
13.srt	00:01:49.769 --> 00:01:56.150	Alright, that was how the module looks like and then let's have a look at how to use it.
13.srt	00:01:56.310 --> 00:01:57.950	So basic usage is like this.
13.srt	00:01:58.310 --> 00:02:09.909	So you can just call the module and you can throw in your preferred options and then you can do the .fit and inside of this fit function, you're gonna throw your data.
13.srt	00:02:09.909 --> 00:02:14.800	So it's features for the training and this y is the labels for the training.
13.srt	00:02:15.409 --> 00:02:18.490	And you can call this object as model or some other name.
13.srt	00:02:19.329 --> 00:02:27.280	And from this model object, after this fitting has been done, it has some number of useful stuff inside.
13.srt	00:02:27.519 --> 00:02:35.699	So for example, model.coef underscore will give us the coefficient values for all the features in this feature matrix.
13.srt	00:02:35.699 --> 00:02:43.179	And intercept is a separate, so you will have to do model.intercept underscore, then it's going to give the value for intercept.
13.srt	00:02:43.179 --> 00:02:48.000	The model.predict parentheses and throw your data.
13.srt	00:03:21.949 --> 00:03:23.429	such as test data or train data or any data that you want to get the prediction out then it's going to give the binarized prediction so Y prediction another good comment is predict proba and you can throw in your data features then it's going to produce the probability so row output from sigmoid you can produce you can use that and plot this kind of graph or you can inspect the probability.
13.srt	00:03:25.519 --> 00:03:29.339	Alright, so let's further talk about some example.
13.srt	00:03:29.619 --> 00:03:45.809	So in this example, I'm going to split my original data x and y into train chunk and then test chunk, and that's done by this very popular, very popular function called train test split.
13.srt	00:03:45.809 --> 00:03:52.529	It's inside of sklearn model selection, and be careful of these detailed names.
13.srt	00:03:52.780 --> 00:04:03.229	because sometimes they may upgrade and they may change the names as they change the directory of their you know sub libraries and things like that.
13.srt	00:04:03.339 --> 00:04:05.659	But I think for now it's valid.
13.srt	00:04:06.500 --> 00:04:07.589	So we'll use that.
13.srt	00:04:08.530 --> 00:04:13.620	So we'll call the LogisticRegressionModule and you can name it differently if it's too long.
13.srt	00:04:13.969 --> 00:04:18.889	So I named it as a LR and then I'm gonna throw my preferred options.
13.srt	00:04:19.839 --> 00:04:22.769	There's no reason why I chose it but I just chose it.
13.srt	00:04:22.930 --> 00:04:23.730	for example.
13.srt	00:04:24.009 --> 00:04:35.069	And then I'm gonna fit my data, x train and y train, and if your y train label matrix shape is not correct, it's going to complain.
13.srt	00:04:35.100 --> 00:04:36.020	Use label.
13.srt	00:04:36.180 --> 00:04:38.300	You can use that in that case.
13.srt	00:04:53.060 --> 00:04:57.530	And I'm gonna call my object clf this time, and then if you want to get an accuracy, so this score uses accuracy by default, you can do fitted model.score and throw your test data and it's going to give some result.
13.srt	00:04:57.710 --> 00:05:07.889	So for this example the result was accuracy of 0.96 which is pretty good.
13.srt	00:05:08.009 --> 00:05:13.639	If you get rid of this option it might be slightly lower but yeah that's your choice.
13.srt	00:05:13.639 --> 00:05:19.920	We can use other kinds of metric and they are all in this sklearn.metrics module.
13.srt	00:05:24.540 --> 00:05:31.620	So for example I can predict Yp and then most of them requires Ytrue and then Yprediction.
13.srt	00:05:31.620 --> 00:05:32.950	So I'm gonna throw in there.
13.srt	00:05:34.130 --> 00:05:37.990	So accuracy score function requires Ytrue and then Yprediction.
13.srt	00:05:37.990 --> 00:05:39.390	So I throw that order.
13.srt	00:05:40.170 --> 00:05:43.320	Recall score as well and precision and F1 score.
13.srt	00:05:43.890 --> 00:05:46.280	As a result, it gives these numbers.
13.srt	00:05:46.790 --> 00:05:47.190	Alright.
13.srt	00:05:48.550 --> 00:05:53.070	And then I can do the confusion matrix using confusion matrix function.
13.srt	00:05:53.140 --> 00:05:55.860	And again it needs Ytrue value and Yprediction.
13.srt	00:05:56.220 --> 00:05:59.120	prediction value and it also requires labels.
13.srt	00:05:59.930 --> 00:06:07.920	As we mentioned in the previous lecture, this is going to be y prediction and this is going to be the label.
13.srt	00:06:10.630 --> 00:06:15.770	Okay so more examples.
13.srt	00:06:15.840 --> 00:06:17.990	We can also draw precision recall curve.
13.srt	00:06:18.080 --> 00:06:22.560	So previously we talked about ROC curves and precision recall curve is similar.
13.srt	00:06:22.560 --> 00:06:28.130	So ROC curve had true positive rate versus false positive rate.
13.srt	00:06:29.200 --> 00:06:36.720	precision recall curve is precision versus recall and has this shape.
13.srt	00:06:37.670 --> 00:06:50.620	So ROC curve it was better when it's close to the left top corner and precision recall curve is better when it's close to the right top corner because we want to high precision and high recall as well.
13.srt	00:06:58.560 --> 00:07:03.760	So using precision recall curve function it also requires true value and prediction probability actually rather than label, binarized label.
13.srt	00:07:03.760 --> 00:07:19.110	So I'm using this predict proba and then the column one actually gives the probability of being label being one so I'm gonna use that and I can just further draw this curve.
13.srt	00:07:21.990 --> 00:07:24.420	So ROC curve also works as a similar.
13.srt	00:07:30.710 --> 00:07:44.569	So I use ROC curve and then it's going to output FPR, TPR, and the threshold that was used to calculate these kind of spots and then the result looks like this and I've included the random guess and AUC score as well.
13.srt	00:07:44.600 --> 00:07:49.980	The AUC score can be also automatically calculated using this function, out of series score.
13.srt	00:07:50.689 --> 00:07:55.019	Again, it needs a true label and then the prediction probability.
13.srt	00:07:56.500 --> 00:07:57.590	So it's very handy.
13.srt	00:08:00.590 --> 00:08:05.660	So we talked about some barosymmetrics and how to get the coefficient values from VTID model.
13.srt	00:08:05.960 --> 00:08:07.950	But how about statistics?
13.srt	00:08:10.040 --> 00:08:15.190	Unfortunately, the logistic regression module in sklearn doesn't give statistics right away.
13.srt	00:08:15.720 --> 00:08:16.980	So we have two choices.
13.srt	00:08:17.440 --> 00:08:23.010	One is using the stats model library as we did before as in linear regression.
13.srt	00:08:23.900 --> 00:08:29.470	So instead of linear regression, we can use dot logit module and then throw our data.
13.srt	00:08:30.170 --> 00:08:37.180	So be careful that their order of feature and label is different here.
13.srt	00:08:37.180 --> 00:08:39.889	So they take the label first and then the features.
13.srt	00:08:41.350 --> 00:08:47.120	and then similarly we can do the dot fit here and it's gonna give summary table.
13.srt	00:08:48.960 --> 00:09:07.000	So different from linear regression that gave a lot of other metric like r squared or just r squared and many other metrics such as f statistics but here they don't have that perhaps because we don't need that in the nonlinear case.
13.srt	00:09:08.179 --> 00:09:16.519	However it does give the coefficient value and then the standard error for that and then g test instead of t test but they are kind of the same.
13.srt	00:09:17.230 --> 00:09:22.780	So here we can see that this p value is very small so this coefficient value is significant.
13.srt	00:09:26.180 --> 00:09:30.140	Another way to do it using sklearn library is bootstrapping.
13.srt	00:09:30.410 --> 00:09:32.830	So bootstrapping is like this.
13.srt	00:09:33.070 --> 00:09:39.129	As a reminder, this is the original sample and then we can resample it multiple times like this.
13.srt	00:09:42.050 --> 00:09:46.340	We can resample with the replacement so you might see some duplicate data.
13.srt	00:10:18.240 --> 00:10:21.690	samples and then we can fit the model so logistic model here here separately and then get the coefficient values and we can do the statistics conveniently there is a module exist called the bagging classifier which is essentially a wrapper so this is class inside of skl and angsangbo module and then takes the base estimator so it can be any estimator so any kinds of model not only the logistic regression but you can do linear regression or you can do tree models and others.
13.srt	00:10:21.690 --> 00:10:28.740	You can throw in here and then number of estimators means that how many times we will bootstrap and then fit the model.
13.srt	00:10:28.950 --> 00:10:35.309	And it says the bootstrap is true so it's going to use bootstrapping.
13.srt	00:10:36.059 --> 00:10:38.080	It can do the bootstrap features.
13.srt	00:10:38.139 --> 00:10:42.289	That means it can also select the features randomly but we don't need that here.
13.srt	00:10:42.879 --> 00:10:51.129	Ob score means the out of bag so it can set aside some of the bootstrap samples and then it can use it as a validation.
13.srt	00:10:52.020 --> 00:10:52.710	purposes.
13.srt	00:10:53.780 --> 00:11:00.800	And jobs we can do also a minus one then it will utilize all the computing resources that we have.
13.srt	00:11:02.440 --> 00:11:07.800	Alright so using that we can use as this.
13.srt	00:11:08.700 --> 00:11:22.009	So bagging classifier is a wrapper and inside the wrapper we're gonna throw our base estimator which is a logistic regression model and in the logistic regression model I'm gonna use weight equals balanced.
13.srt	00:11:53.480 --> 00:12:00.570	because it will give a slightly better result and then number of estimators is a thousand and then for this wrapper I'm gonna do the dot fit and throw my data here and as a result I call this object resulting object as a CLF and then I can pull some useful things from it so for example dot estimators underscore will give all the fitted model objects inside of list and since I asked for number of estimators equals thousand, it's going to have a thousand models inside.
13.srt	00:12:02.540 --> 00:12:06.019	And then I can pull some of the model's coefficients.
13.srt	00:12:06.019 --> 00:12:10.649	So for example my first model coefficient values are like this.
13.srt	00:12:10.800 --> 00:12:13.250	So I use the two features here.
13.srt	00:12:14.460 --> 00:12:18.540	So it's going to give two coefficient values and then one intercept value.
13.srt	00:12:21.620 --> 00:12:24.180	And I can pull all of this from this list.
13.srt	00:12:25.170 --> 00:12:26.490	and then do the statistics.
13.srt	00:12:26.600 --> 00:12:42.460	So I draw histogram first to see how they look like and because n is reasonably big they look like normal distributions skewed sometimes but roughly they have some mean and some width.
13.srt	00:12:43.750 --> 00:12:49.390	Alright so what do I do with this all thousand values for each coefficients?
13.srt	00:12:49.510 --> 00:12:50.580	I can do the t-test.
13.srt	00:12:50.580 --> 00:12:55.800	So there is a convenient Python package here scipy stats.
13.srt	00:12:56.870 --> 00:12:58.659	ttest one sample.
13.srt	00:12:59.060 --> 00:13:01.149	We are doing ttest for the p values.
13.srt	00:13:02.560 --> 00:13:04.060	So the usage is like this.
13.srt	00:13:04.330 --> 00:13:06.620	So I put the list of coefficients.
13.srt	00:13:06.950 --> 00:13:19.610	So I'm gonna put one kind of coefficient at a time and then it can be in a for loop by the way and then this value is the mean that it wants to compare with.
13.srt	00:13:19.690 --> 00:13:27.909	So for the hypothesis testing, the null hypothesis says that my coefficient value is zero.
13.srt	00:13:28.990 --> 00:13:30.720	Therefore, I can put 0 here.
13.srt	00:13:31.440 --> 00:13:37.019	The alternative says that my coefficient is not 0.
13.srt	00:13:38.240 --> 00:13:44.240	And to test that, we're gonna pull out the p-value and if p-value is smaller than certain threshold, I'm gonna choose 5% error.
13.srt	00:13:44.509 --> 00:13:52.019	That means if p-value is smaller than 0.025, because t-test or g-test, they have two wings.
13.srt	00:13:52.019 --> 00:13:55.929	So if p-value is smaller than this value.
13.srt	00:13:59.139 --> 00:14:03.509	That means my coefficient value is significant, right?
13.srt	00:14:03.620 --> 00:14:06.879	So the result I can pull out and print.
13.srt	00:14:07.350 --> 00:14:08.889	It has two components inside.
13.srt	00:14:09.149 --> 00:14:10.940	The t-statistic value and then the p-value.
13.srt	00:14:10.940 --> 00:14:15.480	And I can pull each of them by just doing dot and their name.
13.srt	00:14:15.480 --> 00:14:21.279	So I just pulled t-statistic value for example, but you can pull the p-value as well.
13.srt	00:14:21.279 --> 00:14:24.480	Furthermore you can console this documentation.
13.srt	00:14:30.169 --> 00:14:32.609	So as you can see, All coefficients are very significant.
13.srt	00:14:32.799 --> 00:14:37.579	Few hundreds t values away from the zero and p values are all zeros.
13.srt	00:14:37.639 --> 00:14:39.870	So all of them are significant.
13.srt	00:14:41.089 --> 00:14:52.909	And in this video we talked about how to use logistic regression module from the sklearn and then we talked about how to use the various metrics from sklearn metrics module.
13.srt	00:14:53.309 --> 00:14:57.759	We talked about also how to do the bootstrapping using the bootstrapping wrapper.
22.srt	00:00:05.719 --> 00:00:09.050	Hey everyone, in this video we're going to talk about gradient boosting.
22.srt	00:00:11.400 --> 00:00:27.300	So previously we talked about generic boosting algorithm that we iteratively add a stamp model to our initial model and each stamp model fits the data to predict the residual from each stage.
22.srt	00:00:29.190 --> 00:00:33.420	And with the shrinkage parameter we add this stamp model iteratively.
22.srt	00:00:33.800 --> 00:00:38.930	and also the residual gets smaller and smaller as we go through this iteration.
22.srt	00:00:40.150 --> 00:00:43.609	And then as an output, we're going to have the combined model.
22.srt	00:00:46.350 --> 00:00:50.219	Gradient boosting is a generalization of this boosting algorithm.
22.srt	00:00:51.939 --> 00:01:00.240	Instead of fitting the residual, which is y minus fx at each stage, we're going to use gradient of a loss function.
22.srt	00:01:00.740 --> 00:01:06.170	So if you remember loss function is some generalization form of measuring some error.
22.srt	00:01:07.269 --> 00:01:17.920	So we're going to measure an error by having a data x and y and our prediction yp, which is essentially the fx.
22.srt	00:01:19.710 --> 00:01:23.439	So this can be MSE or RSS in the regression.
22.srt	00:01:23.439 --> 00:01:25.260	So for example, something like this.
22.srt	00:01:28.849 --> 00:01:32.400	Or some other function if it's classification.
22.srt	00:01:32.480 --> 00:01:35.650	So this loss function can be very general form.
22.srt	00:01:36.480 --> 00:01:41.530	And this can be a measure of error, but loss function is more generalized form.
22.srt	00:01:42.159 --> 00:01:54.199	So by measuring the gradient of loss function with respect to our change of model at each iteration, we can measure the gradient of the loss function.
22.srt	00:01:54.949 --> 00:02:03.519	And the goal is to fit our tree to predict the negative gradient minus g instead of just pure residual.
22.srt	00:02:04.339 --> 00:02:07.629	So that's a little bit difference from our previous model.
22.srt	00:02:08.659 --> 00:02:10.280	And everything else is the same.
22.srt	00:02:11.629 --> 00:02:13.579	So we're going to see more in detail here.
22.srt	00:02:14.689 --> 00:02:19.229	So we start by fitting our initial model to minimize the loss function.
22.srt	00:02:20.530 --> 00:02:27.250	This is something similar to minimizing entropy or minimizing MAC loss for regression in decision tree.
22.srt	00:02:27.959 --> 00:02:29.189	So we're going to have some split.
22.srt	00:02:29.989 --> 00:02:41.849	And then for each iteration, we're going to calculate the negative gradient, which is again gradient of loss function with respect to the change of this function.
22.srt	00:02:43.329 --> 00:02:43.969	And with this...
22.srt	00:02:44.120 --> 00:02:50.840	gradient value, we're gonna fit the stump tree to this training data to predict this negative gradient value.
22.srt	00:02:52.610 --> 00:03:03.229	And this will give some set of parameters while it's fitting and then we will update our loss function using this updated parameter values.
22.srt	00:03:05.379 --> 00:03:13.900	And also we're gonna update the function and as we go this iteration, we're gonna have this additive model as a result.
22.srt	00:03:17.819 --> 00:03:22.710	So let's talk about why we want to use a gradient instead of just a residual.
22.srt	00:03:24.039 --> 00:03:38.810	So if we use just generic boosting algorithm, which is a greedy algorithm, which will look into all the possible split of a stump or small tree, and then it will pick one that gave the best split.
22.srt	00:03:39.389 --> 00:03:44.770	That means it will choose the parameters such that the reduction in residual is the biggest.
22.srt	00:03:46.580 --> 00:03:58.070	So, measuring gradient of this multi-dimensional space is very similar to this greedy approach, but it's even better because it's going to choose the direction that's the steepest descent.
22.srt	00:03:58.070 --> 00:04:06.890	So steepest descent in terms of reducing the loss function.
22.srt	00:04:19.139 --> 00:04:27.500	And when you think about classification problem where we chose some different function like entropies or Gini in decision tree classifier instead of whether it's right or wrong, which is residual in classifier, that is more true to how the decision tree split happens.
22.srt	00:04:27.790 --> 00:04:30.819	So having loss function is more expressive in that way.
22.srt	00:04:34.790 --> 00:04:44.670	Okay, so I'm trying to convince you that the gradient boosting should in theory work better than four stepwise or generic boosting algorithm.
22.srt	00:04:44.980 --> 00:04:47.840	So let's have some comparison.
22.srt	00:04:48.189 --> 00:04:50.050	So I prepared two data.
22.srt	00:04:50.780 --> 00:04:52.810	each of which are very similar to each other.
22.srt	00:04:54.300 --> 00:05:03.000	So they are they have a small number of features, 13 features versus 20 features and they have approximately 5,000 or more samples.
22.srt	00:05:03.000 --> 00:05:14.920	The data one is a little bit difficult so having fully grown decision tree will give about 61% accuracy whereas data 2 it's a little easier.
22.srt	00:05:14.980 --> 00:05:20.410	So that decision tree fully grown will give performance of 89% accuracy.
22.srt	00:05:23.710 --> 00:05:34.980	So even though number of features and the number of samples are similar, sometimes depending on how one or more features are a good predictor of the target variable, things can be different.
22.srt	00:05:37.370 --> 00:05:42.500	But as you can imagine, the gradient boosting is much better than decision tree already.
22.srt	00:05:43.830 --> 00:05:45.320	But how about AdaBoost?
22.srt	00:05:45.500 --> 00:05:48.320	But how about comparing to AdaBoost?
22.srt	00:05:48.990 --> 00:05:54.700	So if we compare to AdaBoost, the data one on the data one gives similar result.
22.srt	00:05:55.329 --> 00:06:02.150	Both of AdaBoost and Gradient Boosting gave much better results than just the Decision Tree.
22.srt	00:06:02.150 --> 00:06:08.480	In Data 2, much better results than Decision Tree alone.
22.srt	00:06:08.520 --> 00:06:13.100	However, you can see the Gradient Boosting works slightly better than AdaBoost.
22.srt	00:06:16.780 --> 00:06:22.300	So the conclusion is that whether the Gradient Boosting is always better than AdaBoost, it depends on the data.
22.srt	00:06:22.949 --> 00:06:28.030	But most of time, it is likely to be better performing than AdaBoost.
22.srt	00:06:29.510 --> 00:06:34.110	Also, gradient boosting is less sensitive to mislabeled data.
22.srt	00:06:35.100 --> 00:06:41.280	So for example, AdaBoost is sensitive to mislabeled data because it uses a weight to each data samples.
22.srt	00:06:41.600 --> 00:06:45.800	Therefore, if the label is wrong, it's likely to suffer.
22.srt	00:06:45.830 --> 00:06:49.250	However, gradient boosting doesn't have that problem.
22.srt	00:06:55.070 --> 00:06:56.370	How about some other aspects?
22.srt	00:06:57.220 --> 00:07:02.950	So these graphs were generated at a different learning rate.
22.srt	00:07:04.939 --> 00:07:13.340	As you know from previous video, any boosting algorithm can deteriorate if learning rate is too high and number of trees are too many.
22.srt	00:07:15.040 --> 00:07:25.300	So in order to prevent overfitting, when we have a large number of trees in additive model like boosting algorithm, we need to reduce the learning rate.
22.srt	00:07:25.710 --> 00:07:34.610	So this graph shows that, and then you can see that both the boost and gradient boosting, they require smaller learning rate as the number of trees increases.
22.srt	00:07:36.800 --> 00:07:42.629	This one is time, so I ran a five-fold cross-validation for each model.
22.srt	00:07:42.659 --> 00:07:51.990	In this case, gradient boosting was time-efficient than AdaBoost, but just empirically speaking, it depends on the data.
22.srt	00:07:51.990 --> 00:08:05.730	And also, you have to keep in mind that AdaBoost uses a stump, which means the max steps equals 1, whereas a gradient boosting SK-1 library, they by default use max steps equals 3.
22.srt	00:08:08.259 --> 00:08:11.790	Now let's talk about performance comparison with the random forest even.
22.srt	00:08:12.629 --> 00:08:26.730	So the data one, which was a difficult case, we saw that the random forest didn't do much better than decision tree and boosting algorithms were much better than random forest.
22.srt	00:08:27.170 --> 00:08:36.549	Whereas this little bit easier data with the data two, all of the ensemble algorithm did better, much better than just a decision tree.
22.srt	00:08:40.379 --> 00:08:44.970	So can you say random forest which is a parallel ensemble algorithm versus boosting algorithm.
22.srt	00:08:45.669 --> 00:08:46.850	Which one would be better?
22.srt	00:08:47.850 --> 00:08:57.799	It is difficult to tell when we have such small number of features because when the random forest really shines is when the number of features are a lot.
22.srt	00:08:58.309 --> 00:09:07.590	So I prepared the data3 which has 145 features, which is a lot more features than previous data and has 3000 samples.
22.srt	00:09:12.100 --> 00:09:17.480	And single-disk entry performance is almost 70%, which is kind of medium difficulty.
22.srt	00:09:18.279 --> 00:09:21.019	Then ran three different ensemble models.
22.srt	00:09:21.080 --> 00:09:25.779	As you can see, Random Forest did better than boosting algorithm.
22.srt	00:09:27.420 --> 00:09:30.540	Now you have some sense of when to use which algorithm.
22.srt	00:09:31.509 --> 00:09:36.070	When you have a lot of features, Random Forest will work better.
22.srt	00:09:36.639 --> 00:09:43.389	When you have a smaller number of features, usually the gradient boosting will do better.
22.srt	00:09:45.019 --> 00:09:51.220	And as I mentioned before, it all depends on data too, but in general, that's the trend.
22.srt	00:09:53.750 --> 00:09:55.500	We can also think about the time.
22.srt	00:09:56.950 --> 00:10:05.799	So as you can see, the boosting algorithm takes much longer time than random forest and it's not surprising because we have a lot of number of features.
22.srt	00:10:06.169 --> 00:10:14.649	All the gradient boosting algorithms, they inspect all the features, whereas the random forest will take only subset of features and by default is square root.
22.srt	00:10:14.840 --> 00:10:25.140	So about 12 features they will only look at and the other boosting algorithm they will look at all 145 features here.
22.srt	00:10:44.910 --> 00:10:57.910	So there's an interesting feature in gradient boosting in sklearn library that it can take an option called max features so you can actually set it to random sample the features whereas other boost algorithm doesn't have this option so It will consider all the number of features, whereas gradient boosting, if you set to do something similar to random foresting, it will run faster, so you can save some time at the expense of a slight performance drop.
22.srt	00:10:57.910 --> 00:11:03.470	However, I think if you have a lot of features, it can be worthwhile.
22.srt	00:11:06.730 --> 00:11:10.690	So let me just mention briefly other useful packages.
22.srt	00:11:11.080 --> 00:11:15.610	XGBoost is an external library, so it's not part of sklearn.
22.srt	00:11:15.940 --> 00:11:19.200	However, it's a separate library that can be useful.
22.srt	00:11:20.730 --> 00:11:40.500	XGBoost is a acronym for Extreme Gradient Boost and nothing very different from gradient boosting, but they implement some other tricks such as regularization and random sampling of the data and random sampling of features like random forest do.
22.srt	00:11:40.710 --> 00:11:45.750	So, XWBoost is time efficient, also provides a good performance because of built-in regularization.
22.srt	00:11:51.250 --> 00:11:59.640	Light GBM is another external package that's not part of sklon and it makes the boosting faster by binning the value of each feature.
22.srt	00:11:59.640 --> 00:12:13.600	So if the feature has some continuous values a lot instead of looking into all these chopped values, it can bin larger size like this so it can split faster so that way it can be useful.
22.srt	00:12:24.350 --> 00:12:27.570	sklon also has a counterpart to this one so I think you can get similar results from sklon library ExtraTree is similar to RandomForest.
22.srt	00:12:27.840 --> 00:12:29.930	It's also part of sklearn library.
22.srt	00:12:30.260 --> 00:12:33.640	ExtraTree means extreme randomized tree.
22.srt	00:12:33.900 --> 00:12:37.170	It works very similarly to RandomForest in sklearn.
22.srt	00:12:37.300 --> 00:12:41.450	And the only difference is that it doesn't do bagging.
22.srt	00:12:41.740 --> 00:12:42.600	So no bagging.
22.srt	00:12:45.030 --> 00:12:47.690	But it still randomly sampled the features.
22.srt	00:12:48.400 --> 00:12:51.240	And also why it's extreme randomized?
22.srt	00:12:53.240 --> 00:12:57.080	Because it picks split value randomly instead of doing the best split.
22.srt	00:12:57.830 --> 00:13:01.850	Here is a full list of ensemble models in SKLearn libraries.
22.srt	00:13:02.570 --> 00:13:14.010	So we talked about AdaBoost and they have both classification and regression and bagging classifier would be random for something like random forest without random sampling on features.
22.srt	00:13:14.500 --> 00:13:19.210	So it has just a bagging part and extractory classifier it's the opposite.
22.srt	00:13:24.690 --> 00:13:25.200	So it does not have bagging but it random samples on the features.
22.srt	00:13:26.450 --> 00:13:31.480	Gradient boosting, we talked about it, and random forest we also mentioned.
22.srt	00:13:31.480 --> 00:13:41.880	There are some other more complicated stuff and this heat gradient boosting would be something equivalent to light GBM.
22.srt	00:13:44.540 --> 00:13:48.280	As a recap, we talked about ensemble method in this module.
22.srt	00:13:48.810 --> 00:13:53.220	Ensemble methods are ways to strengthen the decision tree model.
22.srt	00:13:53.580 --> 00:13:55.840	Decision tree model is a weak learner.
22.srt	00:13:56.330 --> 00:14:01.690	So it can overfeed and overall its performance isn't very good.
22.srt	00:14:01.690 --> 00:14:08.000	However, by taking parallel ensemble or serial ensemble, we can make the performance better.
22.srt	00:14:08.610 --> 00:14:13.020	So parallel ensemble, we talked about random forest.
22.srt	00:14:13.110 --> 00:14:25.050	So random forest is a parallel method, ensemble method, and we also talked about boosting method, which is a serial.
22.srt	00:14:27.340 --> 00:14:27.620	ang-sang-bulling method.
22.srt	00:14:27.620 --> 00:14:32.470	So this is just growing different trees, randomized.
22.srt	00:14:32.470 --> 00:14:39.710	They look different because we random sample data and features and then we just average them, right?
22.srt	00:14:39.710 --> 00:14:56.480	And in boosting, we use a smaller tree like stump and try to fit to the residual and then we additively add these small models to create a stronger model.
22.srt	00:15:01.560 --> 00:15:06.180	And we also talked about when to use this random forest versus boosting.
22.srt	00:15:06.180 --> 00:15:10.480	So random forest usually works better when there is a large number of features.
22.srt	00:15:10.880 --> 00:15:12.750	So number of features is large.
22.srt	00:15:14.260 --> 00:15:19.120	Whereas boosting can take longer because it's additive.
22.srt	00:15:19.120 --> 00:15:23.410	So we prefer using when the number of features are smaller.
22.srt	00:15:23.490 --> 00:15:30.370	However, it can also take advantage of random subsampling of features by using the max features option.
22.srt	00:15:31.610 --> 00:15:34.760	Ok, so this is the end of Triangul models.
22.srt	00:15:34.940 --> 00:15:37.540	We'll talk about Conner method in the next module.
23.srt	00:00:05.509 --> 00:00:06.160	Hello everyone.
23.srt	00:00:06.400 --> 00:00:09.550	In this video, we're going to talk about support vector machine.
23.srt	00:00:12.779 --> 00:00:14.259	So let's review briefly.
23.srt	00:00:14.289 --> 00:00:17.660	So in machine learning, we have different learning tasks.
23.srt	00:00:18.280 --> 00:00:21.339	So in this class, we focus on supervised learning.
23.srt	00:00:21.739 --> 00:00:24.920	That means given the data, we would like to predict the labels.
23.srt	00:00:25.920 --> 00:00:31.690	And this prediction task have two different categories such as regression and classification.
23.srt	00:00:31.980 --> 00:00:39.090	Regression means that the prediction value would be real valued, whereas classification, the prediction value would be the categories.
23.srt	00:00:40.460 --> 00:00:44.460	And we talked about binary class classification and multi-class classification.
23.srt	00:00:44.460 --> 00:00:49.750	And according to these different tasks, there are different models that we can apply.
23.srt	00:00:49.750 --> 00:00:53.390	So for example, linear regression applies to regression problems.
23.srt	00:00:53.390 --> 00:01:00.980	And logistic regression, although the name says regression, it is for binary class classification.
23.srt	00:01:00.980 --> 00:01:09.920	And we talked about we can generalize logistic regression using Softmax, and then we can do the multi-class classification.
23.srt	00:01:11.520 --> 00:01:19.800	or we can apply a logistic regression model to do the multi-class classification if we choose one class versus the other ones.
23.srt	00:01:19.800 --> 00:01:27.670	And then we moved on to non-parametric models such as a k-nearly neighbor and decision trees.
23.srt	00:01:28.050 --> 00:01:40.000	k-nearly neighbor doesn't have a parameter unlike linear regression or logistic regression, and it is one of the most simplest models in machine learning, and it can do both regression and classification.
23.srt	00:01:45.890 --> 00:01:50.300	Decision trees are weak learners, but it's very flexible and it's easy to interpret.
23.srt	00:01:51.000 --> 00:01:53.140	It can also do regression and classification.
23.srt	00:01:54.850 --> 00:01:59.050	And also we talked about the angsangbul method, which can apply to any model.
23.srt	00:01:59.200 --> 00:02:06.490	However, it is most beneficial for decision trees because decision trees are weak learners and by angsangbuling them, they can be a strong learner.
23.srt	00:02:07.330 --> 00:02:16.129	So for example, we talked about parallel angsangbul method, which is random forest, which we grow the trees in a decorrelated way and then average them.
23.srt	00:02:17.310 --> 00:02:22.010	Another method that we talked about was serial ensembleing method, which is a boosting method.
23.srt	00:02:22.500 --> 00:02:28.140	So instead of growing the full tree, we let them grow very slowly and small one at a time.
23.srt	00:02:29.010 --> 00:02:38.100	So we talked about adding a stump, which has one or just a few decision splits, and then we additively added them with some learning rate.
23.srt	00:02:38.100 --> 00:02:46.810	The rest of the class will talk about SVM, which is another powerful non-parametric model.
23.srt	00:02:48.390 --> 00:02:52.969	And there are some other supervised learning models that can perform well, such as a neural network.
23.srt	00:02:53.210 --> 00:03:01.560	However, we won't have a time to go deeply into neural network in this course, so we'll skip that.
23.srt	00:03:06.000 --> 00:03:10.009	Let's briefly talk about hyper parameters and what's the criteria.
23.srt	00:03:10.439 --> 00:03:11.909	So a little bit in depth.
23.srt	00:03:17.680 --> 00:03:25.090	So linear regression, there was no hyper parameters, but we need to design in the feature space how many features we want to include, how many high order terms that we want to include.
23.srt	00:03:25.500 --> 00:03:31.419	That is domain of more feature engineering, but it can be a design consideration.
23.srt	00:03:32.740 --> 00:03:34.519	And linear regression has parameters.
23.srt	00:03:34.689 --> 00:03:42.169	So, you know, w1 x1 plus w2 x2 plus intercept.
23.srt	00:03:42.769 --> 00:03:46.129	That could be, so all these w's are parameters.
23.srt	00:03:47.490 --> 00:03:52.269	Loss function for linear regression, we talked about MSC loss.
23.srt	00:03:53.160 --> 00:03:54.670	similarly RSS.
23.srt	00:03:55.760 --> 00:03:57.670	Those are loss functions that we use.
23.srt	00:03:58.520 --> 00:04:08.240	Logistic regression is very similar to linear regression except that it has a sigmoid function that threshold the probability at the end.
23.srt	00:04:08.860 --> 00:04:19.340	So there is no hyper parameter and again there is a design consideration such as how many features that we want to include and how many higher order terms that we want to include.
23.srt	00:04:20.040 --> 00:04:22.060	And parameters they are the same.
23.srt	00:04:24.129 --> 00:04:39.740	We have the same form of this and then there is a threshold, sigmoid threshold at the end, but these are the parameters and it's very much same as linear regression.
23.srt	00:04:40.800 --> 00:04:44.780	For loss function in logistic regression, it uses a binary cross entropy.
23.srt	00:04:47.810 --> 00:04:51.830	And in KNN, the K is the hyper parameter.
23.srt	00:04:51.830 --> 00:05:01.019	K means the number of neighbors that we want to consider when we decide whether a point around some other points are certain class.
23.srt	00:05:02.070 --> 00:05:10.600	Alright, and there is no parameter because KNN is a non-parametric model and there's no loss function because there is no optimization going on.
23.srt	00:05:10.600 --> 00:05:13.600	However, there is a some kind of rule how to decide.
23.srt	00:05:13.600 --> 00:05:28.400	So when there are neighbors like this, then this point here would be having more neighbors around this with this X class.
23.srt	00:05:28.620 --> 00:05:30.980	So it will classify this X.
23.srt	00:05:32.079 --> 00:05:39.670	So, in KNN, to determine which neighbors are close by, it uses a distance metric such as Euclidean distance.
23.srt	00:05:40.259 --> 00:05:47.360	So, KNN doesn't have loss function, therefore no optimization, however, it uses a distance metric in order to make a decision.
23.srt	00:05:48.920 --> 00:05:51.870	And decision trees is again non-parametric models.
23.srt	00:05:51.870 --> 00:05:54.519	So, there is no parameters, therefore there is no optimization.
23.srt	00:06:03.430 --> 00:06:05.820	However, decision trees have hyperparameters such as max-depths and What's the minimum samples in the terminal node?
23.srt	00:06:06.310 --> 00:06:07.040	Things like that.
23.srt	00:06:08.240 --> 00:06:16.730	And as optionally, if you were to do some pruning, there was something called the CCP alpha, which is set the threshold of pruning criteria.
23.srt	00:06:19.740 --> 00:06:25.780	So there was no parameter for decision trees because it doesn't have explicit optimization process.
23.srt	00:06:26.240 --> 00:06:30.000	However, it requires some criteria for splitting.
23.srt	00:06:34.560 --> 00:06:47.189	So if you remember, when the samples are in one box, when split, the decision tree models go through all these features and pick the split value of that feature which that minimize this criteria function.
23.srt	00:06:47.850 --> 00:07:05.900	So this criteria function was something like Gini index and entropy for classification MSC or RSS for regression tests.
23.srt	00:07:09.589 --> 00:07:12.750	And then we also talked about ang-sang-buling method that derives from this decision trees.
23.srt	00:07:12.750 --> 00:07:16.550	So ang-sang-buling method, they all share similar hyperparameters as decision trees.
23.srt	00:07:16.550 --> 00:07:28.069	And on top of that, they can have different, they have additional hyperparameters such as number of trees because it's going to ang-sang-bul, you know, several number of trees.
23.srt	00:07:28.800 --> 00:07:32.870	Or for boosting, it can have also learning rate.
23.srt	00:07:32.939 --> 00:07:37.550	And again, there is no parameters for this ang-sang-buling method.
23.srt	00:07:41.670 --> 00:07:47.740	And the criteria function, decision split criteria, they have the same criteria functions as decision trees.
23.srt	00:07:50.319 --> 00:07:58.709	In SVM that we're going to talk about, there is one hyperparameter called the C parameter, which we'll talk about what the role of this C parameter is.
23.srt	00:07:59.389 --> 00:08:03.339	And there is no parameter because SVM is also a non-parametric method.
23.srt	00:08:03.339 --> 00:08:09.060	However, SVM has an internally have some optimization process.
23.srt	00:08:11.679 --> 00:08:21.509	And neural networks, although we're not going to talk about deeply here, they have both parameters and hyperparameters and loss functions as well.
23.srt	00:08:23.099 --> 00:08:25.370	Alright, so let's talk about the supervector machine.
23.srt	00:08:25.859 --> 00:08:28.549	So here are some few facts about the supervector machine.
23.srt	00:08:28.689 --> 00:08:32.169	It uses a hyperplane to make a decision boundary.
23.srt	00:08:32.439 --> 00:08:36.000	We'll talk about it more later in this lecture.
23.srt	00:08:36.459 --> 00:08:41.039	And uses a kernel which is a function that applies on feature space.
23.srt	00:08:41.529 --> 00:08:45.329	And especially it's useful when we deal with the high dimensional.
23.srt	00:08:45.740 --> 00:08:48.320	feature space such as images or text.
23.srt	00:08:49.409 --> 00:09:02.620	So for example, instead of doing feature engineering on image pixels, we can apply some functions such as finding similarity between some pixel patches and then that way we can save some computation.
23.srt	00:09:03.200 --> 00:09:11.170	Because of that, support vector machine was widely used and developed during the 90s before the neural network became very popular.
23.srt	00:09:17.159 --> 00:09:18.559	It uses some mathematical kernel tricks to deal with the high dimensional data such as images.
23.srt	00:09:21.259 --> 00:09:24.360	And it is one of the high performing off-the-shelf machine learning method.
23.srt	00:09:24.360 --> 00:09:31.429	So all of the three ensemble methods, support vector machine and neural network, they are popular high performing method.
23.srt	00:09:31.429 --> 00:09:41.500	Support vector machines can do regression and classification and especially it works natively on binary class classification.
23.srt	00:09:41.500 --> 00:09:47.429	However, we can also use one versus the other method to do the multi-class classification.
23.srt	00:09:51.409 --> 00:09:54.129	Well, so let's talk about binary class classification.
23.srt	00:09:54.470 --> 00:09:56.340	It is essentially a yes or no problem.
23.srt	00:09:56.830 --> 00:10:09.120	So for example, it could be some problem like whether this credit card user will pay the debt or not, or this insurance claim is fraudulent or not, or maybe this email is spam or not.
23.srt	00:10:21.039 --> 00:10:22.750	And it can be medical diagnosis problem, whether this patient has certain disease or not, whether the patient will survive or not, whether this customer will continue for the service or not.
23.srt	00:10:23.539 --> 00:10:31.169	And as you know already, the binary class classification can take any data format as long as the label is yes or no.
23.srt	00:10:32.169 --> 00:10:42.259	So for example, image recognition can be binary class classification whether the object in the driving scene is a pedestrian or not, something like that.
23.srt	00:10:43.009 --> 00:10:49.490	Also, we can also do binary class classification on text data such as sentiment analysis.
23.srt	00:10:50.909 --> 00:10:57.389	And previously, we talked about logistic regression as a simplest model to do the binary class classification.
23.srt	00:11:05.049 --> 00:11:20.629	And as you know, this curve is a representation of a probability which is actually sigmoid function as a function of G. So this is a G and G is called logit and described by this linear combination of feature x with the weight and bias like in the linear regression.
23.srt	00:11:21.330 --> 00:11:27.870	And when G is 0, the probability of the sigmoid function becomes 0.5.
23.srt	00:11:28.309 --> 00:11:29.970	Therefore, it becomes a decision boundary.
23.srt	00:11:35.669 --> 00:11:53.279	And previously we talked about this decision boundary can be a threshold point when it's only one-dimensional feature space or it can be a line like this when it's a two-dimensional feature space and it can be a plane in the three-dimensional space or hyperplane when it's a multi-dimensional space.
23.srt	00:11:53.279 --> 00:11:57.159	So, now you know what the hyperplane is.
23.srt	00:11:57.159 --> 00:12:02.840	Now, the question is how do we find this hyperplane that becomes a decision boundary using SVM?
23.srt	00:12:08.190 --> 00:12:13.509	And we would like to find the hyperplane that separates the data points according to the right class like this.
23.srt	00:12:15.529 --> 00:12:23.649	But depending on how the data points are distributed, there could be more than one way to separate those data points.
23.srt	00:12:24.080 --> 00:12:29.980	So for example, this can be a perfect choice, but also this can be a good choice.
23.srt	00:12:31.779 --> 00:12:36.330	And this hyperplane can also separate the data perfectly.
23.srt	00:12:38.070 --> 00:12:40.639	So the question is which hyperplane should we choose?
23.srt	00:12:42.930 --> 00:12:50.820	And we're going to introduce a classifier called the maximum margin classifier and sometimes it is just called hard margin SVM.
23.srt	00:12:53.270 --> 00:12:59.460	So one thing that we can consider is that we want to train our model such that it can generalize better.
23.srt	00:13:00.140 --> 00:13:07.500	That means if we have another new data point like this, our model should be able to classify that correctly.
23.srt	00:13:08.070 --> 00:13:10.580	In other words, we would like to have a hyperplane.
23.srt	00:13:11.009 --> 00:13:13.830	that's less likely to misclassify the new data.
23.srt	00:13:15.540 --> 00:13:16.660	And how can you achieve that?
23.srt	00:13:16.970 --> 00:13:19.750	We can select the hyperplane that has the biggest margin.
23.srt	00:13:20.570 --> 00:13:22.110	So let's see what that means.
23.srt	00:13:23.280 --> 00:13:24.680	So here is the data again.
23.srt	00:13:25.310 --> 00:13:27.570	And let's say this is the hyperplane.
23.srt	00:13:29.350 --> 00:13:33.420	And these points are closest to the hyperplane.
23.srt	00:13:35.070 --> 00:13:36.410	And those are called support.
23.srt	00:13:41.690 --> 00:13:42.740	And the distance between the hyperplane to those support closes the points.
23.srt	00:13:42.860 --> 00:13:44.930	I'll call it margins.
23.srt	00:13:45.690 --> 00:13:46.560	These are margins.
23.srt	00:13:47.899 --> 00:13:54.350	The maximum margin classifier learns how to maximize the distance between the hyperplane and its supports.
23.srt	00:13:56.320 --> 00:14:01.279	Let's talk about how the maximum margin classifier finds a hyperplane.
23.srt	00:14:01.750 --> 00:14:05.899	Initially, because it doesn't know the right hyperplane, it's going to look like this.
23.srt	00:14:12.400 --> 00:14:13.740	It randomly chose a hyperplane which makes this pointer the wrong side of the margin.
23.srt	00:14:14.550 --> 00:14:18.640	When data points are wrong side of margin, it will make the loss function bigger.
23.srt	00:14:20.460 --> 00:14:24.190	And the optimizer in the SVM will try to reduce this error.
23.srt	00:14:25.060 --> 00:14:28.290	So it will adjust the coefficients of the hyperplane equation.
23.srt	00:14:29.000 --> 00:14:31.230	So now the hyperplane looks like this.
23.srt	00:14:31.480 --> 00:14:37.640	We still find the data points that are wrong side of the margin, but it is a smaller error compared to the previous one.
23.srt	00:14:38.030 --> 00:14:39.400	So smaller loss function.
23.srt	00:14:39.950 --> 00:14:42.980	And again the optimizer will try to reduce the error.
23.srt	00:14:43.680 --> 00:14:46.909	and updates its hyperplane and that look like this.
23.srt	00:14:47.090 --> 00:14:56.889	So when we go this iteration over and over again, finally the hyperplane will be optimized such that the margin between the supports are maximized.
23.srt	00:15:00.070 --> 00:15:01.730	Alright, here is a short quiz.
23.srt	00:15:02.060 --> 00:15:05.899	What happens to the separating hyperplane if we add a new data point?
23.srt	00:15:06.389 --> 00:15:09.889	The answer is that it depends where the data points get added.
23.srt	00:15:14.569 --> 00:15:19.169	So for example, if the new data point like this are added outside of the margin, it will not do anything about the hyperplane.
23.srt	00:15:19.740 --> 00:15:27.509	However, if the data points are added inside the margin or even the wrong side of the margin, the hyperplane must change.
23.srt	00:15:29.559 --> 00:15:35.000	Let's say we have new data points like this and obviously it's the wrong side of the margin.
23.srt	00:15:35.949 --> 00:15:38.649	The blue points should be upper to the hyperplane.
23.srt	00:15:38.699 --> 00:15:43.269	However, this new data point is the wrong side below the hyperplane.
23.srt	00:15:43.759 --> 00:15:46.929	So we need to relax the condition of having hard margin.
23.srt	00:15:48.860 --> 00:15:53.629	And therefore, another method called the soft margin classifier can be useful in this case.
23.srt	00:15:54.539 --> 00:15:56.389	So we'll talk about that in the next video.
21.srt	00:00:05.719 --> 00:00:06.360	Hello everyone.
21.srt	00:00:06.450 --> 00:00:09.130	In this video, we're going to talk about AdaBoost algorithm.
21.srt	00:00:10.279 --> 00:00:17.629	So previously we talked about generic boosting algorithm, which iteratively fit the stump tree to the data to predict the residual.
21.srt	00:00:18.510 --> 00:00:26.140	And then this each stump from each iteration is added together with some shrink parameter lambda here.
21.srt	00:00:26.829 --> 00:00:31.719	And this lambda helped the model to learn slowly so that we can avoid overfitting.
21.srt	00:00:36.109 --> 00:00:38.310	There are many variants of boosting algorithms.
21.srt	00:00:38.600 --> 00:00:41.339	However, these two are most used and most popular.
21.srt	00:00:41.509 --> 00:00:42.729	So we'll talk about those.
21.srt	00:00:43.589 --> 00:00:46.479	So first algorithm we'll talk about is called AdaBoost.
21.srt	00:00:46.979 --> 00:00:49.989	AdaBoost is originally developed for classification.
21.srt	00:00:50.479 --> 00:00:54.079	However, later it was developed to also do regression as well.
21.srt	00:00:54.269 --> 00:00:58.640	What makes AdaBoost interesting is that it uses weights to data samples.
21.srt	00:00:58.879 --> 00:01:06.640	That means it will make some more emphasis on the misclassified samples so that you can learn more from these errors.
21.srt	00:01:38.890 --> 00:01:43.490	and this dump fits to y instead of residual and then because it's a classification it gives a discrete values but instead of 0 or 1 we're gonna use minus 1 or 1 and then it uses exponential weight to update the data sample weights alright so Adabo's algorithm we want to have a classifier that gives a minus 1 or 1 and this This model is a linear combination of this stump model and B is the iteration.
21.srt	00:01:45.020 --> 00:01:57.430	And a little difference from the genetic boosting algorithm, this lambda B now in AdaBoost is not the shrinkage parameter, but it's kind of representing this model importance from each iteration.
21.srt	00:01:59.250 --> 00:02:07.640	So this algorithm starts by initializing all the sample weights to 1 over n, which means that all the data points are equally important.
21.srt	00:02:08.890 --> 00:02:10.700	And then we're going to repeat for B times.
21.srt	00:02:11.150 --> 00:02:25.650	that we fit the stump tree to the training data to predict the label instead of residual with the sample weight w. If you remember, the stump model is actually decision tree.
21.srt	00:02:26.510 --> 00:02:31.450	Decision tree can use a sample weight when calculating the split criteria.
21.srt	00:02:32.180 --> 00:02:42.980	So after fitting this stump model, using this stump model, and here is that, and then we compare how much accurate it is.
21.srt	00:02:44.670 --> 00:02:53.170	So this i is an identity function which will give 0 when it's correctly classified and which will give 1 when it's misclassified.
21.srt	00:02:53.770 --> 00:02:58.280	So this means that we calculate the error only using misclassified examples.
21.srt	00:02:59.550 --> 00:03:02.270	And the first iteration, these weights are all equal.
21.srt	00:03:02.650 --> 00:03:05.690	However, it's going to be updated as we go.
21.srt	00:03:09.120 --> 00:03:19.520	And using this error, we're going to calculate the model coefficient, lambda b, which again tells how much we should include this stump model into the total model.
21.srt	00:03:20.090 --> 00:03:22.130	and this lambda is given by this formula.
21.srt	00:03:22.450 --> 00:03:30.620	And sometimes you're gonna see one half in front of this formula, which is also popular convention, but with or without, it's fine.
21.srt	00:03:32.000 --> 00:03:45.350	And using this model coefficient, we're going to update the sample weight, and this sample again, when there was a misclassification, the weight of that sample becomes larger by this exponential factor.
21.srt	00:03:46.520 --> 00:03:52.320	And after we do the iteration for b times, we finally get our output model that looks like this.
21.srt	00:03:52.460 --> 00:03:54.870	The linear combination of this stump model.
21.srt	00:03:56.270 --> 00:03:58.370	And then the final sign is given by that.
21.srt	00:04:00.719 --> 00:04:03.280	So here is a brief example with the picture.
21.srt	00:04:03.380 --> 00:04:13.250	So initialize sample weights W. So in this data, these are the features and this is the target Y and this is the initial weight.
21.srt	00:04:14.020 --> 00:04:22.830	And then for this iteration, we're going to fit the stump model to training data with some sample weights.
21.srt	00:04:22.920 --> 00:04:26.139	And it's going to give some kind of output like this.
21.srt	00:04:27.480 --> 00:04:30.930	And then we notice that these two samples are misclassified.
21.srt	00:04:32.180 --> 00:04:38.829	Therefore, when we calculate the error, it's going to give a 0.2, so 2 misclassifications out of 10 examples.
21.srt	00:04:38.829 --> 00:04:44.720	And then we further calculate the model coefficients and it gives this value.
21.srt	00:04:46.350 --> 00:04:52.230	By the way, this function can go from minus infinity to infinity.
21.srt	00:04:56.680 --> 00:05:00.350	So this parameter doesn't have to be somewhere between 0 and 1, unlike the shrinkage parameter.
21.srt	00:05:02.050 --> 00:05:09.589	And then using this model coefficients, we're going to update the weight using this exponential factor that gives this weight.
21.srt	00:05:10.069 --> 00:05:19.279	So this misclassified example receives more weight, four times more than the others, and this one as well receives a bigger weight.
21.srt	00:05:19.279 --> 00:05:27.100	And then we can normalize this weight so that these all examples, some of these weights becomes one.
21.srt	00:05:27.669 --> 00:05:31.779	Alright, so let's have a look at some usage.
21.srt	00:05:32.009 --> 00:05:37.979	So AdaBoost is available in sklearn ensemble module.
21.srt	00:05:37.979 --> 00:05:42.500	So AdaBoost and sklearn both have a classifier and regressor.
21.srt	00:05:43.149 --> 00:05:57.079	So classifier has these kind of options and base estimator is not specified and it's a decision tree classifier with the maximum depth equals one, that means it's a stump.
21.srt	00:06:00.059 --> 00:06:05.639	You can also see this learning rate on top of this lambda b which was the weight to the model.
21.srt	00:06:06.359 --> 00:06:15.039	There is also learning rate as a hyperparameter so you can reduce the learning rate if you want to make the Adaboost classifier run slowly.
21.srt	00:06:17.009 --> 00:06:21.059	And by default, the semi.r algorithm is used, which is a real Adaboost.
21.srt	00:06:21.059 --> 00:06:24.119	This r comes from real Adaboost.
21.srt	00:06:24.119 --> 00:06:28.819	They make a use of predict probability.
21.srt	00:06:30.349 --> 00:06:36.889	the probability of being each class instead of using just a binarized classifier.
21.srt	00:06:37.389 --> 00:06:41.449	So SEMI-R is advanced version of original AdaBoost algorithm SEMI.
21.srt	00:06:41.449 --> 00:06:48.399	And it is good for multi-class classifier, but it also works better for the binary class classification.
21.srt	00:06:48.399 --> 00:06:51.599	So you can just leave it as is and use it.
21.srt	00:06:51.599 --> 00:07:00.349	Here are some more resources on how this real AdaBoost algorithm, which is SEMI-R, is a little bit better than real AdaBoost algorithm.
21.srt	00:07:01.329 --> 00:07:03.669	the original discrete AdaBoost algorithm.
21.srt	00:07:04.619 --> 00:07:14.329	And again, this boosting algorithm gives much better performance than just one stump as well as the fully grown decision tree.
21.srt	00:07:17.289 --> 00:07:23.579	So being AdaBoost originally developed for the classification problem, can AdaBoost also do regression?
21.srt	00:07:23.789 --> 00:07:24.749	The answer is yes.
21.srt	00:07:25.259 --> 00:07:29.259	You just need to call the AdaBoost regressor in sklearn-angsangbul module.
21.srt	00:07:30.579 --> 00:07:32.359	And everything is very similar.
21.srt	00:07:32.619 --> 00:07:34.899	It also accepts a learning rate.
21.srt	00:07:35.709 --> 00:07:41.629	And the only difference in the regressor is that we can specify the loss function, which is by default is a linear loss.
21.srt	00:07:43.029 --> 00:07:47.969	Okay, let's talk about how good is the AdaBoost.
21.srt	00:07:49.049 --> 00:07:55.119	So I picked two different datasets, each of which have about 5000 samples and then 20 features.
21.srt	00:07:56.399 --> 00:08:02.409	And as you can see, depending on the problem difficulty, the absolute accuracy can be different.
21.srt	00:08:02.509 --> 00:08:10.569	However, regardless of its difficulty, the boosting algorithm is always better than fully grown decision trees.
21.srt	00:08:10.569 --> 00:08:14.669	So this is decision tree fully grown and this is boosting algorithm.
21.srt	00:08:15.379 --> 00:08:28.509	So this left graph being more difficult case, so overall accuracy isn't too good, but this right one is a little easier data, so they had a higher accuracy.
21.srt	00:08:29.409 --> 00:08:36.569	As you can see here, boosting algorithm can have overfitting as well if the running rate is too big and number of trees are too big as well.
21.srt	00:08:37.339 --> 00:08:40.439	So there is a trade-off between the running rate and the number of trees.
21.srt	00:08:40.439 --> 00:08:46.839	Alright, so that's it for this Adaboost video, and we're going to talk about gradient boost in the next video.
20.srt	00:00:05.360 --> 00:00:10.269	Hello everyone, in this video we're going to talk about ensemble method second part, boosting.
20.srt	00:00:12.339 --> 00:00:19.550	Previously we talked about the trees have a problem that they are weak learner and they can overfit very easily.
20.srt	00:00:20.969 --> 00:00:30.629	So the first idea we used to address this issue was let's try to ensemble them by introducing diversity.
20.srt	00:00:31.609 --> 00:00:44.750	which were trained on different subsets of data, we can have diversified trees and then hopefully the averaging this diversified tree will give better performance than just picking one single tree.
20.srt	00:00:48.010 --> 00:00:59.789	On top of that, we also add an idea that we can further de-correlate the trees, so make sure we actually pick the trees that are different from each other so that we can have a true diversification.
20.srt	00:01:03.299 --> 00:01:05.209	So the random forest used that idea.
20.srt	00:01:05.540 --> 00:01:06.689	How it did that?
20.srt	00:01:06.689 --> 00:01:19.530	So not only training the trees in the different subset of data, we also, when we sample the data, we also random sample the features and that was implemented in the random forest.
20.srt	00:01:23.469 --> 00:01:31.730	So again, bagging in random forest, bagging random samples of data, which means the row in the table and random forest also.
20.srt	00:01:31.959 --> 00:01:33.390	random sample zone features.
20.srt	00:01:35.689 --> 00:01:41.079	Therefore, it can further de-correlate the trees and both of bagging classifier and random forest.
20.srt	00:01:41.079 --> 00:01:50.579	They are parallel ang-sang-bulling method, which means the training of each tree on different subsets of data, they can be trained at the same time.
20.srt	00:01:50.579 --> 00:01:56.620	We also showed that the performance increased dramatically by ang-sang-bulling trees.
20.srt	00:01:56.620 --> 00:02:01.319	So, here is the single tree performance and here is the bagging classifier alone.
20.srt	00:02:05.369 --> 00:02:12.750	I can make this a huge difference and then on top of that if we further decorate the tree we can gain another performance increase.
20.srt	00:02:16.299 --> 00:02:25.069	Okay so we're going to introduce a second ensemble method which is a sequential ensemble whereas previously we talked about parallel ensemble.
20.srt	00:02:25.689 --> 00:02:28.299	So this sequential ensemble is called boosting.
20.srt	00:02:36.080 --> 00:02:42.990	So boosting also solves the same problem that trees are weak learner and trees overfit but instead of diversifying and averaging those different many trees, we're gonna make single tree a stronger runner.
20.srt	00:02:43.909 --> 00:02:44.909	And how do we do that?
20.srt	00:02:45.650 --> 00:03:00.710	We're gonna grow a small stump at a time to fit the error from the previous stage and then we're gonna grow another tree in another stage in the next stage to fit the error from the previous stage.
20.srt	00:03:01.329 --> 00:03:03.280	So you can think about this analogy.
20.srt	00:03:03.979 --> 00:03:12.039	When we have a big problem like this, maybe the first scientist will look at it and quickly solve the problem by this much.
20.srt	00:03:12.799 --> 00:03:27.349	leave this problem more, we need a more serious investigation, and the second scientist will ignore all this, but only look at this part, focus on this part, and then solve maybe this much.
20.srt	00:03:30.039 --> 00:03:41.519	And then the rest, the third scientist or investigator will look at it and then solve more problems and then reduce the gap of this error gradually.
20.srt	00:03:42.689 --> 00:03:44.759	So we can do the same with the small tree.
20.srt	00:03:53.329 --> 00:03:55.769	Instead of growing large tree that try to solve this big problem all at once, we're gonna have some small tree that will solve some part of this problem and then leave this error.
20.srt	00:03:56.549 --> 00:04:01.569	And the second small tree will only look at this error and try to solve it.
20.srt	00:04:02.419 --> 00:04:03.869	Then reduce the gap of error.
20.srt	00:04:05.199 --> 00:04:08.119	And third one will even further reduce the gap of the error.
20.srt	00:04:08.810 --> 00:04:11.389	So this process is called boosting.
20.srt	00:04:13.099 --> 00:04:24.089	And boosting just means that we will make one single tree to a strong learner or performs better or will boost the performance by growing the tree slowly.
20.srt	00:04:24.490 --> 00:04:25.800	link or two at a time.
20.srt	00:04:27.279 --> 00:04:31.409	So a single decision tree is grown to the maximum depth.
20.srt	00:04:32.639 --> 00:04:36.129	So it's large and try to solve the problem all at once.
20.srt	00:04:36.839 --> 00:04:44.099	Whereas boosting tree grow very simple and very little one or two depths at a time.
20.srt	00:04:44.849 --> 00:04:53.939	And the rest of the error will be fit to another small tree in the second stage and then the rest of error will be fit by another small tree.
20.srt	00:04:54.420 --> 00:05:00.250	in the third stage and we continue that and our final model will be some of these small trees.
20.srt	00:05:03.019 --> 00:05:05.089	Okay so let's have a look at algorithm.
20.srt	00:05:05.159 --> 00:05:19.550	So we're gonna initialize our model to zero that means our model doesn't know anything about our data and let's say our error is as big as the label and then we're gonna iterate for b times.
20.srt	00:05:20.769 --> 00:05:30.730	Then we'll try to fit a stump in the stage b to train data so the data x and then the label is now the residual.
20.srt	00:05:31.159 --> 00:05:36.310	In the first iteration this residual is the same as y so we try to fit the y first.
20.srt	00:05:38.129 --> 00:05:48.810	And then in the first stage since this was 0, we're gonna have our model called our stump times some constant.
20.srt	00:05:49.229 --> 00:05:50.889	This constant is less than 1.
20.srt	00:05:51.359 --> 00:06:00.120	That means we will add the stump model to our whole model by certain fraction.
20.srt	00:06:00.729 --> 00:06:07.799	And the reason why is that we want to consider our new model kind of conservatively rather than adding all of them together.
20.srt	00:06:08.919 --> 00:06:09.310	Okay?
20.srt	00:06:10.699 --> 00:06:12.919	And this helps our learning slow.
20.srt	00:06:13.129 --> 00:06:14.889	So it's something similar to learning rate.
20.srt	00:06:16.199 --> 00:06:28.969	We're gonna update the residual in the current stage that our residual is also from the previous residual minus the prediction, the shrinked prediction from our current stump model.
20.srt	00:06:38.600 --> 00:06:39.560	And then after we repeat the times, the final output model will be the sum of this shrinked stump models.
20.srt	00:06:41.100 --> 00:06:45.360	All right, so graphically it looks like this.
20.srt	00:07:11.310 --> 00:07:14.750	So here's the data and then it feeds to our first stump model and then the stump model will predict the prediction and in first stage it will be compared against the label and then its difference that we define as here, we will get the residual from the first stage and from the second stage we build another stump model and try to fit the data.
20.srt	00:07:15.520 --> 00:07:29.879	It will predict the residual predicted and we're gonna also take this residual from the first stage as a label and then we'll get the error to produce the residual from the second stage.
20.srt	00:07:30.660 --> 00:07:41.680	And as you can guess, I will continue that with the third stump model to the data and then I'll try to predict this R2.
20.srt	00:07:42.379 --> 00:07:48.060	and we're gonna have another residual from the third stage and so on.
20.srt	00:07:53.819 --> 00:08:11.230	So we just showed the generic boosting algorithm which iteratively fit the small model to residuals from the previous stage and then we add up all these small models with some shrinkage to have the final model.
20.srt	00:08:13.850 --> 00:08:21.689	Okay, so that boosting algorithm was a generic form and we're gonna introduce two different boosting algorithms that are most popular.
20.srt	00:08:22.920 --> 00:08:33.300	For example, AdaBoost uses exponential loss instead of just residual, and then AdaBoost also uses different weighting to the data points.
20.srt	00:08:34.730 --> 00:08:43.320	And it can achieve better performance by weighting more to the data points, data samples that were previously misclassified.
20.srt	00:08:44.980 --> 00:08:55.060	Another popular method is called GradientBoost, and GradientBoost method tries to fit the gradient of residual instead of residual itself.
20.srt	00:08:55.519 --> 00:08:58.170	So we're going to talk about this method in the next videos.
18.srt	00:00:05.150 --> 00:00:08.759	Hey everyone, in this video we're going to talk about how to prune trees.
18.srt	00:00:10.009 --> 00:00:16.079	So last time we talked about some ways to prevent overfitting in decision trees.
18.srt	00:00:16.410 --> 00:00:21.890	Decision trees are very easy to overfit, so to mitigate we talked about all the stopping last time.
18.srt	00:00:22.379 --> 00:00:27.679	So we talked about number of hyper parameters that can be used for stopping growing trees early.
18.srt	00:00:30.960 --> 00:00:32.520	So for example we can set the maximum depth of the tree.
18.srt	00:00:32.859 --> 00:00:36.299	So after that certain depth, the tree stops growing.
18.srt	00:00:37.030 --> 00:00:41.689	And another example was set the minimum sample leaves.
18.srt	00:00:41.960 --> 00:00:49.850	That means we set some threshold such that the number of samples need to be in the node in order to split further.
18.srt	00:00:51.460 --> 00:00:53.760	Another strategy was the information gain.
18.srt	00:00:53.900 --> 00:01:02.070	So we look at the information gain and if the gain is not enough by splitting the node, then we stop splitting there.
18.srt	00:01:03.950 --> 00:01:13.520	So this strategy can be effective for preventing overfitting, but it doesn't guarantee that the performance of the tree will be better.
18.srt	00:01:15.520 --> 00:01:30.510	So the issue is that we can have some good split after the tree stops growing, or maybe we locally look at some node and stop splitting from that node because maybe we saw the information gain wasn't enough.
18.srt	00:01:30.890 --> 00:01:35.819	However, further split can have some huge reduction in impurities, for example.
18.srt	00:01:36.159 --> 00:01:39.750	So we never know what's going to happen after a certain point.
18.srt	00:01:41.090 --> 00:01:58.770	So another idea that we can try is maybe we can let the tree grow fully and then prune back because it's a hind site we can make sure the prune tree is good enough both in performance and overfitting.
18.srt	00:02:01.780 --> 00:02:03.790	Alright so how are we gonna do that?
18.srt	00:02:04.030 --> 00:02:14.220	We're gonna use a algorithm called the minimal cost complexity pruning and this feature is implemented since two versions ago in the SQLon library.
18.srt	00:02:16.810 --> 00:02:18.039	So here is a big tree.
18.srt	00:02:18.039 --> 00:02:22.230	We grow the tree fully and we will call it T0.
18.srt	00:02:22.920 --> 00:02:31.939	And then a certain point, maybe pick this point that this is node T and the impurity can be measured as RT.
18.srt	00:02:45.260 --> 00:02:48.800	So impurity can be you know, gene index or entropy for classification task, but it could be something else like RSS or mean-scaled error if it's a regression.
18.srt	00:02:48.800 --> 00:02:54.520	So RT really means that some error measure of the node before the splitting.
18.srt	00:02:55.900 --> 00:03:08.050	And then we can add some additional penalty alpha t which term is a measure of complexity by splitting further.
18.srt	00:03:08.050 --> 00:03:10.870	So this alpha t is a measure of complexity.
18.srt	00:03:10.870 --> 00:03:20.909	It's proportional to a complexity parameter and also proportional to the number of terminal nodes from that node t. So we define a sub-tree.
18.srt	00:03:22.269 --> 00:03:36.579	everything below this node T, and we count the number of terminal nodes, in this case 3, and the bigger the subtree, that means we penalize more, that means we add more term into our error term.
18.srt	00:03:37.239 --> 00:03:43.379	So effectively the error term is larger when we add this penalization term, or regularization term.
18.srt	00:03:43.549 --> 00:03:46.719	And you can check more details in these documents.
18.srt	00:03:55.000 --> 00:03:57.269	Okay, so we talked about that this alpha is complexity parameter, and this size T is number of leaf nodes or terminal nodes in the sub-tree.
18.srt	00:03:57.269 --> 00:04:07.159	And this is again gray area is a sub-tree from that node T. So let's say this is node T and these are the leaf nodes.
18.srt	00:04:07.159 --> 00:04:17.139	And as you might guess, the impurity at the node T before the split is larger than the impurity of the sub-tree.
18.srt	00:04:17.139 --> 00:04:19.759	Otherwise, it won't split, right?
18.srt	00:04:19.759 --> 00:04:24.689	So this is generally larger than the impurity of the sub-tree.
18.srt	00:04:25.790 --> 00:04:28.500	So how do we calculate the impurity of subtree?
18.srt	00:04:28.529 --> 00:04:33.560	It's just a sum of all the impurities in the leaf node of that subtree.
18.srt	00:04:34.550 --> 00:04:34.860	Alright?
18.srt	00:04:35.889 --> 00:04:41.579	So, so far these were the pure impurities at the node T and the subtree.
18.srt	00:04:41.930 --> 00:04:45.689	The sum of the impurities into the leaf nodes.
18.srt	00:04:46.199 --> 00:04:54.339	Then now let's think about what happens if we add this complexity term or regularization term.
18.srt	00:04:54.339 --> 00:04:57.990	So each case we can add this regularization term.
18.srt	00:04:57.990 --> 00:05:02.659	So for node T, we can say the effective error at the node T.
18.srt	00:05:03.289 --> 00:05:18.420	is its plane impurity plus the complexity term, but remember it was before the split, so our complexity term, the number of terminal node is just one here, so we're gonna just add alpha here.
18.srt	00:05:19.669 --> 00:05:22.240	And let's think about the subtree hole itself.
18.srt	00:05:22.779 --> 00:05:34.889	So subtree, the effective error of the subtree is going to be the impurity of the subtree, which again is a sum of all the impurities at the terminal nodes.
18.srt	00:05:35.699 --> 00:05:49.079	plus the complexity parameter alpha times the complexity of the tree, of that subtree, which is number of leaf nodes, in this case, three.
18.srt	00:05:49.639 --> 00:05:53.689	So that is the effective error of that subtree.
18.srt	00:05:54.399 --> 00:06:04.459	And at certain point, if we pick the alpha carefully here, then we may be able to set these two numbers to be equal.
18.srt	00:06:04.879 --> 00:06:12.490	So error of this node before split and error of the entire subtree below that tree, below that node.
18.srt	00:06:13.889 --> 00:06:21.090	So the alpha that makes this possible is called alpha effective.
18.srt	00:06:21.840 --> 00:06:30.590	So this alpha effective is actually a number that will set the threshold when we can split further.
18.srt	00:06:31.790 --> 00:06:35.699	And if we do the algebra using this formula, then we get this formula.
18.srt	00:06:36.660 --> 00:06:37.500	Okay so great.
18.srt	00:06:37.500 --> 00:06:43.460	So we can define a threshold at the node T that tells whether we should split or not.
18.srt	00:06:44.220 --> 00:06:46.820	Okay, so with that, how do we do the pruning?
18.srt	00:06:47.660 --> 00:06:52.960	So we can calculate all alpha effective for intermediate nodes.
18.srt	00:06:53.140 --> 00:07:05.960	So alpha effective here, here, and every intermediate node except terminal node will have its own alpha effective and their numbers can be different.
18.srt	00:07:05.960 --> 00:07:14.890	And we have that list of that alpha effective for all the intermediate nodes and then we pick the one.
18.srt	00:07:16.470 --> 00:07:19.230	that's smallest and then remove it.
18.srt	00:07:19.870 --> 00:07:24.710	And we can iteratively remove the smallest alpha effective.
18.srt	00:07:25.830 --> 00:07:38.870	So for example, if this node had the smallest alpha effective among this all other intermediate nodes, then we can remove this node as well as its subtree, like that.
18.srt	00:07:39.840 --> 00:07:46.290	And let's say this one was the next, then we get rid of that, get rid of this.
18.srt	00:07:48.500 --> 00:07:52.970	and we repeat until we meet some criteria.
18.srt	00:07:53.230 --> 00:07:54.600	So when do we stop the pruning?
18.srt	00:07:55.320 --> 00:08:08.020	We set some threshold called alpha CCP or CCP alpha such that we stop pruning when all of the alpha effectives are bigger than this number.
18.srt	00:08:08.790 --> 00:08:13.680	That means the link strength is strong enough that we don't need to prune anymore.
18.srt	00:08:14.890 --> 00:08:19.740	And again this threshold value is called a CCP alpha in the SQL library.
18.srt	00:08:20.770 --> 00:08:25.660	So again, this alpha effective is a measure of strength of that link.
18.srt	00:08:26.210 --> 00:08:34.460	If the alpha effective is bigger, that means the split at that node was worth, so we don't prune that link.
18.srt	00:08:34.789 --> 00:08:42.220	If the alpha effective is smaller than certain threshold, that means it was not worth splitting, so we just prune that branch.
24.srt	00:00:06.139 --> 00:00:06.730	Hello everyone.
24.srt	00:00:06.879 --> 00:00:10.349	In this video, we're going to continue to talk about the support factor machine.
24.srt	00:00:12.019 --> 00:00:25.929	Last time, we talked about maximum margin classifier, another name hard margin classifier, which has a hyperplane such that the margins or the distance between the support and the hyperplane will be maximized.
24.srt	00:00:26.550 --> 00:00:30.329	So these points closest to the hyperplane are called support.
24.srt	00:00:31.190 --> 00:00:37.700	And these dashed lines or the planes that are parallel to the hard margin hyperplane are called the margins.
24.srt	00:00:39.420 --> 00:00:42.539	And the goal is to make these margins as big as possible.
24.srt	00:00:43.859 --> 00:00:49.049	Having bigger margin means that we have more safety or confidence in terms of classification.
24.srt	00:00:51.219 --> 00:00:58.760	Last time, we also mentioned that the maximum margin classifier uses internal optimization to find this hyperplane.
24.srt	00:00:59.950 --> 00:01:03.439	Before we go further, let's derive some math formula.
24.srt	00:01:03.679 --> 00:01:07.280	that can be useful for describing this optimization technique.
24.srt	00:01:08.489 --> 00:01:09.939	So here's the hyperplane.
24.srt	00:01:10.299 --> 00:01:19.829	Here is one support point that's above this hyperplane and we'd like to measure this distance, shortest distance between this point and the hyperplane.
24.srt	00:01:34.040 --> 00:01:37.200	To do that, we're going to choose an arbitrary point on the hyperplane and let's say this vector to the support point is called xA and this vector to the point that's on this hyperplane is called xb.
24.srt	00:01:38.910 --> 00:01:42.370	And now this vector will be xa minus xb.
24.srt	00:01:45.230 --> 00:01:50.490	And then we'd like to calculate the distance between this support point to the hyperplane.
24.srt	00:01:52.010 --> 00:01:57.360	To do that, we just draw a line between this projection point and this point b.
24.srt	00:02:05.730 --> 00:02:07.890	All right and this will be 90 degree and now let's say this is the vector that's normal to this hyperplane.
24.srt	00:02:08.060 --> 00:02:23.250	So we will call it n, which is normal vector, and then the angle between these two vectors, let's call it s. So between this s vector and normal vector n will be called zeta.
24.srt	00:02:36.299 --> 00:02:51.269	And this distance is d. So we would like to calculate the d which will be s scalar value times the cosine zeta, which is the same as the S vector, dot product, the unit vector n. And just an example, in the three dimension, the S vector would have three components, S1, S2, S3 for example.
24.srt	00:02:51.699 --> 00:02:57.169	If it was in p-high dimensional space, you would have a p component, S1 to Sp.
24.srt	00:02:57.169 --> 00:03:02.759	Similarly, the unit vector will also have three components in the three dimension.
24.srt	00:03:02.759 --> 00:03:07.479	So W1, W2, W3 for example.
24.srt	00:03:07.479 --> 00:03:13.199	And because it's a unit vector, we require that the length of this unit vector is 1.
24.srt	00:03:13.869 --> 00:03:21.679	so that means this would have to be 1 in three dimension.
24.srt	00:03:23.359 --> 00:03:39.539	So for D, it's going to be S1 W1 plus S2 W2 plus S3 W3 for this three-dimensional example, and we can also rewrite XA1 W1 XA2 W2.
24.srt	00:03:44.419 --> 00:04:00.979	XA 3W 3 minus XB 1 W 1 minus XB 2 W 2 minus XB 3 W 3.
24.srt	00:04:02.959 --> 00:04:10.849	And because this point B was kind of arbitrary, we don't care what that point was, but we do care about this one.
24.srt	00:04:11.199 --> 00:04:15.849	So let's just simplify XA is actually X.
24.srt	00:04:16.219 --> 00:04:30.639	then we can do x1w1 plus x2w2 plus x3w3 and we can call this guy, the rest, to be just some simple constant.
24.srt	00:04:31.179 --> 00:04:31.869	Let's say b.
24.srt	00:04:33.969 --> 00:04:45.509	So this formula is for the distance d and again if this point a was the support, then this distance between the support and the hyperplane becomes the margin.
24.srt	00:04:47.269 --> 00:04:51.999	And now let's think about how to take care of a point that's below the hyperplane.
24.srt	00:04:52.669 --> 00:04:57.989	So when it's below the hyperplane like this, let's say it's called A prime.
24.srt	00:04:58.649 --> 00:05:02.239	As you know, this cosine value will be negative.
24.srt	00:05:02.799 --> 00:05:09.649	So this quantity becomes negative when the support vector is below the hyperplane.
24.srt	00:05:11.149 --> 00:05:15.309	To take care of that case, we're going to assign a variable.
24.srt	00:05:15.529 --> 00:05:20.559	Let's say y for the point A is going to be plus 1 value.
24.srt	00:05:21.039 --> 00:05:22.389	When it's above the hyperplane.
24.srt	00:05:23.729 --> 00:05:26.919	and it's minus 1 when it's below the hyperplane.
24.srt	00:05:28.139 --> 00:05:37.199	So that gives an idea of how to combine this together and we can use this Y and this formula to make a mass expression for the optimization condition.
24.srt	00:05:37.239 --> 00:05:37.729	Something like YI.
24.srt	00:05:38.449 --> 00:05:42.869	So the I means the index for the data point.
24.srt	00:05:53.509 --> 00:05:59.829	So YI times XI, first component, the coefficient for the first component, XI2W2 all the way to xipwp for the p-dimensional hyperplane and plus b for the constant.
24.srt	00:05:59.829 --> 00:06:15.879	And this quantity needs to be greater than equal to the margin m. So this inequality equation sets the condition that the optimization needs to satisfy for all the data points.
24.srt	00:06:15.879 --> 00:06:22.309	Alright, so let's talk about how should that formula that we just derived has to change.
24.srt	00:06:23.439 --> 00:06:25.449	when you have inseparable data.
24.srt	00:06:27.419 --> 00:06:33.489	So when we have inseparable data, what we need to do is that we need to just relax the condition.
24.srt	00:06:33.949 --> 00:06:40.449	Instead of having hard margin, that will require that all the points has to be above and below these margins.
24.srt	00:06:40.739 --> 00:06:43.719	Instead, we accept some errors by softening the margin.
24.srt	00:06:43.869 --> 00:06:51.119	And this is called soft margin classifier or in other words, a support vector classifier.
24.srt	00:06:52.699 --> 00:06:55.249	So let's have a look what does the soft margin mean.
24.srt	00:06:56.079 --> 00:06:58.809	So this is the hard margin that we showed before.
24.srt	00:06:59.199 --> 00:07:05.349	We had the coefficients to each component of the vector x and then a constant.
24.srt	00:07:05.769 --> 00:07:13.229	So sum of this times the y which is plus 1 when it's above the hyperplane and minus 1 below the hyperplane.
24.srt	00:07:13.619 --> 00:07:20.279	So this value should be greater than equal to m. That was hard margin classifier.
24.srt	00:07:21.309 --> 00:07:26.369	When we say we relaxed the condition, we introduce a new variable called slack variable.
24.srt	00:07:26.399 --> 00:07:27.169	So this one.
24.srt	00:07:27.729 --> 00:07:30.469	Which helps to give some wiggle room for this m.
24.srt	00:07:31.489 --> 00:07:35.949	So, in addition to this, we have to satisfy this condition as previously.
24.srt	00:07:36.429 --> 00:07:40.269	And then, this slack variable is always positive value.
24.srt	00:07:41.609 --> 00:07:44.329	And also, we have to satisfy this condition.
24.srt	00:07:44.749 --> 00:07:54.789	So sum of this slack variable need to less than or equal to a value called c. And this c represents the budget for the error.
24.srt	00:07:55.689 --> 00:07:58.949	In other words, if c is large, then we can tolerate more errors.
24.srt	00:07:59.619 --> 00:08:05.069	And also c is a hyperparameter, so the user get to choose how much of error budget we have.
24.srt	00:08:06.969 --> 00:08:09.569	Alright, so let's talk about some definitions here.
24.srt	00:08:10.119 --> 00:08:13.989	So this is a hyperplane, as you know, and these are the margins.
24.srt	00:08:14.599 --> 00:08:22.629	And if you look at carefully, blue texts are on this side and red texts are mostly on this side.
24.srt	00:08:23.369 --> 00:08:28.169	So when the data points are above the margin, this is a safe margin.
24.srt	00:08:31.589 --> 00:08:35.179	Above the safe margin, then this is correctly classified.
24.srt	00:08:35.389 --> 00:08:39.519	So these are correctly classified.
24.srt	00:08:42.539 --> 00:08:50.379	And when the data points are on the margin itself, just saying we can just say it's on the margin.
24.srt	00:08:50.519 --> 00:08:52.179	So this is also on the margin.
24.srt	00:08:52.949 --> 00:08:55.759	How about these data?
24.srt	00:08:56.179 --> 00:09:02.399	So blue points here, red point here, they are in the wrong side of the hyperplane.
24.srt	00:09:03.799 --> 00:09:06.559	So these are wrong side.
24.srt	00:09:07.120 --> 00:09:11.349	of the hyperplane.
24.srt	00:09:11.769 --> 00:09:12.439	What about these?
24.srt	00:09:17.229 --> 00:09:25.809	These are still on the correct side of the hyperplane, but it's the wrong side of the margin.
24.srt	00:09:26.409 --> 00:09:28.240	So, there are two margins.
24.srt	00:09:28.240 --> 00:09:37.149	However, we only care this margin when it comes to blue data points and we'll care about this margin when it comes to red data points.
24.srt	00:09:37.919 --> 00:09:44.669	So blue data points that are just below the margin but above the hyperplane are called wrong side of the margin.
24.srt	00:09:53.709 --> 00:10:04.689	And this one, although it seems like on the margin because it's not sitting on its correct margin or the safe margin to the blue data point, so it's still on the wrong side of the hyperplane.
24.srt	00:10:06.859 --> 00:10:07.669	Time for this.
24.srt	00:10:09.419 --> 00:10:11.169	So, that's some kind of definitions.
24.srt	00:10:11.239 --> 00:10:16.319	And with that, let's see what happens to the slack values for all these different situations.
24.srt	00:10:17.149 --> 00:10:21.119	And remember, this is the condition for the hyperplane.
24.srt	00:10:21.659 --> 00:10:27.409	And all of this slack variable needs to be positive value, either equal or greater than zero.
24.srt	00:10:40.259 --> 00:10:46.159	When the data points are on the correct side of the margin, which means these points, these, the slack variable values for those points are zero, so it doesn't do anything on this equation so it doesn't change and satisfy the hard margin requirement.
24.srt	00:10:47.899 --> 00:11:08.559	And when it's a wrong side of margin, so anything below this margin for the blue points and anything below this but above the hyperplane for red ones, these slack variable will have some value between 0, something greater than 0 and something less than equal to 1.
24.srt	00:11:10.759 --> 00:11:17.649	If it is sitting right on the hyperplane, which is very rare, it's going to have the slack variable equals 1.
24.srt	00:11:19.079 --> 00:11:20.599	What about wrong side of hyperplane?
24.srt	00:11:21.509 --> 00:11:25.399	If it's wrong side of hyperplane, the slack variable will be larger than 1.
24.srt	00:11:25.859 --> 00:11:28.769	So this value becomes negative.
24.srt	00:11:29.599 --> 00:11:31.929	Therefore, we want to avoid that situation.
24.srt	00:11:33.269 --> 00:11:35.769	Alright, so again, the role of the C parameters.
24.srt	00:11:41.029 --> 00:11:43.949	C is the error budget that bounds the total number of errors as well as the severity of the violations.
24.srt	00:11:45.099 --> 00:11:50.279	And as we mentioned before, C is also a hyperparameter, then we need to pick the budget of the error.
24.srt	00:11:52.329 --> 00:11:54.809	So with that in mind, we're going to address three questions.
24.srt	00:11:55.209 --> 00:12:01.449	So first one would be what is the maximum number of supports in the wrong side of the hyperplane when the C is given?
24.srt	00:12:02.469 --> 00:12:10.549	And secondly, we're going to also answer what happens to the margin M when C changes whether increases or decreases.
24.srt	00:12:12.749 --> 00:12:15.409	And what does that mean in terms of bias and variance?
24.srt	00:12:16.879 --> 00:12:24.119	So the first question, what is the maximum number of supports on the wrong side of the hyperplane given the C?
24.srt	00:12:27.509 --> 00:12:41.709	So when you remember this formula, you can think that every slack variable needs to be positive value and this sum of the slack variables need to be smaller than equal to the C parameter.
24.srt	00:12:43.469 --> 00:12:49.299	That means the maximum number of adders can be C if all of the select variables are equal to 1.
24.srt	00:12:51.139 --> 00:12:55.079	Alright, so second question, what happens to the margin when C decreases?
24.srt	00:12:58.649 --> 00:13:02.789	So which one of these will have smallest C?
24.srt	00:13:06.019 --> 00:13:07.649	The answer is this one.
24.srt	00:13:08.139 --> 00:13:12.829	The smaller the C, we have smaller tolerance for the adder.
24.srt	00:13:13.429 --> 00:13:15.539	Therefore, the margins gets tighter.
24.srt	00:13:16.860 --> 00:13:19.139	So what happens to the margin when C decreases?
24.srt	00:13:19.340 --> 00:13:22.209	The answer is the margin becomes narrower.
24.srt	00:13:24.090 --> 00:13:25.120	Alright, the next question.
24.srt	00:13:25.779 --> 00:13:28.379	What happens to the bias and variance when C is small?
24.srt	00:13:28.929 --> 00:13:31.000	So small C means a tighter margin.
24.srt	00:13:31.419 --> 00:13:34.120	That means we have a less tolerance to the error.
24.srt	00:13:35.490 --> 00:13:39.929	And less tolerance to the error means that we will get a more accurate model.
24.srt	00:13:39.959 --> 00:13:41.059	That means less bias.
24.srt	00:13:41.299 --> 00:13:45.039	So bias decreases but we will have instead a higher variance.
24.srt	00:13:49.350 --> 00:14:01.190	So, as a recap, we talked about hard margin classifier which has a hyperplane that separates the support which are this closest point to the hyperplane as much as possible.
24.srt	00:14:01.330 --> 00:14:03.759	There are these overlaps.
24.srt	00:14:04.129 --> 00:14:10.100	So these are called support again and the distance between this hyperplane and these supports are called the margin.
24.srt	00:14:11.580 --> 00:14:18.769	And we derived this formula that expresses the condition that all the points need to be satisfied for the hard margin classifier.
24.srt	00:14:20.490 --> 00:14:26.330	And we also talked about some general cases where the data points are not perfectly separable.
24.srt	00:14:27.100 --> 00:14:39.370	We need to introduce a slack variable that will make this condition a little bit softer, which allows some of the data points can be wrong side of the hyperplane or wrong side of the margin.
24.srt	00:14:40.650 --> 00:14:47.290	And we also talked about C parameter, which is a hyperparameter that we set, which value acts as a budget for the total error.
24.srt	00:14:49.840 --> 00:14:54.700	So far we talked about linearly separable data, that means our hyperplane was not curved.
24.srt	00:14:55.100 --> 00:14:58.000	It was kind of straight, multidimensional.
24.srt	00:14:58.400 --> 00:15:06.170	plane, hyperplane, and we show that it's a plane can be described by this linear formula.
24.srt	00:15:06.750 --> 00:15:13.190	However, in some cases like this, there is no way to separate this data with just one hyperplane.
24.srt	00:15:14.460 --> 00:15:19.320	And for that, we'll need some other ways to separate the data like this.
24.srt	00:15:19.629 --> 00:15:23.280	In that case, we will have to use some more general form of kernel.
24.srt	00:15:23.770 --> 00:15:26.530	So we'll talk about kernel method in the next video.
25.srt	00:00:05.089 --> 00:00:05.780	Hello everyone.
25.srt	00:00:05.809 --> 00:00:09.660	In this video, we're going to talk about support vector machine with the kernel tricks.
25.srt	00:00:11.220 --> 00:00:21.809	So just a brief recap, we talked about hard margin classifier, which goal is to maximize its margin, which is the distance between the hyperplane and the support vectors.
25.srt	00:00:22.410 --> 00:00:31.339	And then, in case we had inseparable data like this, we simply added the slack variable epsilon to all this.
25.srt	00:00:31.679 --> 00:00:38.490	data points and then this epsilon just specify how much they deviates from the margin.
25.srt	00:00:38.870 --> 00:00:45.560	So like this for red points and then this amount for different blue points here.
25.srt	00:00:45.560 --> 00:00:59.260	And these slack variables need to satisfy two conditions such as it has to be no negative value and then we also define the c parameter which gives an idea how much of error budget we have.
25.srt	00:01:02.379 --> 00:01:08.890	Also we mentioned that Sometimes the data can be not possible to use one hyperplane to separate the data.
25.srt	00:01:09.569 --> 00:01:16.329	So in that case, we need to use some special trick called the kernel trick, which will be the subject of this video.
25.srt	00:01:18.209 --> 00:01:21.439	Before we go on what the kernels are, let's think about this.
25.srt	00:01:21.849 --> 00:01:31.650	So in support vector classifier, which is another name for submargin classifier, we mentioned that we have to satisfy all these conditions.
25.srt	00:01:32.939 --> 00:01:36.209	And this part is the formula for the hyperplane.
25.srt	00:01:36.590 --> 00:01:37.759	Let's call it f .
25.srt	00:01:40.539 --> 00:01:48.280	And then this beta zero and beta one and all the way to the beta p are the coefficients for this equation.
25.srt	00:01:48.280 --> 00:01:52.030	And the optimizer will find the values for these coefficients.
25.srt	00:01:52.030 --> 00:02:02.229	Now we can ask ourselves, why do we call SVM as a non-parametric method when we do see these parameters in the equation?
25.srt	00:02:02.969 --> 00:02:03.280	18.
25.srt	00:02:03.750 --> 00:02:06.040	That's very much related to the use of Connors.
25.srt	00:02:06.450 --> 00:02:15.140	You might notice that I use the term SVC, which is a Supervector Classifier, versus SVM, Supervector Machine.
25.srt	00:02:15.140 --> 00:02:28.230	It's not very important, but Supervector Machine generally refers to some generalization of Supervector Classifier, whereas Supervector Classifier usually refers to the Soft Margin Classifier.
25.srt	00:02:29.290 --> 00:02:31.400	In SKLUN, they use a different algorithm.
25.srt	00:02:31.400 --> 00:02:34.270	SVC uses a lip linear.
25.srt	00:02:35.530 --> 00:02:40.390	It's very much similar to the optimization algorithm that we use in logistic regression.
25.srt	00:02:40.930 --> 00:02:52.409	Whereas this SVM uses a libSVM algorithm which is specially made for SVM and this algorithm uses the kernels.
25.srt	00:02:54.469 --> 00:02:56.650	Alright, so let's talk about what the kernels are.
25.srt	00:02:57.319 --> 00:03:04.909	So this is again hard margin classifier and this is soft margin classifier and this is the formula for the hyperplane.
25.srt	00:03:06.500 --> 00:03:11.810	We're going to introduce a different math formula which is equivalent to this formula f .
25.srt	00:03:12.220 --> 00:03:15.290	However, we'll skip the derivation and just show the result.
25.srt	00:03:15.290 --> 00:03:23.750	So using the inner product, it is known that this formula f can be rewritten to this formula.
25.srt	00:03:23.750 --> 00:03:25.560	And this is a dot product.
25.srt	00:03:25.560 --> 00:03:33.460	So if you have xi', then this is dot product between the point i' and point i.
25.srt	00:03:33.460 --> 00:03:38.760	And this dot product represents the linear corner.
25.srt	00:03:39.610 --> 00:03:44.040	Oftentimes, we will call it as K kernel, xi and xi'.
25.srt	00:03:44.140 --> 00:03:47.690	That product again is a linear kernel.
25.srt	00:03:47.850 --> 00:03:50.930	So essentially, this is same as this one.
25.srt	00:03:50.930 --> 00:03:56.780	However, when we implement the algorithm, it will have a different time complexity.
25.srt	00:03:56.780 --> 00:04:09.090	So for example, the SVC that uses a linear library will have time complexity of number of data point times number of features.
25.srt	00:04:09.920 --> 00:04:17.069	And if we use the libSVM and solve for linear data, then it's going to take more time.
25.srt	00:04:17.530 --> 00:04:23.680	It's going to have SVM with linear corner.
25.srt	00:04:24.450 --> 00:04:34.620	It's going to take n squared times p. So by using kernel, it doesn't seem it's useful for the linear data.
25.srt	00:04:34.790 --> 00:04:38.740	However, the kernel method shines when it comes to complex data.
25.srt	00:04:39.360 --> 00:04:40.330	So let's have a look.
25.srt	00:04:42.820 --> 00:04:51.370	When we have this type of data that's not possible to separate by linear hyperplane, what we want to do is this.
25.srt	00:04:51.780 --> 00:04:57.500	So let's say a simple example, we have a data that's not linearly separable.
25.srt	00:04:57.920 --> 00:05:03.850	So in the one-dimensional, the hyperplane will be just a point.
25.srt	00:05:04.750 --> 00:05:09.330	So we need two hyperplanes in order to separate it perfectly.
25.srt	00:05:09.330 --> 00:05:11.130	However, it's not possible.
25.srt	00:05:13.450 --> 00:05:25.660	So the trick is, We can add one dimension here and then now we can separate this perfectly with this one hyperplane So adding one more dimension is a key and it's called the kernel trick.
25.srt	00:05:26.340 --> 00:05:34.770	So again, this data is not separable In 2D using linear hyperplane So what we do is we add the third dimension.
25.srt	00:05:45.940 --> 00:05:50.580	So this is a G by the way, and this is Maybe we can call it X and Y So we're gonna introduce G and maybe X here and Y here And now we can see that this data is separable with the hyperplane like this.
25.srt	00:05:53.470 --> 00:05:58.880	Adding one more dimension means that we want to make a higher order terms in the function.
25.srt	00:05:59.700 --> 00:06:11.640	Okay, so we have a p number of features in the data and then we can add a higher order terms in order to make extra dimensions to separate the data points, which was previously not separable in the linear fashion.
25.srt	00:06:54.910 --> 00:06:59.150	So we can add higher order terms like this, but then, naively, what happens is that Now our optimization need to find all these parameter values for the higher order terms Which might be a lie if you add even more higher order terms And as well as if you have a large number of features It's going to be a problem like we saw in the polynomial regression So instead of adding directly higher order terms we can use a kernel trick instead So let's make a use of this inner product We can create a function kernel function k that has this form this is the dot product and then represent a first order terms.
25.srt	00:06:59.750 --> 00:07:08.120	And by having a constant plus this dot product to the order of d, we can create the polynomial function for the high order terms.
25.srt	00:07:10.960 --> 00:07:14.870	And then we can generalize our function to be a form that has these corners.
25.srt	00:07:17.250 --> 00:07:22.000	So let's have a look when we have this type of data that might involve a nonlinear decision boundary.
25.srt	00:07:22.509 --> 00:07:25.540	We can use polynomial kernel that we just saw.
25.srt	00:07:26.810 --> 00:07:30.560	By having polynomial kernel, we can have this type of decision boundary.
25.srt	00:07:30.819 --> 00:07:35.110	Shows the data result when we had the d equals 2 for polynomial kernels.
25.srt	00:07:35.889 --> 00:07:42.110	Which nicely separates these blue points and the red points by adding another dimension to the data.
25.srt	00:07:44.099 --> 00:07:53.629	There are other types of kernels and another very famous one is called the radial kernel or sometimes called the radial basis functional kernel or RBF for short.
25.srt	00:07:54.079 --> 00:07:57.399	And it takes this form, this kind of Gaussian shape kernel.
25.srt	00:07:59.280 --> 00:08:03.079	defines the RBF corner and the result is like this.
25.srt	00:08:03.079 --> 00:08:05.870	So it's like a round shape.
25.srt	00:08:07.840 --> 00:08:11.970	Basis corner will be able to separate this data into three blobs.
25.srt	00:08:15.080 --> 00:08:16.259	So having corner is great.
25.srt	00:08:16.259 --> 00:08:18.120	You can solve some complex data.
25.srt	00:08:18.400 --> 00:08:22.629	However, we need to think ahead what kind of kernels that we should use.
25.srt	00:08:23.019 --> 00:08:27.430	So when it's a linear separable, you can see that we don't need any fancy kernels.
25.srt	00:08:27.629 --> 00:08:33.919	Just a linear kernel or linear SVM or SVC that does not use a kernel at all will solve perfectly.
25.srt	00:08:35.279 --> 00:08:44.759	Whereas RBF corner is fancy corner so the radial basis corner can also solve the problem depending on how the data look like.
25.srt	00:08:44.949 --> 00:08:49.840	So this data was generated by a blob data and linear is approvable.
25.srt	00:08:50.259 --> 00:08:54.879	So it was both linear SVM and RBF-SVM worked well.
25.srt	00:08:56.679 --> 00:09:06.590	You know different types of data like this, this shape like yin and yang or moon shape in sklon, they look like this type of data usually.
25.srt	00:09:07.289 --> 00:09:09.509	The linear SVM doesn't work very well.
25.srt	00:09:09.819 --> 00:09:12.759	However, the radial basis corner did well on this.
25.srt	00:09:13.120 --> 00:09:17.850	Other corners did not do well for this type of data.
25.srt	00:09:19.529 --> 00:09:23.850	How about this circular donut shape of data?
25.srt	00:09:24.459 --> 00:09:29.579	Linear SVM did not do very well as you can expect.
25.srt	00:09:30.169 --> 00:09:34.759	But radial kernel is perfect for this type of data because the data shape is radial.
25.srt	00:09:38.240 --> 00:09:41.379	So now you can see that The choice of kernel strongly depends on the pattern of the data.
25.srt	00:09:42.479 --> 00:09:53.679	So although the kernel is very convenient for this nonlinear data, it requires the user to think about what the data looks like and guess what the best kernel would be.
19.srt	00:00:05.589 --> 00:00:10.570	Hello everyone, in this video we're gonna talk about ensemble method and especially random forest.
19.srt	00:00:12.449 --> 00:00:13.779	So what is an ensemble?
19.srt	00:00:14.080 --> 00:00:17.440	If you hear ensemble, you might imagine this kind of image.
19.srt	00:00:18.219 --> 00:00:22.149	So individual instrument players can make some sound in the music.
19.srt	00:00:22.679 --> 00:00:29.109	However, the sound characteristic and spectrum can be limited by one instrument alone.
19.srt	00:00:30.820 --> 00:00:39.090	But if you have a collection of these different types of instruments, you can make very rich and flavorable musical sound.
19.srt	00:00:39.369 --> 00:00:40.549	And that's the ensemble.
19.srt	00:00:42.159 --> 00:00:45.420	So this kind of analogy can apply to machine learning model.
19.srt	00:00:46.250 --> 00:00:48.920	So for example, decision tree can be a weak learner.
19.srt	00:00:49.700 --> 00:00:54.230	However, if they are aggregated in certain ways, they can be much better.
19.srt	00:00:57.109 --> 00:01:02.369	So here is an intuition why the collection of machine learning model can be better.
19.srt	00:01:02.869 --> 00:01:07.819	Well, let's say we have some problem to solve in the general public community.
19.srt	00:01:08.179 --> 00:01:21.430	If you sample people that has the same race, same gender, same age group, and same kind of background, it's likely to have only represent those kind of people.
19.srt	00:01:21.590 --> 00:01:37.460	The other hand, if you have a diverse people that has a different gender, different age group, and race, and background, it's likely to have more representative of the different groups and then therefore we are likely to make a better decision.
19.srt	00:01:38.500 --> 00:01:40.219	Okay, so diversity is great.
19.srt	00:01:40.879 --> 00:01:43.179	Then how can we make our models diverse?
19.srt	00:01:44.619 --> 00:01:49.229	One idea might be maybe we can train our models on different subsets of data.
19.srt	00:01:51.219 --> 00:01:55.560	So training model on different random subset of data is called bagging.
19.srt	00:01:56.199 --> 00:02:03.379	You can think about like putting different data set into bag and then make the model trained on this bag of the data.
19.srt	00:02:04.519 --> 00:02:08.579	Well, but actually the name is not because they put the data into bags, but...
19.srt	00:02:09.959 --> 00:02:12.089	The full name is a bootstrap aggregation.
19.srt	00:02:12.899 --> 00:02:15.019	So bootstrap aggregation is like this.
19.srt	00:02:15.189 --> 00:02:20.699	You have different ways to random sample the data.
19.srt	00:02:20.729 --> 00:02:26.889	So first step would be a randomly sample subset of training data with the replacement.
19.srt	00:02:26.889 --> 00:02:29.229	So we can use the replacement.
19.srt	00:02:29.229 --> 00:02:33.349	That means we can sample the same data that we already sampled.
19.srt	00:02:33.659 --> 00:02:40.019	So let's say you can sample this yellow sections out of this whole data.
19.srt	00:02:40.659 --> 00:02:43.120	and you can also have this overlaps like this.
19.srt	00:02:43.659 --> 00:02:45.259	So that's a bootstrap process.
19.srt	00:02:46.519 --> 00:02:50.469	And then we can grow a tree on this selected data.
19.srt	00:02:50.930 --> 00:02:56.519	So tree number 1 and tree number 2, tree number 3, tree number 4, etc.
19.srt	00:02:58.229 --> 00:03:05.650	And in general, we don't let this tree prune because they may become similar to each other.
19.srt	00:03:05.650 --> 00:03:11.729	However, it is also possible in practice that we can grow prune the tree and then ensemble them.
19.srt	00:03:13.319 --> 00:03:20.699	Alright, and then after growing this tree to each different subset of data, we can ensemble them.
19.srt	00:03:21.250 --> 00:03:29.379	So ensemble method in regression, the famous method is just averaging the result and for classification we can use the voting method.
19.srt	00:03:32.139 --> 00:03:36.479	Another bonus by doing bagging is we can use the auto bag error.
19.srt	00:03:37.149 --> 00:03:43.569	So auto bag error is kind of validation error that we can test our fitted tree.
19.srt	00:03:44.150 --> 00:03:46.129	that was trained on these yellow chunks.
19.srt	00:03:46.530 --> 00:03:52.680	Then we can test on this the rest of the data that we didn't select to train on.
19.srt	00:03:53.259 --> 00:03:55.930	So let's talk about random forest.
19.srt	00:03:58.060 --> 00:04:02.250	So random forest has another added idea to the bagging.
19.srt	00:04:03.150 --> 00:04:11.959	So bagging classifier alone can give some performance boost because we can diversify our models by training on the random sample data subsets.
19.srt	00:04:15.849 --> 00:04:19.600	And in random forest on top of that We also have some process of decorrelation.
19.srt	00:04:19.769 --> 00:04:29.550	That means we random sample the features and have the model fitted to this subset of the features instead of whole features.
19.srt	00:04:30.949 --> 00:04:32.759	So why is it called the decorrelation?
19.srt	00:04:47.490 --> 00:04:47.620	Because if we have the same features all the time to grow the trees, even though the data subsets are slightly different, the individual tree might have the same structure of splitting in the same orders of so on.
19.srt	00:04:47.620 --> 00:05:00.069	So if you have a random sampling of features, individual trees grown on the subset of the data and subset of features will be likely to have a different structure from each other.
19.srt	00:05:00.569 --> 00:05:03.360	So that is why it is called a decorrelation.
19.srt	00:05:04.310 --> 00:05:10.830	So having this bagging and decorrelation together, the whole algorithm is called the random forest.
19.srt	00:05:11.720 --> 00:05:18.160	Okay great I get that why the random sampling features might help, then how do I randomly sample these features?
19.srt	00:05:19.569 --> 00:05:22.480	A rule of thumb is the square root method.
19.srt	00:05:23.060 --> 00:05:28.860	So when we have 100 features in the data, we will select 10 features in the data subset.
19.srt	00:05:30.710 --> 00:05:33.829	Here are the results of random forest classifiers.
19.srt	00:05:34.490 --> 00:05:44.699	The green line shows that we had a square root method for selecting features versus the red curve means a random forest using all samples.
19.srt	00:05:44.699 --> 00:05:46.850	So essentially that's the bagging.
19.srt	00:05:47.600 --> 00:05:51.509	So you can see some increased performance when we de-correlate the trees.
19.srt	00:05:54.310 --> 00:05:56.320	or use a smaller number of features.
19.srt	00:05:59.720 --> 00:06:02.520	Here is another result showing the power of angsangbol.
19.srt	00:06:03.330 --> 00:06:07.250	So this green star point is actually a single tree test performance.
19.srt	00:06:08.100 --> 00:06:21.010	And then as you can see, as we increase the number of trees in the angsangbol, it generally goes up and then at a certain point, they kind of behave similar.
19.srt	00:06:21.060 --> 00:06:26.150	So this blue curve, for example, is a random forest test error.
19.srt	00:06:28.230 --> 00:06:30.630	And this red curve is a bagging test error.
19.srt	00:06:32.320 --> 00:06:42.090	So both the random forest and the bagging method, they are ensembleing methods and they increase the performance a lot compared to just a single tree.
19.srt	00:06:42.420 --> 00:06:53.270	However, as you can see, decolonizing trees make it a little better than just bagging, just random sampling the data.
19.srt	00:06:55.030 --> 00:06:56.560	You can also see the out-of-bag test error.
19.srt	00:06:56.560 --> 00:07:00.300	So these are kind of validation error during the training process.
19.srt	00:07:03.910 --> 00:07:06.880	And random forests also have a cool feature.
19.srt	00:07:07.050 --> 00:07:08.460	It has a built-in feature importance.
19.srt	00:07:08.460 --> 00:07:15.440	So in SQL library you can pull out feature importance after fitting the random forest model in the data.
19.srt	00:07:15.440 --> 00:07:24.230	Oftentimes this is useful because you can figure out some feature importance and then use it as a feature selection.
19.srt	00:07:24.230 --> 00:07:35.830	So even if you want to use some different model you can still use random forest to do the feature selection and then you can build some more serious model on top of it.
19.srt	00:07:35.830 --> 00:07:37.770	So that can be a handy tool.
19.srt	00:07:39.950 --> 00:07:51.850	Alright, so far we talked about some basics of random forest, what their definition is, and why they are useful, and what kind of mechanism they work on.
19.srt	00:07:52.290 --> 00:07:58.020	And next video, we're going to talk about another angsangbuk method called boosting.
26.srt	00:00:05.139 --> 00:00:05.919	Hello everyone.
26.srt	00:00:06.000 --> 00:00:10.960	In this video, we're going to compare support vector machines' performance with other models.
26.srt	00:00:12.949 --> 00:00:19.559	Last time, we talked about kernel tricks which are used to treat the nonlinear data such as this one.
26.srt	00:00:20.579 --> 00:00:22.210	This is not linearly separable.
26.srt	00:00:22.510 --> 00:00:26.629	Therefore, SPM with the linear hyperplane wouldn't be able to separate.
26.srt	00:00:27.519 --> 00:00:30.839	So the trick was to add higher order terms.
26.srt	00:00:31.410 --> 00:00:36.950	which is essentially making the data to lie in the higher dimension like in the picture on the right.
26.srt	00:00:37.509 --> 00:00:44.780	So by adding higher dimension, we might be able to separate the data points which wasn't possible in a low dimension.
26.srt	00:00:46.890 --> 00:00:51.140	So to do that, we introduce several corners that treats a higher dimension.
26.srt	00:00:51.210 --> 00:00:54.620	So one of them was called the polynomial corner.
26.srt	00:00:55.120 --> 00:01:03.530	So with this degree d, we can specify how many higher order terms of the features we would like to include.
26.srt	00:01:04.669 --> 00:01:10.390	And as a result, we were able to separate this data, otherwise linearly not separable.
26.srt	00:01:12.259 --> 00:01:18.079	We also talked about radial basis function kernel, which is discussion shape function.
26.srt	00:01:18.539 --> 00:01:23.239	This is good for data that's a radial shape, such as a donut shape that we saw before.
26.srt	00:01:25.789 --> 00:01:28.709	So let's briefly talk about properties of SVMs.
26.srt	00:01:29.899 --> 00:01:31.469	SVM needs a feature scaling.
26.srt	00:01:31.469 --> 00:01:38.819	That means we need to normalize a feature by column so that all the features are more or less in the same range of the values.
26.srt	00:01:40.149 --> 00:01:44.199	And also their time complexity scales linearly to number of features.
26.srt	00:01:45.109 --> 00:01:50.009	That means SVM will treat well when the number of features are many.
26.srt	00:01:52.159 --> 00:01:58.729	However, SVM time complexity goes quadratics to cubic to the number of observations.
26.srt	00:01:58.969 --> 00:02:04.239	So SVM is usually good for small to medium sized data with a large number of features.
26.srt	00:02:05.399 --> 00:02:08.419	SVM also works well on sparse features.
26.srt	00:02:08.629 --> 00:02:14.609	That means even though the feature value has a lot of zeros, SVM will be able to handle gracefully.
26.srt	00:02:16.319 --> 00:02:21.099	Comparing to random forest which is also good for a large number of features.
26.srt	00:02:21.689 --> 00:02:27.589	However, random forest can be very slow if the feature values are all real values and it's dense.
26.srt	00:02:27.619 --> 00:02:33.389	Whereas SVM, it can handle more or less similarly to categorical variables.
26.srt	00:02:35.519 --> 00:02:40.579	Another property for SVM, as you know, the SVM has a C parameter.
26.srt	00:02:41.509 --> 00:02:44.949	And we mentioned the C parameter gives the budget of the error.
26.srt	00:02:45.549 --> 00:02:49.169	So small c means that the model can tolerate small error.
26.srt	00:02:49.709 --> 00:02:58.879	That means a high variance and low bias model, whereas a larger C, the model can tolerate more errors and therefore higher bias, lower variance.
26.srt	00:02:58.949 --> 00:03:07.479	In fact, the optimization under the hood of SVM looks like this.
26.srt	00:03:08.000 --> 00:03:18.179	So we talked about making all of the data points within this margin with some kind of slack variables.
26.srt	00:03:19.009 --> 00:03:22.750	That's equivalent to minimizing this loss function.
26.srt	00:03:24.149 --> 00:03:27.489	So without the mathematical proof, we're going to use it.
26.srt	00:03:27.529 --> 00:03:30.219	And this loss function is called the hinge loss.
26.srt	00:03:33.289 --> 00:03:39.249	And let's call this g. Then this is a positive value for loss.
26.srt	00:03:39.249 --> 00:03:42.399	And then it becomes zero, where this g becomes one.
26.srt	00:03:43.339 --> 00:03:48.159	And then after that, the loss stays at zero.
26.srt	00:03:48.189 --> 00:03:49.759	So this looks like a hinge.
26.srt	00:03:52.959 --> 00:03:54.979	So Therefore, the name is Hinge loss.
26.srt	00:03:56.059 --> 00:04:01.689	And then, but not only the Hinge loss, but it also has regularization term.
26.srt	00:04:02.609 --> 00:04:12.019	And this parameter lambda, regularization parameter, is proportional to C. So if we have a larger C, that means we have a higher regularization parameter.
26.srt	00:04:14.339 --> 00:04:16.539	So let's talk about SK-Lon library usage.
26.srt	00:04:16.539 --> 00:04:20.299	So SK-Lon library has a couple of functions.
26.srt	00:04:20.299 --> 00:04:22.439	So one of them is linear SPC.
26.srt	00:04:58.970 --> 00:05:01.759	support vector classifier and it uses a liblinear algorithm which does not use conor so no conor so this linear SVC function works better when the data is a linear is a problem and as you can see the penalty is L2 already and then loss function uses a slightly different one it's called the scared hinge however everything else is works similar And you can also handle multi-class classification problem.
26.srt	00:05:01.759 --> 00:05:07.220	It just uses a one versus the rest type of strategy.
26.srt	00:05:07.779 --> 00:05:13.490	Because SVC or SVM, they are built for binary class classification.
26.srt	00:05:13.490 --> 00:05:18.870	In order to do the multi-class classification, one needs to have one versus the other.
26.srt	00:05:18.870 --> 00:05:24.730	Alright, this is another classification function called SVC.
26.srt	00:05:24.730 --> 00:05:26.230	And it's using libSVM.
26.srt	00:05:29.969 --> 00:05:35.250	and it uses a kernel, so it can have different kernels.
26.srt	00:05:35.339 --> 00:05:37.870	So by default, it's using radial basis function.
26.srt	00:05:38.169 --> 00:05:42.529	However, we can also change to poly if you are using polynomial.
26.srt	00:05:42.949 --> 00:05:47.099	This degree only applies when we are using polynomial kernel.
26.srt	00:05:50.189 --> 00:05:55.389	Oh, by the way, important thing to mention, sklon has a C parameter.
26.srt	00:05:56.129 --> 00:06:01.149	However, the definition of this C hyperparameter is inverse to the textbook's notation.
26.srt	00:06:01.859 --> 00:06:07.879	In the textbook, C directly means that it's number of violations that we can handle.
26.srt	00:06:07.879 --> 00:06:10.069	However, this is the inverse of that.
26.srt	00:06:10.889 --> 00:06:16.429	So that means if we were to have more regularization, we need to make this C smaller.
26.srt	00:06:18.099 --> 00:06:18.429	All right.
26.srt	00:06:20.519 --> 00:06:25.639	In SKLUN, SVM module can also do the regression, and that function is called SVR.
26.srt	00:06:25.759 --> 00:06:31.399	So you can just simply call SVR function, and everything else is pretty much similar.
26.srt	00:06:31.399 --> 00:06:32.250	To SVC.
26.srt	00:06:34.219 --> 00:06:36.909	Just a quick explanation how the SVR works.
26.srt	00:06:37.289 --> 00:06:39.629	It is the reverse of SVC.
26.srt	00:06:40.049 --> 00:06:41.719	What do I mean by reverse is this.
26.srt	00:06:41.719 --> 00:06:46.759	This is the hyperplane and this is the margin.
26.srt	00:06:46.829 --> 00:06:54.979	Then in SVC, we want the classified points are outside of this margin.
26.srt	00:06:55.019 --> 00:06:59.789	And depending on how many errors we allow, they may be just inside the margin or something like that.
26.srt	00:07:00.549 --> 00:07:04.599	However, in SVR, we...
26.srt	00:07:05.049 --> 00:07:12.539	reverse that condition that we want them to be as close to as possible with this decision boundary.
26.srt	00:07:12.539 --> 00:07:18.799	So we kind of fit this hyperplane or line to the data and do the regression.
26.srt	00:07:19.759 --> 00:07:22.749	So yeah, that's essentially how the SVR works.
26.srt	00:07:24.579 --> 00:07:34.409	All right, so for the rest of the video, we're going to talk about SVM performance in comparison with the other high-performing models such as ensemble method.
26.srt	00:07:35.829 --> 00:07:42.729	So to do that, we prepared five data that are similar or different to each other.
26.srt	00:07:44.019 --> 00:07:48.459	All of these five data has a task of binary class classification.
26.srt	00:07:50.209 --> 00:07:53.689	And they have different number of features and different number of observations.
26.srt	00:07:54.599 --> 00:08:05.509	So first data, as you might have seen before, it has a certain features and little more than 5000 samples, of which 80% of them will be used for training.
26.srt	00:08:07.119 --> 00:08:17.709	And just naive decision tree performance after this training and testing on the testing sample is a little more than 60% accuracy.
26.srt	00:08:19.659 --> 00:08:25.629	And as you can see, some of the columns here, this data has a sparse and most categorical features.
26.srt	00:08:26.349 --> 00:08:30.129	So these are real value features.
26.srt	00:08:30.169 --> 00:08:37.110	But however, most of them in this data are categorical and a lot of them also have zero values.
26.srt	00:08:39.469 --> 00:08:47.149	The second data of our choice has 20 features, similar number of samples, similar number of training samples.
26.srt	00:08:47.859 --> 00:08:55.039	So even though the number of features and number of samples are similar to the first one, its problem is a little bit more easier.
26.srt	00:08:55.039 --> 00:08:59.489	So even the decision tree can perform well, about 90% accuracy.
26.srt	00:09:01.439 --> 00:09:09.099	And as you can see, all these features are very dense, so all of them have some numbers, and they are also real value features.
26.srt	00:09:11.009 --> 00:09:20.349	CERN data has more than 100 features and about 3,000 samples and about 80% of them will be used for training.
26.srt	00:09:20.349 --> 00:09:25.419	And decision tree performance was about 70 something percent.
26.srt	00:09:25.419 --> 00:09:31.229	And as you can see, all of these features are categorical and they are also sparse.
26.srt	00:09:31.359 --> 00:09:39.259	And data 4 has even more features, 300 and more.
26.srt	00:09:39.859 --> 00:09:46.919	And then about a little less than 6,000 samples and 80% of them will be used in training.
26.srt	00:09:46.919 --> 00:09:51.789	And decision tree performance on this data was about 70% or less.
26.srt	00:09:51.789 --> 00:09:58.499	And as you can see, all of these data have real value features and they are very dense.
26.srt	00:09:58.499 --> 00:10:03.529	All right, this is our last data.
26.srt	00:10:10.759 --> 00:10:12.609	It has even more features, about 1800 features, and it has a little less than 4000 samples.
26.srt	00:10:12.609 --> 00:10:16.469	However, we're going to select only very small number for training.
26.srt	00:10:16.469 --> 00:10:23.479	So 375 training samples and let the rest to be testing data.
26.srt	00:10:25.739 --> 00:10:38.449	We chose arbitrarily low training samples just because we wanted to show the performance of different models on the data that has more features than the training examples and also small training data.
26.srt	00:10:39.789 --> 00:10:43.409	The decision tree performance on this data is about 70%.
26.srt	00:10:44.089 --> 00:10:51.589	And this data consists of 93% of most categorical variables, and it has some real value variables as well.
26.srt	00:10:55.479 --> 00:11:02.049	With this long description about our data, here are a summary table for the performance of different models.
26.srt	00:11:03.539 --> 00:11:06.849	So you can see we compare five different models.
26.srt	00:11:07.239 --> 00:11:11.769	This decision tree and logistic regression being simple model as a baseline.
26.srt	00:11:12.209 --> 00:11:17.179	And then we also compare other high-performing models from three ang-sang-bu methods.
26.srt	00:11:18.919 --> 00:11:21.449	random forest and gradient boosting machine.
26.srt	00:11:23.269 --> 00:11:39.879	So as you can see for the data one which has a relatively small number of features and moderate size of data, you can see interestingly the logistic regression was the best model and GBM and SVM also performed reasonably well.
26.srt	00:11:40.379 --> 00:11:46.869	And because it's a relatively simple data, this didn't get to use a lot of trees.
26.srt	00:11:48.689 --> 00:11:54.019	The data number two, if you remember, is a relatively easier data in terms of having higher accuracy.
26.srt	00:11:54.019 --> 00:11:58.169	So even the decision trees and logistic regression have a high performance.
26.srt	00:11:59.489 --> 00:12:06.629	As you can see, fancier models performed a little bit better with the expense of lots of trees for the triangle symbols.
26.srt	00:12:08.979 --> 00:12:14.019	For data number 3 from our baseline, the season tree was a little more than 70%.
26.srt	00:12:14.019 --> 00:12:17.879	And logistic regression also performed reasonably well.
26.srt	00:12:17.879 --> 00:12:26.169	However, as you can see here, a little star mark, that means the sklearn library gave a little warning that the max iteration has reached.
26.srt	00:12:27.029 --> 00:12:36.169	I found it happens usually when we have a lot of features, so the optimization in logistic regression becomes little unstable.
26.srt	00:12:37.189 --> 00:12:41.519	Nevertheless, it gives some number so we can use that.
26.srt	00:12:43.449 --> 00:12:47.439	The Triang Sang Bulls and SBM, they worked pretty well.
26.srt	00:12:47.629 --> 00:12:53.809	And they gave a good result by big margin to the baseline models.
26.srt	00:12:53.809 --> 00:13:01.399	And again, you can see Triang Sang Bulls used hundreds of trees.
26.srt	00:13:02.729 --> 00:13:06.699	And then DataFour, which has even more features.
26.srt	00:13:08.069 --> 00:13:09.409	gave some similar results.
26.srt	00:13:09.919 --> 00:13:14.199	All of these tri-ensemble models and SVM, they worked better.
26.srt	00:13:14.199 --> 00:13:23.619	You can see sometimes SVM and tri-ensemble, their performance are similar or sometimes tri-ensemble are slightly better.
26.srt	00:13:23.619 --> 00:13:27.449	But as you will see later, there are some trade-offs.
26.srt	00:13:27.969 --> 00:13:40.259	For the Data 5 that had a more number of features than the number of samples, the decision tree and logistic regression, those baseline models didn't do very well.
26.srt	00:13:40.259 --> 00:13:43.679	However, the tri-ensemble and the SVM model worked much better.
26.srt	00:13:48.329 --> 00:13:52.709	By the way, all of these accuracy values are from a 5-fold cross-validation.
26.srt	00:13:52.709 --> 00:14:01.279	And also, these numbers in the parentheses are the selected hyperparameters after we do the grid search with the 5-fold cross-validation.
26.srt	00:14:01.279 --> 00:14:13.359	Alright, so performance-wise, in terms of accuracy, we saw that all of the ensemble models and SVM, they are comparable or sometimes ensemble models are better.
26.srt	00:14:13.359 --> 00:14:15.659	But how about the training time?
26.srt	00:14:15.659 --> 00:14:26.849	If they give a similar performance, however, one model gives a much shorter training time than the other, then that model that took less time to train seems a better choice, right?
26.srt	00:14:29.109 --> 00:14:30.109	So let's see here.
26.srt	00:14:30.789 --> 00:14:43.479	So the ensemble methods, random force and gradient boosting machine, they took about 100 milliseconds to a little less than 100 seconds, depending on how many number of trees they used.
26.srt	00:14:43.479 --> 00:14:52.069	And as you can see, even the number of features and the data dimension are similar.
26.srt	00:14:52.259 --> 00:14:54.989	Sometimes the other data takes much longer.
26.srt	00:14:55.889 --> 00:14:56.889	And the reason is this.
26.srt	00:14:59.859 --> 00:15:04.719	So for example, data 1 had Therefore, any tree-based method can be slower when there is a lot of real-value features.
26.srt	00:15:04.719 --> 00:15:14.859	This can be avoided if you use some models that uses a histogram-based split or split randomly instead of going through all these values.
26.srt	00:15:14.859 --> 00:15:18.969	On the other hand, SVM doesn't suffer from that problem.
26.srt	00:15:18.969 --> 00:15:31.339	So as you can see, it takes not only shorter than tree ensemble method usually, it also doesn't care whether the feature values are real-valued or the categorical.
26.srt	00:15:33.619 --> 00:15:38.409	So, in that case, we can see SVM is a little more advantageous.
26.srt	00:15:41.089 --> 00:15:51.539	So, as a conclusion, which models to use, we recommend you inspect the data first and then pick the method that's likely to be better suited for the data.
26.srt	00:16:07.099 --> 00:16:16.389	So, we recommend to use SVM model if it has large number features and small to medium size data, which means a few hundreds to few thousand, and also if the data features are mostly real-valued, it's likely that the SVM performs comparable to random forest and GBM, and it takes much less training time.
26.srt	00:16:16.389 --> 00:16:25.039	A good thing that we should keep in mind is that always try simple model first and see how it goes.
26.srt	00:16:25.039 --> 00:16:34.599	You may have to think about Occam's Razor principle, which tells that if the model performances are similar, the simpler model is always better.
26.srt	00:16:39.349 --> 00:16:47.079	On the other hand, Choice of model can be depending on your goal and also the computation resource and the data size as well.
26.srt	00:16:47.479 --> 00:16:59.559	So, for example, if you are running for a machine learning challenge, you might want to try fancy models at the expense of some training time because a little bit of higher performance would be helpful.
26.srt	00:17:00.339 --> 00:17:10.679	However, if you are dealing with a really big system with really big data, you want to go with a simple model that takes less time to get you an idea how the model and data interacts.
7.srt	00:00:09.900 --> 00:00:19.859	Okay, so the behavior of test header that goes down first and goes up later as we increase the model's flexibility can be explained by bias-variance trade-off.
7.srt	00:00:19.859 --> 00:00:23.699	So what is bias and variance?
7.srt	00:00:23.699 --> 00:00:25.190	Let's have a look at graphical explanation.
7.srt	00:00:30.870 --> 00:00:31.350	So when the bullets are well-centered and well- grouped.
7.srt	00:00:31.710 --> 00:00:33.939	They are called low bias and low variance.
7.srt	00:00:36.240 --> 00:00:44.740	When the bullets are well grouped but far away from the target center, then it has a high bias because it's far away from the center or true value.
7.srt	00:00:45.549 --> 00:00:48.100	But it has a low variance because they are well grouped.
7.srt	00:00:50.630 --> 00:01:01.210	On the other hand, if the bullets are quite spread, but it's still well centered around the target, then we can say it has a low bias and high variance.
7.srt	00:01:03.169 --> 00:01:11.810	And as you can imagine, if bullets are not close to the center but it also has a large spread, then we say it's a high bias and high variance.
7.srt	00:01:12.719 --> 00:01:14.829	So how does that translate to machine learning?
7.srt	00:01:15.259 --> 00:01:23.599	In machine learning, we have data from real life and this data can be very complex and we don't know what the true model is.
7.srt	00:01:25.310 --> 00:01:34.169	By making a model, we introduce some assumption and there is an error that's caused by a simplification by choosing our model.
7.srt	00:01:34.469 --> 00:01:36.119	and this error is called the bias.
7.srt	00:01:38.159 --> 00:01:41.959	On the other hand, variance in machine learning means a variability of the model.
7.srt	00:01:42.279 --> 00:01:44.099	So what is the variability of the model?
7.srt	00:02:05.869 --> 00:02:25.609	Let's say we had some data that look like this and then we were to fit the data with the simple model and maybe we can also fit with the more complex model that tends to have a lower error so lower bias However, if we chose different data set, like this for example, then if we fit the simple model again, they will be very similar.
7.srt	00:02:27.149 --> 00:02:35.239	But if we fit the complex model, now it's going to be a little different from the previous.
7.srt	00:02:39.399 --> 00:02:40.599	So this variability of the model is called a variance of the model.
7.srt	00:02:42.349 --> 00:02:47.560	So if the model is simple, they tend to have low variance.
7.srt	00:02:47.879 --> 00:02:50.859	They don't change much even though we change the training data.
7.srt	00:02:52.769 --> 00:02:59.009	But when we have a more flexibility in the model, they may change quite a bit depending on how we choose the training samples.
7.srt	00:02:59.099 --> 00:03:01.449	So they tend to have high variance.
7.srt	00:03:04.899 --> 00:03:09.239	All right, so simpler model tends to have a high bias and low variance.
7.srt	00:03:09.299 --> 00:03:11.139	So it will correspond to this one.
7.srt	00:03:11.909 --> 00:03:17.759	A more complex or flexible model tend to have a lower bias but has a higher variance.
7.srt	00:03:17.879 --> 00:03:19.469	So it will be this case.
7.srt	00:03:20.459 --> 00:03:26.269	In machine learning, a lot of models are either this case or this case.
7.srt	00:03:26.269 --> 00:03:27.659	There is a trade-off between the two.
7.srt	00:03:27.659 --> 00:03:30.909	That's where the bias-variance trade-off coming from.
7.srt	00:03:30.909 --> 00:03:37.479	Sometimes if the model is not very good, then you may encounter this case.
7.srt	00:03:37.479 --> 00:03:46.199	So some type of model, such as a deep neural network with some other tricks, they may have low bias and low variance.
7.srt	00:03:48.509 --> 00:03:52.489	But most of cases, we have the trade-off between the bias and variance.
7.srt	00:03:52.629 --> 00:03:59.079	So back to our test error, why it goes down and then goes up.
7.srt	00:03:59.079 --> 00:04:00.359	Because of the bias-variance trade-off.
7.srt	00:04:00.679 --> 00:04:05.209	So when we have this is model complexity.
7.srt	00:04:05.209 --> 00:04:11.789	This is test error.
7.srt	00:04:11.789 --> 00:04:16.469	Or error in general.
7.srt	00:04:19.959 --> 00:04:33.529	The bias goes down as our model complexity increases and the model variability goes up as our model complexity goes up.
7.srt	00:04:34.819 --> 00:04:42.789	And when you use a squared error, you can actually derive the general relationship between bias and variance to the test error.
7.srt	00:04:49.919 --> 00:05:07.839	So test error, let me see, can be written as a variance of the model, estimated model, and then the bias of the estimated model also and squared plus some irreducible error, the variance of the residuals.
7.srt	00:05:09.899 --> 00:05:21.649	You can have a look at the supplemental note but this is the result and according to this the test error is a sum of this variance of the model and bias scaled of the model.
7.srt	00:05:22.229 --> 00:05:34.419	So in the end, our test header will have a shape of this because it adds this too and then there is some irreducible header from the residuals.
7.srt	00:05:34.559 --> 00:05:36.969	So that's how the test header shape looks like this.
7.srt	00:05:38.099 --> 00:05:54.069	However, in reality, depending on your model and data, your test header may look, just go down and then flattens and that's very common, whereas your training header goes down and down down.
7.srt	00:05:55.229 --> 00:05:59.899	And sometimes the simple model fits well to the data already.
7.srt	00:05:59.979 --> 00:06:06.219	In that case you may have already good test error for the simple model as well like this.
7.srt	00:06:06.729 --> 00:06:08.159	And then it goes up like this.
7.srt	00:06:10.819 --> 00:06:11.579	Something like this.
7.srt	00:06:12.089 --> 00:06:14.459	And also note that this doesn't have to be squared error.
7.srt	00:06:15.059 --> 00:06:20.539	It is very general behavior no matter which loss function or error function you have.
7.srt	00:06:23.289 --> 00:06:28.489	Alright so in summary we talked about what happens if we add more complexity to our model.
7.srt	00:06:28.919 --> 00:06:30.869	We talked about polynomial regression.
7.srt	00:06:31.060 --> 00:06:39.649	and where we stop adding more terms to the model by monitoring train and tester error, and we also talked about the bias-variance trade-off principle.
6.srt	00:00:05.259 --> 00:00:05.960	Hi everyone.
6.srt	00:00:06.059 --> 00:00:09.169	In this video, we're going to talk about multilinear regression.
6.srt	00:00:09.939 --> 00:00:24.129	So previously, we talked about simple linear regression where we have only one variable, and now we're going to add more variables, whether it's a higher order terms for that single variable or other features into the model.
6.srt	00:00:24.160 --> 00:00:34.579	And then the key idea we're going to discuss is that when the model complexity increases by adding more features, it can fit the data better, but it can also introduce some other problems.
6.srt	00:00:34.579 --> 00:00:37.750	So we'll introduce a concept of bias-variance trade-off.
6.srt	00:00:39.039 --> 00:00:45.640	and then we'll talk about how to select the features that are most contributing to the model.
6.srt	00:00:45.739 --> 00:00:56.640	So last time we talked about single variable linear regression which takes the form of y equals a0 plus a1 x1.
6.srt	00:00:57.299 --> 00:01:03.859	So x1 is a one feature that we care about and a1 is a slope and a0 is a coefficient for intercept.
6.srt	00:01:10.969 --> 00:01:16.060	So the example we had was the You can predict the price of the house sales as a function of size of the house.
6.srt	00:01:16.900 --> 00:01:22.890	Size could be x1 and price is the y that we want to predict.
6.srt	00:01:23.870 --> 00:01:31.099	And now let's say we want to add another feature, say size of the lot.
6.srt	00:01:36.150 --> 00:01:41.349	So when it has a big lot, then maybe it's more expensive than the same.
6.srt	00:01:41.609 --> 00:01:44.390	small house that has a smaller lot.
6.srt	00:01:44.640 --> 00:01:56.640	So we can think about this and we can add the new term, new feature into our model a2x2.
6.srt	00:01:59.010 --> 00:02:03.540	And similarly, we can add more features such as a number of bedrooms and things like that.
6.srt	00:02:03.770 --> 00:02:07.730	Then it becomes more complex model and so on.
6.srt	00:02:09.330 --> 00:02:16.219	So this is also linear regression especially it's called multi linear regression because it has multiple features.
6.srt	00:02:19.260 --> 00:02:25.030	But we can also make some other model that has a higher order terms of the house size.
6.srt	00:02:26.030 --> 00:02:29.670	For example, we can have a square term of the house size.
6.srt	00:02:30.880 --> 00:02:37.510	So in that case, we'll have a1x1 plus a2x1 squared.
6.srt	00:02:38.820 --> 00:02:40.830	And that could be also a good model.
6.srt	00:02:49.830 --> 00:02:59.240	And if we want to add more complexity or higher order term to in this model with the same feature, We could add a third term, the cubic term of the house size like this and we can add more.
6.srt	00:03:00.650 --> 00:03:04.439	So in this case, it's called the polynomial regression.
6.srt	00:03:10.889 --> 00:03:14.180	This is multilinear regression.
6.srt	00:03:15.969 --> 00:03:21.710	We can also engineer some features.
6.srt	00:03:23.010 --> 00:03:31.490	Instead of having square term and cubic term and so on, we are not restricted to have just higher order terms.
6.srt	00:03:31.750 --> 00:03:36.370	But we can create some other variable or features using existing features.
6.srt	00:03:38.439 --> 00:03:53.670	So for example, if we are predicting some probability of getting diabetes based on height of a person and weight of the person.
6.srt	00:03:54.750 --> 00:03:58.000	and some other features that we measured from the lab and so on.
6.srt	00:03:58.730 --> 00:04:20.939	Instead of having this model height plus a2, weight plus and so on, we can construct another variable let's say called x prime and which is BMI which is proportional to weight divided by height squared.
6.srt	00:04:24.770 --> 00:04:47.340	So this BMI is a function of x1 and x2 and this becomes a new feature x' and we can have instead a0 plus a1 x' and the things that we wanted to add like lab test and things like that something like this instead of having a height and weight separate features.
6.srt	00:04:48.490 --> 00:04:58.080	So there are many different possibilities that we can engineer like relevant features depending on your domain knowledge or your intuition on the problem and so on.
6.srt	00:04:58.680 --> 00:05:02.789	So linear model can become really flexible in this case.
6.srt	00:05:03.610 --> 00:05:14.019	So we're going to talk about what happens if we start adding more complexity into model and then there are some things that we need to be careful.
6.srt	00:05:15.250 --> 00:05:16.500	So we'll talk about those.
6.srt	00:05:18.819 --> 00:05:20.990	So let's start by polynomial regression.
6.srt	00:05:21.689 --> 00:05:24.519	This M represents the order of the maximum term.
6.srt	00:05:25.560 --> 00:05:29.839	So M equals 1 represents the simple linear regression AX plus B.
6.srt	00:05:30.739 --> 00:05:38.709	then m equals 2 will be a0 plus a1x plus a2x squared and so on.
6.srt	00:05:39.599 --> 00:05:42.789	So these are the complexity of our model.
6.srt	00:05:44.069 --> 00:05:54.009	So when you look at the simple linear regression, it looks a straight line which is okay but it's still maybe a little too simple for this data.
6.srt	00:05:54.329 --> 00:06:01.310	So let's add another term, square term, and then maybe it fits a little bit better and we can add a cubic term.
6.srt	00:06:02.279 --> 00:06:13.879	And then you can see as you add more high-order terms, the line, the fitted line becomes a little more flexible and have different shapes of the curve.
6.srt	00:06:13.879 --> 00:06:18.389	At some point, the fitting fails actually.
6.srt	00:06:18.389 --> 00:06:24.649	And what happens here is that I wasn't very careful about scaling of the feature x.
6.srt	00:06:24.689 --> 00:06:35.949	So in my simple linear regression model, this was on the order of 1000.
6.srt	00:06:37.560 --> 00:06:51.860	and my y is going to be on the order of million, then this coefficient could be on the order of thousand or less and so on.
6.srt	00:07:08.329 --> 00:07:16.769	And then this square term would be on the order of million and by the time I have the size of the house of six power, this could be 10 to 18, which is a really big number and the coefficient to match this number should be very small.
6.srt	00:07:17.249 --> 00:07:22.039	That means the computer has a hard time to calculate all these coefficients.
6.srt	00:07:22.649 --> 00:07:24.359	Therefore, the fitting may not work very well.
6.srt	00:07:24.359 --> 00:07:36.009	In order to prevent this disaster, one way you can do it is just scale the feature to something on the order of 1 instead of 1000.
6.srt	00:07:41.979 --> 00:07:45.839	So if you just divide by 1000 of your features, then you could have 1 to 6, 7 something like that.
6.srt	00:07:46.599 --> 00:07:51.689	And then here you're gonna have 1 to 6 or 10 to 6.
6.srt	00:07:51.769 --> 00:07:53.099	So it's more manageable.
6.srt	00:07:53.759 --> 00:07:56.589	Therefore you can add more high order terms if you want to.
6.srt	00:07:57.659 --> 00:08:03.019	However you will see shortly that we don't want to add high order terms indefinitely.
6.srt	00:08:04.589 --> 00:08:09.929	So it leads to a question where do we want to stop adding high order terms.
6.srt	00:08:13.719 --> 00:08:19.519	Obviously when you see the model fitness The model fitness will go up and up as you add more model complexity.
6.srt	00:08:20.069 --> 00:08:23.239	So you have some data like this.
6.srt	00:08:25.699 --> 00:08:35.169	And your model could be a little crazy that it has a really high order and can fit everything like this.
6.srt	00:08:36.250 --> 00:08:38.209	This model is not very good.
6.srt	00:08:38.209 --> 00:08:45.399	First, it's not very interpretable, but second, it's more vulnerable to new data points, say this one.
6.srt	00:08:46.379 --> 00:08:53.919	It will have a huge error, or maybe like something like here, you'll have a huge error with this.
6.srt	00:08:54.590 --> 00:09:02.090	However, if you have a simpler model, it will have a smaller error at this new data point and things like that.
6.srt	00:09:03.679 --> 00:09:05.210	That's the motivation.
6.srt	00:09:05.559 --> 00:09:10.970	How do we determine where to stop when we add model complexity?
6.srt	00:09:11.600 --> 00:09:16.860	We want to monitor the error that's introduced when we introduce new data points.
6.srt	00:09:18.769 --> 00:09:24.860	So you remember we talked about how to measure the test data error and training data error.
6.srt	00:09:25.169 --> 00:09:35.429	So we had a data set that we have both feature and label and then we set aside some portion of this data and called it test data.
6.srt	00:09:36.730 --> 00:09:41.649	Another name for test data that's used while we are training is called validation.
6.srt	00:09:44.169 --> 00:09:46.970	So we can call them interchangeably.
6.srt	00:09:47.989 --> 00:09:57.669	in machine learning community validation error is more used term for the data set that's set aside for the purpose of testing while you're training the model.
6.srt	00:09:58.419 --> 00:10:04.589	But anyway, with this we can measure errors for the training and testing.
6.srt	00:10:06.219 --> 00:10:08.429	So let's say we picked MSC.
6.srt	00:10:20.309 --> 00:10:28.429	Then as we mentioned before, we have a trained model and measure the we can have the prediction from the training data and With the training label, we can calculate the mean squared error or any error metric of your choice.
6.srt	00:10:29.019 --> 00:10:31.129	So that becomes the error for the training.
6.srt	00:10:32.579 --> 00:10:36.149	And we can do the similar for the test data.
6.srt	00:10:36.659 --> 00:10:41.719	XTE prediction value and then YTE.
6.srt	00:10:42.389 --> 00:10:45.159	Then we can have the error for the test data.
6.srt	00:10:46.629 --> 00:10:52.819	And this F corresponds to each different model with the different...
6.srt	00:10:53.339 --> 00:10:54.949	high-order terms or different model complexity.
6.srt	00:10:54.949 --> 00:11:01.389	So this is M equals 1 and this is model with M equals 2 etc.
6.srt	00:11:02.129 --> 00:11:18.109	Then when you plot, the exact shape of the curve for training error and test error will be different depending on your number of data and data itself that you randomly sample.
6.srt	00:11:18.109 --> 00:11:21.709	Also, it will depend on your model.
6.srt	00:11:21.739 --> 00:11:24.319	Complexity and so on.
6.srt	00:11:24.999 --> 00:11:29.289	However, in general, you're gonna see this type of error curves.
6.srt	00:11:30.189 --> 00:11:36.069	So for training error, it will go down as you increase your model complexity.
6.srt	00:11:38.409 --> 00:11:47.379	However, the test error will go down in the beginning and then at some point it will start going up again as the model complexity is increased.
6.srt	00:11:48.939 --> 00:11:54.979	And we can find the sweet spot here that the test error is minimized.
6.srt	00:11:55.109 --> 00:11:57.969	So we can pick our best model.
6.srt	00:11:58.549 --> 00:11:59.929	complexity equals 2.
6.srt	00:12:01.279 --> 00:12:16.749	You can also see this model complexity M equals 3 model is also comparably good and in some cases depending on your data draw it can show you actually slightly better results than model complexity equals 2.
6.srt	00:12:17.239 --> 00:12:26.549	However, if they are similar, then you want to still choose the simpler model and this kind of principle is called Occam's razor.
6.srt	00:12:33.829 --> 00:12:38.749	It's essentially telling that if the model performance are similar, For simpler model and complex model, we prefer choosing simpler model.
4.srt	00:00:11.160 --> 00:00:14.089	Alright, so let's talk about how well my model fits.
4.srt	00:00:14.630 --> 00:00:19.519	So we're going to look at the numbers R squared value and adjusted R squared.
4.srt	00:00:20.410 --> 00:00:23.609	These are metrics for how well the model fits.
4.srt	00:00:24.750 --> 00:00:30.800	Adjusted R squared is actually same as R squared except that it also takes number of features in top count.
4.srt	00:00:31.349 --> 00:00:39.479	However, when the number of samples are much larger than number of features in the model, these two numbers are essentially the same.
4.srt	00:00:42.129 --> 00:00:45.560	So let's derive R squared as a measure of model fit.
4.srt	00:00:48.670 --> 00:00:50.619	When do we know that model has a good fit?
4.srt	00:00:51.789 --> 00:01:02.179	From the least squared method that we used to determine our coefficient values, we know that model has a good fit when we have a squared error is minimized.
4.srt	00:01:03.479 --> 00:01:06.299	So again, we can use MSC or RSS.
4.srt	00:01:07.039 --> 00:01:09.289	RSS is a residual thermal squares.
4.srt	00:01:09.319 --> 00:01:13.629	It's nothing but same as MSC without the averaging factor.
4.srt	00:01:15.009 --> 00:01:22.299	So we define this quantity and we know that if this quantity is minimized, we know the model has a good fit.
4.srt	00:01:23.479 --> 00:01:26.049	However, there is a little bit of problem with this metric.
4.srt	00:01:35.019 --> 00:01:35.839	One is that this value can be arbitrarily large depending on our unit of the target variable.
4.srt	00:01:37.709 --> 00:01:42.099	And also if we have a different set of data, this quantity will be different.
4.srt	00:01:42.099 --> 00:01:49.499	So we want to normalize by something similar error measure that has same kind of unit.
4.srt	00:01:51.429 --> 00:01:53.539	So what would be a good way to do that?
4.srt	00:01:54.009 --> 00:02:05.399	We can define a benchmark model, say y equals y mean, and then we can compare how good is my error from my model.
4.srt	00:02:06.089 --> 00:02:09.259	y equals beta 0 plus beta 1 x.
4.srt	00:02:10.689 --> 00:02:32.309	Compared to the error of my benchmark model, which is y equals y min, so we're gonna define another quantity called the TSS, total sum of squares, that actually quantifies the error between my null model and my training data points.
4.srt	00:02:34.149 --> 00:02:38.769	So with that, we can define a dimensionless quantity.
4.srt	00:02:39.269 --> 00:02:41.859	by dividing RSS by TSS.
4.srt	00:02:43.059 --> 00:02:54.999	So this is a quantity essentially telling that what's the ratio of the error from my model to the error from the null model or benchmark model.
4.srt	00:02:56.419 --> 00:03:02.579	So this can be a good quantity that measures how my model fits compared to my null model.
4.srt	00:03:12.299 --> 00:03:13.710	We also want our quantity or R-squared value to be higher when when my model fits better.
4.srt	00:03:14.439 --> 00:03:18.750	And if you see, RSS goes down when the model fits better.
4.srt	00:03:18.750 --> 00:03:20.759	So we're going to flip the sign.
4.srt	00:03:20.979 --> 00:03:24.430	We're going to just subtract this quantity from 1.
4.srt	00:03:25.530 --> 00:03:29.139	Then actually it becomes the definition of R squared.
4.srt	00:03:31.229 --> 00:03:36.329	So let's take a moment and think about what values R squared can take.
4.srt	00:03:39.979 --> 00:03:40.509	All right.
4.srt	00:03:41.039 --> 00:03:43.829	So we're gonna think about two extreme cases.
4.srt	00:03:44.210 --> 00:03:53.819	So one extreme case is that when my RSS is 0, that means my model fits perfectly all of the data points, which will never happen in practice.
4.srt	00:03:54.109 --> 00:04:01.759	But let's think that my model is so good that all the data points are on my model's line.
4.srt	00:04:03.129 --> 00:04:06.879	Then this term goes to 0, and my R-squared value will go 1.
4.srt	00:04:06.909 --> 00:04:08.709	So that's one extreme.
4.srt	00:04:08.709 --> 00:04:13.169	Can R-squared go 1?
4.srt	00:04:13.939 --> 00:04:14.789	larger than 1.
4.srt	00:04:16.699 --> 00:04:21.539	R-squared value cannot be larger than 1 because RSS cannot be negative, right?
4.srt	00:04:22.979 --> 00:04:35.599	The another extreme case is that my model is actually just as good as my null model y equals y mean.
4.srt	00:04:35.599 --> 00:04:41.029	In that case, my RSS value will be same as TSS.
4.srt	00:04:41.029 --> 00:04:44.209	So this goes to 1.
4.srt	00:04:45.379 --> 00:04:47.720	Then my R squared will go to zero.
4.srt	00:04:50.689 --> 00:04:52.460	Can R squared value go negative?
4.srt	00:04:54.470 --> 00:04:55.069	Yes, it can.
4.srt	00:04:55.069 --> 00:05:02.509	In practice, if you use a package to fit your regression line, it will almost never happen.
4.srt	00:05:03.449 --> 00:05:10.710	But in case your model is this bad, like this, the slope is totally wrong.
4.srt	00:05:10.830 --> 00:05:14.139	And then it might have RSS that's larger than TSS.
4.srt	00:05:14.389 --> 00:05:16.519	Then this R squared value can go negative.
4.srt	00:05:18.230 --> 00:05:27.689	For simple linear regression, this may not happen, but as you might see later, in a more complex model, sometimes the model can fit worse than the baseline.
4.srt	00:05:28.629 --> 00:05:31.590	So remember that R-squared can go negative as well.
4.srt	00:05:31.590 --> 00:05:38.870	Alright, so we saw that R-squared value could be a good measure of how my model fits.
4.srt	00:05:38.870 --> 00:05:43.890	However, you have to be careful when you interpret the value from your summary table.
4.srt	00:05:49.500 --> 00:05:55.579	Let's take an example where we might want a model that takes a form of ax and there is no intercept.
4.srt	00:05:56.180 --> 00:05:57.449	Why would we want to do that?
4.srt	00:05:58.550 --> 00:06:00.770	So let's have a look at the intercept value.
4.srt	00:06:01.379 --> 00:06:08.009	It's a negative value and that means my sales price will go negative when my living space is zero.
4.srt	00:06:08.620 --> 00:06:10.019	That doesn't make a lot of sense.
4.srt	00:06:10.400 --> 00:06:19.269	So maybe instead of having this uninterpretable intercept, maybe we want to have a model that has no intercept.
4.srt	00:06:20.730 --> 00:06:23.660	And then, yeah that sounds good.
4.srt	00:06:24.800 --> 00:06:29.250	My sales price of house should be zero when the living space is zero.
4.srt	00:06:30.639 --> 00:06:34.740	So let's take a fit and look at the summary table.
4.srt	00:06:36.740 --> 00:06:44.150	We have a square fit living coefficient which is similar to the previous value which is good.
4.srt	00:06:45.660 --> 00:06:48.800	But then we suddenly see R-squared value has gone up.
4.srt	00:06:48.800 --> 00:06:50.629	What does that mean?
4.srt	00:06:52.310 --> 00:06:58.980	Does it mean our new model y equals ax is better than our old model ax plus b?
4.srt	00:07:01.010 --> 00:07:02.490	Well, not necessarily.
4.srt	00:07:03.140 --> 00:07:07.520	If you look at carefully, you're going to see uncensored next to the r squared.
4.srt	00:07:07.790 --> 00:07:08.530	What does that mean?
4.srt	00:07:22.340 --> 00:07:37.110	It turns out that this r squared value is calculated such that RSS of our new model, this guy, and then divide by TSS of the new null model which is not y equals y mean but now our new null model is y equals 0.
4.srt	00:07:38.379 --> 00:07:48.060	So this goes to here and then the total sum of squares from y equals 0 will be way higher.
4.srt	00:07:49.129 --> 00:07:51.450	Therefore, the R-squared value ...
4.srt	00:07:53.060 --> 00:07:55.120	can be much larger than the previous one.
4.srt	00:07:57.540 --> 00:08:07.680	So if you want to compare apple to apple how my new model is doing in terms of the error, you can just directly calculate RSS for our new model.
4.srt	00:08:08.850 --> 00:08:19.920	Let's say y equals ax and then compare with the previous model RSS y equals ax plus b.
4.srt	00:08:20.310 --> 00:08:23.480	Then you're gonna see this RSS is larger than this one.
4.srt	00:08:24.850 --> 00:08:29.760	But the value that it gives here in the summary table is a little bit deceptive.
5.srt	00:00:10.070 --> 00:00:13.609	Okay, so let's talk about how significant the coefficient values are.
5.srt	00:00:15.509 --> 00:00:18.449	So when do you say the coefficient values are significant?
5.srt	00:00:18.940 --> 00:00:25.429	And conversely, when can we say that the coefficient value is not significant?
5.srt	00:00:26.760 --> 00:00:36.060	So think about this, when the coefficient value is not significant, that means we picked up some kind of noise from the data and assign some value for coefficients.
5.srt	00:00:36.419 --> 00:00:38.899	when in fact the coefficient value is zero.
5.srt	00:00:39.890 --> 00:00:47.669	So when you hear coefficient value is not significant, that means the coefficient value should be actually zero.
5.srt	00:00:50.409 --> 00:00:53.379	So let's look at this coefficient value.
5.srt	00:00:53.429 --> 00:00:58.759	It's minus 4000 something and it's a 280 something.
5.srt	00:00:59.619 --> 00:01:04.719	So that looks like that's very big number, big difference from zero.
5.srt	00:01:04.719 --> 00:01:09.469	So maybe we can, can you just say my coefficient values are all significant.
5.srt	00:01:11.489 --> 00:01:35.530	So the absolute value of the coefficient value is not enough to say whether it's significant or not, because consider we change the unit of this target variable, then we can suddenly have a very small coefficient number, and it's hard to tell then whether this number should be zero or not.
5.srt	00:01:36.419 --> 00:01:39.530	So we need some comparison.
5.srt	00:01:39.829 --> 00:01:43.929	We need some way to compare whether this number is good enough or not.
5.srt	00:01:43.929 --> 00:01:48.729	And usually the standard error is a good way to tell it.
5.srt	00:01:48.729 --> 00:02:05.409	So that means if my coefficient value is maybe here, let's say mu, average of my coefficient value is here and this is zero.
5.srt	00:02:05.879 --> 00:02:10.319	And we want to know how far away is my average coefficient value.
5.srt	00:02:12.580 --> 00:02:15.930	And also we want to know how much of spread I have.
5.srt	00:02:16.860 --> 00:02:28.150	So if the spread is pretty large like this, then maybe this mean value for my coefficient isn't very real.
5.srt	00:02:45.229 --> 00:02:51.500	However, if I have this sharp distribution that essentially says my spread of my values for the coefficients are this small and this far away from zero, then I can say with the confidence that my coefficient value is actually real.
5.srt	00:02:51.840 --> 00:03:02.259	So we're going to talk about this and all these values that shows here are good measure of those confidence or statistical significance.
5.srt	00:03:02.879 --> 00:03:03.740	So let's dive in.
5.srt	00:03:04.759 --> 00:03:11.060	So we mentioned that it is important to know the standard error or the spread of my coefficient value.
5.srt	00:03:16.629 --> 00:03:19.869	And there are different ways to get the standard error or the spread of my coefficient value.
5.srt	00:03:20.089 --> 00:03:33.519	One is using some theory or assumption that the residual is some normal distribution with the zero mean and certain spread or variance.
5.srt	00:03:34.389 --> 00:03:45.949	Another way to do that is we just resample the data multiple times and then fit onto that data and get the coefficient value and we do that experiment multiple times.
5.srt	00:03:46.589 --> 00:03:52.609	then we can get the standard error of my coefficients and all kinds of statistical values from there.
5.srt	00:03:53.579 --> 00:03:56.869	But let's briefly talk about what this model-based method is.
5.srt	00:03:56.869 --> 00:04:12.179	So VIRAR derivation, this is called the covariance matrix, which is variance of beta zero, variance of beta one, and covariance of beta zero and beta one.
5.srt	00:04:12.179 --> 00:04:14.939	So matrix looks like this.
5.srt	00:04:24.739 --> 00:04:28.729	But however, we don't have to remember all this math.
5.srt	00:04:29.119 --> 00:04:36.369	This is given by this formula and this leads to the standard error value for intercept and slope.
5.srt	00:04:36.469 --> 00:04:38.479	It looks like this.
5.srt	00:04:38.479 --> 00:04:40.849	However, we don't have to remember all this formula.
5.srt	00:04:41.309 --> 00:04:54.319	But important thing to remember is that all of this variance or standard error value is proportional to the variance of the residual.
5.srt	00:04:54.989 --> 00:05:06.250	So that means if I have data that has a large spread like this, then it's likely that my coefficient values also have a large spread.
5.srt	00:05:07.370 --> 00:05:17.359	And also you can see that the spread of the coefficients not only depends on this variance of the residuals, but also the variance of the data itself.
5.srt	00:05:19.909 --> 00:05:22.599	This model assumes a homoscedasticity.
5.srt	00:05:24.609 --> 00:05:28.889	That means the spread of the data is kind of homogeneous over the data.
5.srt	00:05:29.870 --> 00:05:35.439	However, if you look at our data, that looks like some kind of cone shape like this.
5.srt	00:05:37.429 --> 00:05:43.439	The residual, the spread of the residual is not homogeneous.
5.srt	00:05:43.949 --> 00:05:49.810	However, we can still assume this model and then derive the quantities that we need.
5.srt	00:05:56.839 --> 00:06:01.659	If you are not convinced by that, the model's assumption, then maybe we can use bootstrapping method.
5.srt	00:06:02.149 --> 00:06:05.550	So bootstrapping method is resampling method.
5.srt	00:06:06.490 --> 00:06:10.639	So let's say we have data point that looks like this originally.
5.srt	00:06:14.589 --> 00:06:22.919	Then we can sample some experiment that samples some of the data point like this.
5.srt	00:06:29.460 --> 00:06:37.650	And then we can have that and then draw another one that samples this data.
5.srt	00:06:44.040 --> 00:06:44.540	And so on.
5.srt	00:06:44.540 --> 00:06:46.550	And we can have multiple copies of this.
5.srt	00:06:46.949 --> 00:06:49.519	We can have many many samples that we want.
5.srt	00:06:49.720 --> 00:06:53.980	We can even sample the same data twice or multiple times.
5.srt	00:06:54.120 --> 00:06:55.460	It doesn't matter.
5.srt	00:06:55.590 --> 00:06:59.550	We can have some sampling with the replacement.
5.srt	00:07:00.780 --> 00:07:12.500	So let's say we have many data like that and then we can fit the coefficient values and this coefficient value will be different from this one slightly but they will be similar.
5.srt	00:07:12.920 --> 00:07:23.910	So we get all these values then we can get the mean value of this as well as the standard deviation or variance of that value.
5.srt	00:07:26.650 --> 00:07:31.240	Alright so that's how we get standard error for the coefficient values.
5.srt	00:07:32.500 --> 00:07:38.560	So let's talk about how we determine whether our coefficient values are statistically significant.
5.srt	00:07:39.139 --> 00:07:41.310	To do that, we're going to do the hypothesis testing.
5.srt	00:07:42.170 --> 00:07:52.439	With the two hypothesis, the null hypothesis say that our coefficient value is 0 and the alternate hypothesis saying that our coefficient value is not 0.
5.srt	00:07:53.560 --> 00:07:57.660	And to test that, we're going to construct a t-score which is given by this.
5.srt	00:07:57.660 --> 00:08:00.000	The t-score is standardized.
5.srt	00:08:03.770 --> 00:08:14.980	our coefficient value, estimate value, by subtracting the mean which is given by our hypothesis and the standard error of our estimated coefficient.
5.srt	00:08:16.290 --> 00:08:29.319	This is similar to G-score in normal distribution and actually when the number of samples is larger than 30, the t-distribution approximate to normal distribution so they are essentially the same most of cases.
5.srt	00:08:35.850 --> 00:08:37.269	We're going to calculate the p-value and We can briefly review.
5.srt	00:08:38.209 --> 00:08:44.049	So let's say we have a standard normal distribution like this.
5.srt	00:08:45.049 --> 00:08:48.350	This is zero mean and has a unit variance.
5.srt	00:08:49.309 --> 00:08:52.059	And then let's say we want to have a 5% of error rate.
5.srt	00:08:53.090 --> 00:09:05.909	So that gives us critical value which defines that this area within these critical values.
5.srt	00:09:06.309 --> 00:09:12.039	plus 1.96 and minus 1.96 for standard normal distribution.
5.srt	00:09:12.039 --> 00:09:27.600	This area is 0.95 and the rest, this region and that region, the combined area would be 0.05.
5.srt	00:09:27.600 --> 00:09:31.909	So that's our error rate and this value is also called alpha.
5.srt	00:09:37.679 --> 00:09:57.779	And in standard normal distribution, this shaded area are symmetric, so each of them is going to be 0.025 0.025 and then we're going to have p-value according to our t-score.
5.srt	00:10:09.769 --> 00:10:13.359	So whenever our t-score lies in the rejection region which is shaded in this red area, or maybe here, then we can reject the null hypothesis.
5.srt	00:10:15.359 --> 00:10:16.449	And what is the p-value here?
5.srt	00:10:16.449 --> 00:10:29.989	p-value is this area under the curve enclosed by this t-score or this area in case the t-score was negative.
5.srt	00:10:40.709 --> 00:10:42.209	In that case, in this particular example, our p-value is smaller than the half of the alpha.
5.srt	00:10:42.889 --> 00:10:51.329	So this green area is smaller than the rejection area and in that case, our t-score lies in the rejection region.
5.srt	00:10:51.329 --> 00:10:54.899	Therefore, we can reject the null hypothesis.
5.srt	00:10:54.899 --> 00:11:01.230	What if our t-score lied in here?
5.srt	00:11:01.230 --> 00:11:08.929	Then again, our p-value will be this big.
5.srt	00:11:15.259 --> 00:11:17.220	So when our p-value is bigger than the half of the alpha, we cannot reject the null hypothesis.
5.srt	00:11:19.649 --> 00:11:25.389	All right, so let's see if we can reject the null hypothesis from our regression result.
5.srt	00:11:27.710 --> 00:11:29.980	So that looks like this.
5.srt	00:11:32.170 --> 00:11:46.340	When we look at the t value, it's minus 10, I'm gonna change my color, it's minus 10 sigma away from the mean and for the intercept and for the slope, it's 145 sigma away from the mean.
5.srt	00:11:51.610 --> 00:11:55.519	So that's pretty significant and as you can expect the p value is fairly small, almost zero for two coefficients.
5.srt	00:11:56.120 --> 00:12:05.399	Therefore, we can safely reject the null hypothesis and we can conclude that our coefficient values are statistically significant.
5.srt	00:12:09.639 --> 00:12:14.419	Similarly, we can also define 95% CI, competency-inheritance for the coefficients.
5.srt	00:12:14.419 --> 00:12:18.730	To calculate that, the formula is given by this.
5.srt	00:12:23.259 --> 00:12:29.309	Mean of the coefficient plus minus two or actually it's 1.96 times the standard error and the standard error is given here.
5.srt	00:12:29.309 --> 00:12:36.149	We can also define 95% confidence interval for the regression line.
5.srt	00:12:36.149 --> 00:12:42.439	That means 95% of time my regression line will lie within this orange shaded region.
5.srt	00:12:42.439 --> 00:12:47.899	And 95% prediction interval, which is for the sample points.
5.srt	00:12:47.899 --> 00:12:54.429	That means 95% of time the sample points will be within this blue shaded region.
5.srt	00:12:56.120 --> 00:13:05.360	This analysis can be handy when you have some outliers and these outliers may be good to remove to have better regression.
5.srt	00:13:07.909 --> 00:13:13.720	Ok, so let's talk about how we measure the error from the test data and the training data and how to compare them.
5.srt	00:13:27.220 --> 00:13:28.790	So we talked about this popular error measure so we're going to use them or one of them and let's say we have original training data that we used to use to fit the model.
5.srt	00:13:29.600 --> 00:13:34.779	Instead of using all of them to fit the model, we're going to set aside some data.
5.srt	00:13:34.879 --> 00:13:37.190	Some portion of data is test data.
5.srt	00:13:40.100 --> 00:13:43.259	And the rest we're going to use it for training.
5.srt	00:13:45.389 --> 00:13:53.159	train set, test set, and each of them have feature and label.
5.srt	00:13:53.460 --> 00:13:58.519	So train set has feature x train and label y train.
5.srt	00:13:59.720 --> 00:14:05.240	and the test set has feature test and label test.
5.srt	00:14:07.920 --> 00:14:11.019	So using the train set, we're gonna fit the model.
5.srt	00:14:11.430 --> 00:14:15.970	So model initially had undefined coefficient values.
5.srt	00:14:17.210 --> 00:14:24.639	So we're gonna do fit and then supply our train data, X train and Y train.
5.srt	00:14:25.639 --> 00:14:28.759	And this fit function will determine the coefficient values.
5.srt	00:14:29.890 --> 00:14:37.080	And now this model internally will have optimal coefficient values.
5.srt	00:14:37.860 --> 00:14:40.230	So with that, we're gonna predict this time.
5.srt	00:14:41.700 --> 00:14:42.860	So dot predict.
5.srt	00:14:44.360 --> 00:14:46.810	And for prediction, we don't need a label.
5.srt	00:14:47.740 --> 00:14:50.620	So we're gonna put train data.
5.srt	00:14:51.220 --> 00:14:53.920	Then it becomes yprediction.
5.srt	00:14:55.480 --> 00:14:58.570	So I'm gonna put hat here, but from the training data.
5.srt	00:14:59.770 --> 00:15:01.050	We can do the similar.
5.srt	00:15:04.220 --> 00:15:16.580	with the already fitted model and dot predict and supply test data instead this time and this will give Y prediction from the test data.
5.srt	00:15:16.690 --> 00:15:20.830	So what do we do with this?
5.srt	00:15:21.000 --> 00:15:22.480	So this value and this value.
5.srt	00:15:23.430 --> 00:15:28.629	We can measure the error between the prediction value and and a label.
5.srt	00:15:32.009 --> 00:15:54.790	So for example, so training MSC or error for the training data is going to be MSC of Y true value for the training label, so which is this one, and the Y prediction value from the training data from this one.
5.srt	00:15:56.330 --> 00:15:59.460	So that's going to be our train error.
5.srt	00:15:59.540 --> 00:16:01.810	So for example this one.
5.srt	00:16:03.050 --> 00:16:06.050	And we can calculate similarly MSA for test data.
5.srt	00:16:07.270 --> 00:16:19.790	So MSA test TE is going to be MSA y test the true value and then the y prediction from the test data.
5.srt	00:16:21.620 --> 00:16:25.140	And this value is this for example.
5.srt	00:16:26.100 --> 00:16:31.040	So it's very common that the test error is a slightly larger than the train error.
5.srt	00:16:32.310 --> 00:16:39.950	Or if the data were pretty homogeneous and your model is doing well then train error and test error could be similar value.
5.srt	00:16:42.030 --> 00:16:49.810	We'll say later that if my model is overparameterized, then it doesn't do very well in the test data.
5.srt	00:16:50.490 --> 00:16:56.320	And it's an important way to figure out whether my model is overparameterized or not.
5.srt	00:16:56.740 --> 00:16:58.050	So we'll talk about that later.
5.srt	00:17:01.120 --> 00:17:04.510	In summary, we talked about how we determine the coefficients.
5.srt	00:17:05.110 --> 00:17:07.960	So we talked about least squares method.
5.srt	00:17:11.119 --> 00:17:14.480	method, which minimizes the residual sum of squares.
5.srt	00:17:15.329 --> 00:17:21.680	So we talked about what the RSS is, what the mean squared error is, and bunch of other error metrics.
5.srt	00:17:22.910 --> 00:17:26.029	And we also talked about the goodness of the model fit.
5.srt	00:17:26.650 --> 00:17:31.549	So we talked about R squared and how R squared is derived.
5.srt	00:17:31.609 --> 00:17:36.140	So we derived it using RSS and TSS.
5.srt	00:17:37.720 --> 00:17:43.940	And we also talked about some things that we need to be careful when we interpret the R squared value.
5.srt	00:17:46.089 --> 00:17:48.680	We also talked about significance of the coefficients.
5.srt	00:17:48.849 --> 00:17:55.009	So we talked about standard error of the coefficients, how they are derived or can be determined.
5.srt	00:17:55.950 --> 00:18:05.230	And then we talked about t-score and the p-value and hypothesis testing to say whether the coefficients values are significant or not.
5.srt	00:18:05.230 --> 00:18:08.730	And then we talked about confidence intervals as well.
5.srt	00:18:08.730 --> 00:18:16.640	And lastly, we talked about how to measure the error for training data and test data and how to compare them.
5.srt	00:18:18.319 --> 00:18:21.160	And that's it for the simple linear regression.
5.srt	00:18:21.519 --> 00:18:30.990	And in the next video, we're going to talk about what happens when we add more model complexity such as higher order terms or other features into the model.
1.srt	00:00:05.610 --> 00:00:07.580	Hi everyone, welcome to the class.
1.srt	00:00:07.940 --> 00:00:11.130	This video will talk about introduction to machine learning.
1.srt	00:00:13.529 --> 00:00:17.879	So before we talk about machine learning, let's talk about a several buzz word here.
1.srt	00:00:17.879 --> 00:00:24.489	You might have heard about data science, which some of you might be taking other courses in data science.
1.srt	00:00:25.100 --> 00:00:30.719	Surely you heard about machine learning because this course is going to be about machine learning.
1.srt	00:00:31.960 --> 00:00:40.000	Many of you might have heard about artificial intelligence, which is another buzzword, along with the machine learning and data science these days.
1.srt	00:00:40.950 --> 00:00:44.340	Maybe you also heard about deep learning, which is another hype word.
1.srt	00:00:45.170 --> 00:00:47.159	So let's briefly talk about what these are.
1.srt	00:00:47.450 --> 00:00:53.379	So for data science, it's a really big interdisciplinary field about data.
1.srt	00:00:53.679 --> 00:01:03.560	So you can think about it's actually anything to do with data, including data pipelining, even data collection and data munging and cleaning and data analysis.
1.srt	00:01:03.949 --> 00:01:16.069	which may include manual data analysis, including some simple checks or exploratory data analysis, or can also include machine learning techniques to analyze the data.
1.srt	00:01:16.859 --> 00:01:22.750	Since the data science has a really big spectrum, it can oftentimes be called soft and hard data science.
1.srt	00:01:23.150 --> 00:01:30.449	Soft data science means dealing with the techniques that doesn't require a lot of software engineering skills or a lot of math skills.
1.srt	00:01:31.120 --> 00:01:40.229	So something like data visualization and reporting, dashboard, those kind of things, as well as simple data analysis, can fall into that category.
1.srt	00:01:40.670 --> 00:01:51.450	Whereas the hard data science, those involve more mathematical and more technical skills, such as analyzing data or building systems using machine learning.
1.srt	00:01:52.840 --> 00:02:03.479	Also, data science can deal with the data that's a small size that can fit into your Excel file or something like that, or it's a big data that sits in the big data warehouse.
1.srt	00:02:05.379 --> 00:02:08.789	In industry, the job description looks like this.
1.srt	00:02:09.569 --> 00:02:12.500	So the data scientists, their job role can be varied.
1.srt	00:02:12.639 --> 00:02:19.000	They can do data collection, cleaning, and managing data or preparing data for whatever the company needs.
1.srt	00:02:19.560 --> 00:02:24.189	or they can build machine learning models and do the testing on those data or build a system.
1.srt	00:02:25.170 --> 00:02:28.090	As well as they can also do the visualization and stuff.
1.srt	00:02:28.590 --> 00:02:35.590	So usually data scientists have diverse backgrounds and they require interdisciplinary knowledge.
1.srt	00:02:37.450 --> 00:02:42.409	Machine learning, we mentioned that machine learning several times during the talk about data science.
1.srt	00:02:42.550 --> 00:02:48.879	So machine learning is part of data science and it is also a subfield of artificial intelligence.
1.srt	00:02:49.840 --> 00:02:54.140	It focuses on learning algorithms and building models and training them on the data.
1.srt	00:02:55.280 --> 00:03:02.489	Machine learning consists of different types of learning, such as supervised learning, unsupervised learning, or reinforcement learning.
1.srt	00:03:03.539 --> 00:03:06.799	Many machine learning models, they are coming from statistical learning.
1.srt	00:03:07.229 --> 00:03:17.589	So machine learning extends the statistical learning by including more complex algorithms, which deal with more complex data and bigger data, and more efficient algorithms.
1.srt	00:03:18.560 --> 00:03:24.889	In industry, machine learning engineers can develop and test machine learning models and design machine learning experiments.
1.srt	00:03:25.250 --> 00:03:26.830	and build machine learning systems.
1.srt	00:03:28.719 --> 00:03:35.400	Artificial intelligence has a long history in the CS, and it is about problem solving with intelligence.
1.srt	00:03:35.870 --> 00:03:45.360	That means an AI agent will make an optimal decision according to its algorithm, whether it has a learning component or not, to maximize the goal as a response to the environment.
1.srt	00:03:46.789 --> 00:03:52.909	You might think that AI field is very practical because you're seeing a lot of applications these days.
1.srt	00:03:53.310 --> 00:03:56.569	However, AI also has a lot of theoretical components in it.
1.srt	00:03:58.460 --> 00:04:09.909	And in industry, AI engineers and experts, they are more or less similar to ML engineers, but in broad set skills, including math and programming skills, as well as machine learning.
1.srt	00:04:10.189 --> 00:04:18.209	And they work on building AI system, building machine learning models, natural language processing, robotics, and computer vision and stuff.
1.srt	00:04:19.550 --> 00:04:26.089	And deep learning focuses on neural network models, so building neural network models and training them on data.
1.srt	00:04:26.589 --> 00:04:35.019	And it also deals with a lot of optimization algorithms and training techniques in order to deal with the complex neural network model training.
1.srt	00:04:36.439 --> 00:04:43.900	It is very suitable for complex data such as images, texts and voice, and graphs, and hybrid types of data.
1.srt	00:04:45.920 --> 00:04:49.770	It is a subfield of artificial intelligence and subfield of machine learning.
1.srt	00:04:50.129 --> 00:04:53.660	And in industry, deep learning engineers work on machine learning problems.
1.srt	00:04:54.040 --> 00:04:59.760	It deals with complex data such as images and texts and things like that, or even high performance computing.
1.srt	00:05:01.410 --> 00:05:03.110	We're going to show some summary diagram.
1.srt	00:05:03.370 --> 00:05:06.990	So there is a data science, which is a big interdisciplinary field.
1.srt	00:05:07.390 --> 00:05:09.910	And there is AI, also very big field.
1.srt	00:05:10.400 --> 00:05:14.550	Data science is about anything to do with data, including data analysis.
1.srt	00:05:15.010 --> 00:05:19.490	Whereas artificial intelligence is about solving problems using intelligent algorithms.
1.srt	00:05:20.230 --> 00:05:26.430	And in intersection, when the AI algorithm is learning from the data, it is called machine learning.
1.srt	00:05:27.620 --> 00:05:33.500	And particularly, if it deals with complex data with neural network architecture, it is called deep learning.
1.srt	00:05:36.290 --> 00:05:41.000	And here is the Google trend on the term on machine learning and software engineering.
1.srt	00:05:41.470 --> 00:05:46.320	Just to compare how the machine learning becomes popular for recent few years.
1.srt	00:05:47.320 --> 00:05:55.210	The term machine learning has been around for a long time, however, it became much more popular during the last five or more years.
1.srt	00:05:56.820 --> 00:06:06.550	This graph also indicates that the job growth in machine learning has grown up really much, about 350% during the past few years.
1.srt	00:06:07.960 --> 00:06:12.370	As you can see, machine learning is a top skill in the jobs that involves AI skills.
1.srt	00:06:13.940 --> 00:06:16.820	Alright, that already sounds like ML is very cool.
1.srt	00:06:17.150 --> 00:06:19.130	Let's talk about what ML can do.
1.srt	00:06:21.170 --> 00:06:23.620	So, machine learning is applied everywhere these days.
1.srt	00:06:23.850 --> 00:06:30.480	So, for example, when you do online shopping, you often see this product recommendation based on your browsing and shopping history.
1.srt	00:06:31.090 --> 00:06:37.380	And those use machine learning algorithms to predict the products that are more likely to be purchased by the customers.
1.srt	00:06:37.940 --> 00:06:41.120	Same goes for movie recommendation and music recommendations.
1.srt	00:06:41.810 --> 00:06:47.060	So sentiment analysis is very popular applications these days.
1.srt	00:06:47.680 --> 00:06:54.550	It is standard by now that data scientists analyze the texts such as news articles and social media articles.
1.srt	00:06:54.920 --> 00:06:57.970	to figure out citizens' sentiment on political events.
1.srt	00:06:58.800 --> 00:07:07.689	Similarly, the product review scales or restaurant review scales can be predicted by machine learning algorithms, which information can be important for businesses.
1.srt	00:07:09.439 --> 00:07:12.490	Machine learning is also used a lot in the financial industry.
1.srt	00:07:12.550 --> 00:07:16.400	So, for example, we can forecast the stock price using machine learning.
1.srt	00:07:17.240 --> 00:07:27.360	Machine learning is also used for algorithmic trading, as well as RoboAdvisor, which gives advice for people on how to allocate their assets.
1.srt	00:07:28.290 --> 00:07:31.300	Also, it can be used for forecasting housing price.
1.srt	00:07:32.840 --> 00:07:36.790	And as you can imagine, machine learning can be also useful in medical industry.
1.srt	00:07:37.920 --> 00:07:46.080	By applying machine learning models in the images or tables, it can help doctors to make a medical diagnosis or medical decisions.
1.srt	00:07:47.450 --> 00:07:51.440	It is also used in many science disciplines, such as the bioscience.
1.srt	00:07:51.690 --> 00:08:02.060	So for example, machine learning techniques can be applied to this graph data to inspect the protein interactions or this type of data for the study of genetics.
1.srt	00:08:04.560 --> 00:08:07.610	Machine learning is also a key component in Internet of Things.
1.srt	00:08:08.160 --> 00:08:16.160	Smart sensors and smart devices produce a lot of data, and also machine learning plays a key role in analyzing those data.
1.srt	00:08:17.610 --> 00:08:19.690	Machine learning is also used in self-driving cars.
1.srt	00:08:19.690 --> 00:08:25.710	Self-driving cars can use machine learning and deep learning to recognize images and make decisions.
1.srt	00:08:25.710 --> 00:08:30.800	Alright, so let's talk about what we will learn in this course.
1.srt	00:08:30.800 --> 00:08:33.850	So here is the data science project lifecycle.
1.srt	00:08:33.850 --> 00:08:39.430	So data should be collected and pipelined into data warehouse.
1.srt	00:08:39.430 --> 00:08:43.900	And there is a data governance that the data warehouse has to implement.
1.srt	00:08:45.940 --> 00:08:50.450	And there will be also data pulling and cleaning and maintaining the data.
1.srt	00:08:52.000 --> 00:08:54.610	And that part is called data engineering mostly.
1.srt	00:08:56.200 --> 00:09:03.650	Data science, on the other hand, focuses on using the data and analyzing those data that were prepared by the data engineering process.
1.srt	00:09:04.780 --> 00:09:15.720	So you can include the selection of those data from the warehouse and then cleaning those data and exploratory data analysis and data pre-processing, which means that we prepare data for the model to consume.
1.srt	00:09:17.140 --> 00:09:22.240	And after that, data scientists will build the models and then do the model training.
1.srt	00:09:22.690 --> 00:09:23.600	And there is a result.
1.srt	00:09:23.640 --> 00:09:26.990	And depending on the result, they have to go back to build models.
1.srt	00:09:27.230 --> 00:09:33.420	Or if the result is so strange, then they will have to collect more data or select more data and do this cycle again.
1.srt	00:09:34.810 --> 00:09:35.080	All right.
1.srt	00:09:35.080 --> 00:09:41.460	So among the steps that we mentioned in the data science project cycle, we're going to talk about a few things.
1.srt	00:09:41.810 --> 00:09:43.470	And we don't cover everything.
1.srt	00:09:43.720 --> 00:09:47.610	So, for example, this course is not about data collection and pipelining.
1.srt	00:09:48.560 --> 00:10:01.830	We'll talk about a little bit of data cleaning and EDA and data pre-processing but the main focus will be how to build a model, how to select models and how to do the training and do the testing and analyze those results.
1.srt	00:10:01.830 --> 00:10:05.100	Okay so let's talk about what is learning.
1.srt	00:10:05.100 --> 00:10:09.930	When these children learn alphabets they can learn to generalize.
1.srt	00:10:19.900 --> 00:10:23.350	So for example they can recognize this letter whether it's a small or large or it has a different font or it has a different color or the image looks angled, or the letter is in the word.
1.srt	00:10:25.160 --> 00:10:30.760	It seems so obvious for people, however, to make a machine to learn this, it is not trivial.
1.srt	00:10:33.070 --> 00:10:35.910	So here is some example of supervised learning.
1.srt	00:10:36.850 --> 00:10:38.560	There are images and the labels.
1.srt	00:10:38.980 --> 00:10:41.160	The labels are the names of these animals.
1.srt	00:10:41.870 --> 00:10:45.320	And a supervised learning model learns to predict the label given data.
1.srt	00:10:47.880 --> 00:10:51.610	Unsupervised learning actually resembles very much how humans learn.
1.srt	00:10:51.890 --> 00:10:55.790	So in baby stage, they don't know about the geometric shapes and colors.
1.srt	00:10:57.029 --> 00:11:09.970	But over time, they learn to recognize these visual properties and also recognize the similarities and dissimilarities between them, even before they learn about the names of these colors and shapes.
1.srt	00:11:09.970 --> 00:11:13.920	So those types of learning without labels are called unsupervised learning.
1.srt	00:11:15.670 --> 00:11:25.460	And unsupervised learning is about learning underlying features and extracting information, organizing patterns in the data, or clustering similar data points.
1.srt	00:11:26.759 --> 00:11:29.610	Another type of learning is reinforcement learning.
1.srt	00:11:29.970 --> 00:11:32.790	which the AI agent learns how to act from experience.
1.srt	00:11:33.960 --> 00:11:36.810	So experience is either reward or punishment.
1.srt	00:11:38.060 --> 00:11:40.379	It is very much similar to the animal training.
1.srt	00:11:40.780 --> 00:11:45.960	So we give treats or punishment to make the animal behave desired way.
1.srt	00:11:47.400 --> 00:11:50.710	So reinforcement learning is used a lot in AI and robotics.
1.srt	00:11:52.160 --> 00:11:58.290	All right, so let's change gears and then talk about some definitions that frequently show up in machine learning.
1.srt	00:12:02.590 --> 00:12:05.130	So data in machine learning can be any forms such as tables, images and text and sounds and graphs.
1.srt	00:12:05.410 --> 00:12:06.630	It can be any format.
1.srt	00:12:06.810 --> 00:12:10.700	However, we're going to talk about mostly the tabulated data format.
1.srt	00:12:11.730 --> 00:12:13.920	So let's take an example of this table.
1.srt	00:12:14.420 --> 00:12:18.190	Let's say the supervised learning task is to predict the house price.
1.srt	00:12:18.610 --> 00:12:22.870	So this one would be our labels.
1.srt	00:12:24.330 --> 00:12:27.310	And these labels also are called targets.
1.srt	00:12:30.080 --> 00:12:34.770	And all these columns, except the labels, they are called the features.
1.srt	00:12:37.270 --> 00:12:47.710	And also they are called predictors, which means that those features and predictors are used in the machine learning models to predict the labels.
1.srt	00:12:49.830 --> 00:13:01.400	And this row of the tables are called observations, or samples, which means that the instances of data.
1.srt	00:13:03.110 --> 00:13:05.670	Here are some few examples of machine learning tasks.
1.srt	00:13:06.290 --> 00:13:12.509	So for example, prediction includes classification regression, and these are in supervised learning.
1.srt	00:13:14.140 --> 00:13:23.680	And clustering, which groups the similar data points together, and anomaly detection and dimensionality reduction, they are in a category of unsupervised learning.
1.srt	00:13:25.180 --> 00:13:34.870	And there are other machine learning tasks, such as data generation and feature selection, which are typically not categorized as supervised learning or unsupervised learning.
1.srt	00:13:34.870 --> 00:13:41.430	However, these type of machine learning tasks can be used to enhance the performance of supervised learning tasks.
1.srt	00:13:43.460 --> 00:13:46.759	Alright, so let's talk about prediction tasks in supervised learning.
1.srt	00:13:47.309 --> 00:13:53.059	Prediction tasks can be either classification or regression, depending on the label's data type.
1.srt	00:13:54.289 --> 00:14:07.350	So, when the label is a categorical variable, which means 0 or 1, or 1, 2, 3, or a, b, c, these are called categorical variables, and in that case, it becomes classification.
1.srt	00:14:08.480 --> 00:14:12.639	If the categories are binary, it is called binary class classification.
1.srt	00:14:13.190 --> 00:14:16.040	And if it's multi-categories, call them multi-class classification.
1.srt	00:14:16.040 --> 00:14:17.320	Binary, multi-class.
1.srt	00:14:18.160 --> 00:14:34.690	On the other hand, if the label variable is a real value variable, so something like 0.1, 0.999, something like that, or 3.4.
1.srt	00:14:35.170 --> 00:14:43.430	So real value variables, to predict those labels given the data, is called a regression problem.
1.srt	00:14:49.280 --> 00:14:54.070	So with that in mind, let's talk about how supervised learning works briefly.
1.srt	00:14:54.610 --> 00:14:55.670	So here's the data.
1.srt	00:14:55.810 --> 00:14:57.800	Data consists of features and target.
1.srt	00:14:58.050 --> 00:15:01.160	So feature usually we call x and target is y.
1.srt	00:15:01.660 --> 00:15:06.530	And then we have a model and this feature is input to the model.
1.srt	00:15:07.280 --> 00:15:11.120	The model might have parameters inside or hyperparameters.
1.srt	00:15:11.540 --> 00:15:13.720	That means some settings the user get to choose.
1.srt	00:15:14.200 --> 00:15:19.379	And then after feeding the features into model, the model will make a prediction.
1.srt	00:15:20.929 --> 00:15:26.669	Initially, the model doesn't predict very well, so there will be some error between the prediction and target variable.
1.srt	00:15:27.490 --> 00:15:36.240	And this error can be used to tweak the model to have a better prediction next iteration, and over this iteration, the model becomes more accurate.
1.srt	00:15:37.190 --> 00:15:39.029	So that's how supervised learning works.
1.srt	00:15:40.399 --> 00:15:43.639	So here is a brief taxonomy of supervised learning models.
1.srt	00:15:44.080 --> 00:15:49.159	So for models that has internal parameters, it is called parametric models.
1.srt	00:15:49.830 --> 00:15:54.649	We are explaining what those are individually, and these are examples of parametric models.
1.srt	00:15:55.210 --> 00:15:57.879	Whereas non-parametric models doesn't have internal parameters.
1.srt	00:15:57.879 --> 00:16:00.550	And these are examples of non-parametric models.
1.srt	00:16:00.550 --> 00:16:07.790	Although we are not going to talk about every single model here, we'll talk about most of these models in this class.
2.srt	00:00:05.429 --> 00:00:06.209	Hi everyone.
2.srt	00:00:06.419 --> 00:00:08.929	In this video, we're going to talk about linear regression.
2.srt	00:00:09.619 --> 00:00:17.170	So we'll begin by the definition of linear regression, and we'll talk about how this model can optimize to get the best estimate value.
2.srt	00:00:17.589 --> 00:00:26.550	And then we're going to talk about important quantities for linear regression, such as a fitness performance metric, things like that.
2.srt	00:00:26.620 --> 00:00:30.570	And we'll talk about how statistically significant these estimate values are.
2.srt	00:00:31.940 --> 00:00:35.500	Alright, so let's begin by reviewing how supervised learning works.
2.srt	00:00:36.079 --> 00:00:40.210	So supervised learning needs training data that feeds to the model.
2.srt	00:00:40.400 --> 00:00:42.170	And this model has internal parameters.
2.srt	00:00:42.320 --> 00:00:47.280	And sometimes some models don't have parameters at all.
2.srt	00:00:47.820 --> 00:00:51.359	Some models have hyperparameters as well that users need to tweak.
2.srt	00:00:51.590 --> 00:00:55.290	But anyway, with that, the model can predict the value.
2.srt	00:01:01.459 --> 00:01:06.920	And if the parameters for the parametric model is not optimized, then this prediction value will be far away from the target.
2.srt	00:01:07.469 --> 00:01:17.030	And our goal is to tweak this parameter by optimization so that the model makes a prediction that's close to the target as much as possible.
2.srt	00:01:19.590 --> 00:01:21.189	So what is a linear regression?
2.srt	00:01:21.239 --> 00:01:24.949	It is one of the simplest kind of supervised learning model.
2.srt	00:01:26.920 --> 00:01:30.569	And it predicts a real value number which is regression.
2.srt	00:01:32.269 --> 00:01:33.739	And then it has the parameters.
2.srt	00:01:33.959 --> 00:01:37.129	inside and these parameters are often called coefficients.
2.srt	00:01:38.590 --> 00:01:40.379	And it does not have a hyperparameters.
2.srt	00:01:40.769 --> 00:01:46.879	That means the user doesn't need to figure out some design parameters in advance or during the training.
2.srt	00:01:50.079 --> 00:01:56.329	And importantly, linear regression model assumes a linear relationship between the features and the target variable.
2.srt	00:01:57.789 --> 00:01:58.969	Well, what does that mean?
2.srt	00:02:04.419 --> 00:02:06.859	It means the feature, let's say we have only one feature for now, has a linear relationship to the target variable.
2.srt	00:02:08.669 --> 00:02:25.990	So, let's say it's a house size and this is a house price and there could be some data like this that tells us that when the house size gets larger than the house price gets larger.
2.srt	00:02:27.219 --> 00:02:37.509	And another example could be maybe we want to predict the salary of a person as a function of their years of experience.
2.srt	00:02:39.030 --> 00:02:42.599	Then we might have some data like that.
2.srt	00:02:45.259 --> 00:02:52.719	That shows that in general, when the years of experience goes up, then the seller goes up.
2.srt	00:02:54.659 --> 00:02:57.689	It doesn't have to be a positive slope all the time.
2.srt	00:02:58.459 --> 00:03:01.500	There could be some other example like this.
2.srt	00:03:02.509 --> 00:03:08.069	Maybe the data looks like this and this is age.
2.srt	00:03:09.030 --> 00:03:22.490	and this is a survival rate from some disease such as cancer, then maybe there is a trend that looks like this.
2.srt	00:03:22.979 --> 00:03:25.849	As the age goes up, maybe survival rates goes down.
2.srt	00:03:26.849 --> 00:03:33.199	So these examples show some kind of linear relationship of the feature to the target variable.
2.srt	00:03:35.109 --> 00:03:40.780	When we have a multiple features, linear model also have some linear combination.
2.srt	00:03:41.449 --> 00:03:41.939	shape.
2.srt	00:03:42.099 --> 00:03:43.829	So what that means is this.
2.srt	00:03:43.969 --> 00:04:01.549	So if I have a feature 1 all the way to feature p and they are linearly combined to each other so 1x1 there is a coefficient a1 and then I add up with another coefficient times x2 plus etc.
2.srt	00:04:03.249 --> 00:04:12.449	Coefficient for feature p and then I can also add some free parameter a0 for the intercept.
2.srt	00:04:13.289 --> 00:04:17.039	this becomes my linear model.
2.srt	00:04:20.089 --> 00:04:22.079	And this is called linear combination.
2.srt	00:04:22.079 --> 00:04:34.989	So this type of model, whether we have many variables or one variable, that shows some linear relationship of the variable to the target and this type of model is called the linear regression.
2.srt	00:04:38.839 --> 00:04:40.349	Let's take an example.
2.srt	00:04:40.449 --> 00:04:42.909	This data is coming from Kaggle website.
2.srt	00:04:43.139 --> 00:04:46.870	Kaggle is a repository for machine learning data.
2.srt	00:04:47.389 --> 00:04:53.829	So, if you want to build a machine learning model and train to the data, this is a place to go.
2.srt	00:04:54.060 --> 00:04:57.500	And this website also hosts the ML competition.
2.srt	00:04:57.699 --> 00:05:07.730	That means a lot of competitors, they build their models that fits the data and then they will compare their model performance on this platform.
2.srt	00:05:08.339 --> 00:05:10.480	And this is super fun, so you should try.
2.srt	00:05:10.899 --> 00:05:21.420	Anyway, this data comes from there and this data is about predicting the house sales price in Washington state when there are a bunch of features that describes the house.
2.srt	00:05:22.230 --> 00:05:27.389	So, price is our target variable Y and all these other columns are features.
2.srt	00:05:29.029 --> 00:05:43.170	And because we want to build a simple regression model like this, we want to find out which feature could be a good predictor to predict the house sales price.
2.srt	00:05:44.350 --> 00:05:51.459	If you have a domain knowledge, you can think about what feature will be useful to predict the house price.
2.srt	00:05:52.250 --> 00:05:55.850	You can think about maybe number of bedrooms are important.
2.srt	00:05:56.580 --> 00:06:00.530	The more number of bedrooms, then maybe it's more expensive.
2.srt	00:06:01.270 --> 00:06:03.790	Or you can think about the size of the house matters.
2.srt	00:06:04.720 --> 00:06:07.540	Or you can think about the location of the house matters most.
2.srt	00:06:07.670 --> 00:06:08.439	Things like that.
2.srt	00:06:22.910 --> 00:06:24.699	However, to quantify and have some evidence that which features is most important or likely to important to predict the price, we can Have a look at the correlation matrix.
2.srt	00:06:25.689 --> 00:06:30.110	So, correlation matrix gives correlation values between the features.
2.srt	00:06:30.720 --> 00:06:35.360	So, diagonal elements shows the correlation to the cell.
2.srt	00:06:36.090 --> 00:06:38.350	It has the value of 1 all the time.
2.srt	00:06:38.740 --> 00:06:43.150	However, the other off-diagonal terms, they show the correlation between different features.
2.srt	00:06:43.150 --> 00:06:51.259	Because it's too many, 21 features, I'm going to select the first few and then look at it.
2.srt	00:06:51.259 --> 00:06:53.660	And as you can see...
2.srt	00:06:54.490 --> 00:07:07.770	from the first row, which is correlation values for all other features to the price, you can figure out the square foot living, which is the house size, is most correlated to the price.
2.srt	00:07:08.379 --> 00:07:14.530	And there are other features such as the grade of the house that's comparably good to predict the price.
2.srt	00:07:25.570 --> 00:07:41.660	You should be careful when you select multiple features based on correlation metrics because The order of correlation, that means a high correlation or absolute value of a correlation to lower ones, these orders are not directly related to how important the features are.
2.srt	00:07:42.020 --> 00:07:51.380	So, for example, this feature may have the same or comparable value, correlation value to the price with the skirt foot living.
2.srt	00:07:51.710 --> 00:07:55.840	However, skirt foot living and grades are highly correlated.
2.srt	00:07:56.330 --> 00:08:06.120	So, when I add this feature to my model on top of a square foot living, that doesn't add so much value because this is pretty similar to this one.
2.srt	00:08:06.400 --> 00:08:21.440	So, in that case, some other variables such as floors or something like that or maybe view would add better value to predict the price than this one that has a high correlation to the price.
2.srt	00:08:21.730 --> 00:08:23.460	So, you have to be a little bit careful.
2.srt	00:08:23.690 --> 00:08:28.550	We're gonna go through a method that actually helps to select the features.
2.srt	00:08:28.910 --> 00:08:30.110	right order.
2.srt	00:08:32.170 --> 00:08:38.110	But to select just one feature, correlation matrix gives a good information.
2.srt	00:08:40.379 --> 00:08:41.980	So, let's begin by that.
2.srt	00:08:43.029 --> 00:08:45.639	So, let's talk about univariate linear regression.
2.srt	00:08:45.730 --> 00:08:58.570	Univariate means the variable is only one and also for that same reason, univariate linear regression is called a simple linear regression and often takes this form.
2.srt	00:08:59.190 --> 00:09:06.490	that we have a coefficient beta 0 and beta 1, which represent the intercept and slope.
2.srt	00:09:06.910 --> 00:09:15.710	And then it has a residuals that measures the difference between the target value and the prediction value by our model.
2.srt	00:09:16.440 --> 00:09:23.430	So this residual is important to measure the error and this is for each data point.
2.srt	00:09:23.930 --> 00:09:29.180	So for example, if we have some data that looks like this.
2.srt	00:09:31.880 --> 00:09:48.780	and maybe this is my regression line, then this is going to be my intercept and the slope, the one, and each discrepancy of the data points to the regression line is called residuals.
2.srt	00:09:49.590 --> 00:10:01.520	And our goal is to minimize overall residuals of my model and make my model to produce a predictor value that's as close as possible to the target variable.
2.srt	00:10:04.610 --> 00:10:09.860	This can be done using a single line using stat model OLS package.
2.srt	00:10:10.570 --> 00:10:15.100	There are other packages such as sklons linear model.
2.srt	00:10:15.490 --> 00:10:21.980	However, this is widely used and it is useful because it generates some summary table like this.
2.srt	00:10:23.730 --> 00:10:29.340	So this summary table has a lot of information including the most interesting part.
2.srt	00:10:29.370 --> 00:10:31.980	What are the my coefficient values?
2.srt	00:10:35.189 --> 00:10:42.519	So with this coefficient value, I can determine what my slope and my intercept is for my simple linear regression model.
2.srt	00:10:44.159 --> 00:10:53.299	And beside of coefficient values, we can ask some other questions that are important to linear regression.
2.srt	00:10:55.059 --> 00:11:02.429	We'll begin by how do we determine the coefficients, in other words, how does the model training works under the hood of this.
2.srt	00:11:03.569 --> 00:11:04.099	package.
2.srt	00:11:04.829 --> 00:11:09.269	And we'll also discuss how well my model fits.
2.srt	00:11:10.370 --> 00:11:15.009	And from the summary table values, what gives an idea of how my model fits.
2.srt	00:11:17.069 --> 00:11:21.829	And then we'll also talk about how statistically significant my coefficients are.
2.srt	00:11:24.389 --> 00:11:27.789	That means how robust our estimation for the coefficient is.
2.srt	00:11:30.049 --> 00:11:34.120	And we're going to also talk about how well my model predicts on unseen data.
2.srt	00:11:35.250 --> 00:11:38.590	That means how well does it generalize, which is very important in machine learning.
3.srt	00:00:10.509 --> 00:00:12.640	Alright, so how do we find the coefficients?
3.srt	00:00:12.990 --> 00:00:17.760	So again, this beta zero, beta one are coefficients, and this is my model.
3.srt	00:00:18.079 --> 00:00:22.620	And the difference between the target value and the predicted value is called residual.
3.srt	00:00:23.359 --> 00:00:26.190	So here's a plot that plots the residuals.
3.srt	00:00:26.780 --> 00:00:32.049	So this is a residual that has positive value, and these are the residuals that have negative value.
3.srt	00:00:32.840 --> 00:00:39.460	So when we say how good my model is, that means how small is the error overall.
3.srt	00:00:40.090 --> 00:00:45.980	So we need to find an error measure that accounts all these residuals from all the points.
3.srt	00:00:46.520 --> 00:00:47.410	So how do we do that?
3.srt	00:00:48.439 --> 00:00:50.200	One way to do it is just sum them up.
3.srt	00:00:51.730 --> 00:00:55.909	However, it's going to be zero all the times if the regression line was fit.
3.srt	00:00:56.820 --> 00:00:58.850	So this is not very useful.
3.srt	00:01:02.899 --> 00:01:06.979	So we're going to define another error measure that measures the distance instead of just summing all of these residuals.
3.srt	00:01:08.019 --> 00:01:15.719	So we're gonna have absolute value and then sum them up to n samples.
3.srt	00:01:16.450 --> 00:01:29.340	And in case we have many many samples, this quantity can be very big, so we want to divide by n. Then it becomes mean absolute error, which is a one good way to measure error.
3.srt	00:01:34.109 --> 00:01:37.540	Another way we can do it is we can maybe square each residuals and then sum them up.
3.srt	00:01:38.530 --> 00:01:43.430	And also we can divide by n and this gives mean squared error.
3.srt	00:01:44.439 --> 00:01:47.819	So these two are very popular error measure in regression tasks.
3.srt	00:01:50.710 --> 00:01:53.900	There are some other error metrics that can be also useful.
3.srt	00:01:53.930 --> 00:02:02.569	So we talked about MAE, but MAE can be arbitrarily large depending on how large y's are.
3.srt	00:02:02.569 --> 00:02:06.840	Therefore, we can define percent absolute error instead.
3.srt	00:02:07.469 --> 00:02:12.020	So percent absolute error is a mean of absolute value of this.
3.srt	00:02:12.610 --> 00:02:24.090	This is a target variable value and this is a prediction value and that's divided by target variable value again and takes the absolute and sum them up and takes an average.
3.srt	00:02:26.560 --> 00:02:27.889	So that can be handy.
3.srt	00:02:28.629 --> 00:02:30.439	That can be useful metric.
3.srt	00:02:31.050 --> 00:02:38.069	We talked about mean squared error but mean squared error unit is different from Y unit.
3.srt	00:03:09.569 --> 00:03:23.819	So in case we want to compare in the same unit, we can take a square root Then it becomes root mean square error, which is also good metric in regression Alright, so let's talk about how the optimization in linear regression work there could be various method but this method called least squared method is most popular and almost all Python package that solves a linear regression uses this method so So as a reminder, the model takes the features and it has internal parameters and linear regression does not have hyper parameters and it makes a prediction and we want to find out the values of the parameters that will make this prediction accurate as possible.
3.srt	00:03:24.370 --> 00:03:26.079	And this is done by optimization.
3.srt	00:03:26.770 --> 00:03:35.500	And for this squared method that linear regression mostly use, takes the feature and target value and find a solution for the parameters.
3.srt	00:03:37.210 --> 00:03:42.069	And the name suggests least squares because it uses a squared error.
3.srt	00:03:42.710 --> 00:03:48.400	So we're going to use MSA and let's have a look what the error surface of MSA look like.
3.srt	00:03:49.229 --> 00:04:03.900	So in MSA, the error in the coefficient space where this axis is one of the coefficients and the other axis is the other coefficient, and this axis represents the error, the size of the error.
3.srt	00:04:05.120 --> 00:04:08.689	Then this takes a kind of bowl shape like this.
3.srt	00:04:09.049 --> 00:04:12.789	So if you look at from the top, the contour will look like an ellipsoid.
3.srt	00:04:13.500 --> 00:04:22.069	So, for example, partial derivative of MSE with respect to coefficient A and set it to 0.
3.srt	00:04:23.040 --> 00:04:31.750	And similarly, we can take a partial derivative of MSE with respect to B and set it to 0.
3.srt	00:04:32.209 --> 00:04:33.000	Why we do that?
3.srt	00:04:33.620 --> 00:04:40.769	If you think about the parabola shape, at the bottom, the slope or the gradient becomes 0.
3.srt	00:04:40.769 --> 00:04:43.120	So, we'll use that fact.
3.srt	00:04:43.899 --> 00:04:53.259	And if we do the algebra, we're gonna get the solution without derivation, but you can look at the supplemental note that has all the derivation.
3.srt	00:04:54.120 --> 00:05:04.560	Important thing to remember is that the slope is proportional to the covariance of the variable x and y and then inversely proportional to the variance of x.
3.srt	00:05:06.899 --> 00:05:10.990	And similarly, intercept has this relation.
3.srt	00:05:13.990 --> 00:05:24.180	And if you look at carefully, This suggests that actually the regression line passes through the center, which is mean of x and comma mean of y.
3.srt	00:05:27.060 --> 00:05:31.819	So regression line is centered around these mean values for the x and y.
3.srt	00:05:34.779 --> 00:05:38.120	What happens when we change the scale of the variables?
3.srt	00:05:44.539 --> 00:05:46.509	So for example, we have a big value for living space square foot and really big value for a sales price.
3.srt	00:05:46.819 --> 00:05:48.949	So we want to change the unit for example.
3.srt	00:05:49.229 --> 00:05:57.990	Makes it million dollar as a unit and use a small number and maybe we can divide by thousand for the square-fit living.
3.srt	00:05:57.990 --> 00:06:13.120	So in that case, if we change this by one over thousand of original value and this is 10 to the minus 6 of original value, what happens to my value?
3.srt	00:06:14.610 --> 00:06:16.259	for beta 1 and beta 0.
3.srt	00:06:18.269 --> 00:06:19.709	You can think about it for a while.
3.srt	00:06:21.529 --> 00:06:22.250	Alright, we're back.
3.srt	00:06:23.610 --> 00:06:42.040	So, we're gonna call it r and we're gonna call this as s. And as you know, covariance is calculated by this formula, x minus x mean times y minus y mean, an expectation of this value.
3.srt	00:06:45.670 --> 00:06:47.110	And then similarly, the expectation of x-xmean squared.
3.srt	00:06:47.519 --> 00:06:59.729	So this part is scaled by r and this part is scaled by s and this part scaled by r squared.
3.srt	00:06:59.729 --> 00:07:07.990	So as a result, we're gonna get s divided by r times original value of beta 1.
3.srt	00:07:07.990 --> 00:07:17.069	So if we plug these numbers, we're gonna get 10 to the minus 3 times original value for beta 1.
3.srt	00:07:17.999 --> 00:07:30.189	So my slope gets thousand times smaller if I make my X variable thousand times smaller and make my Y variable million times smaller.
3.srt	00:07:32.489 --> 00:07:34.929	What happens to beta0, my intercept?
3.srt	00:07:36.249 --> 00:07:39.639	So this is going to be S times original value of EY.
3.srt	00:07:39.639 --> 00:07:44.019	And this we already calculated.
3.srt	00:07:44.049 --> 00:07:48.199	It's going to be S over R times the original value of EY.
3.srt	00:07:48.539 --> 00:07:58.149	which I'm gonna just say square and then this quantity becomes r times the ex.
3.srt	00:07:59.909 --> 00:08:05.610	So this cancels out and then we're gonna get s times original value beta zero.
3.srt	00:08:06.709 --> 00:08:09.649	So my inner set doesn't change when I scale the x.
3.srt	00:08:10.029 --> 00:08:15.919	However, it's going to change when I scale the y and it only depends on the scaling of the y.
3.srt	00:08:23.149 --> 00:08:25.759	So let's talk about how we generalize the least squares method to multivariate case.
3.srt	00:08:26.009 --> 00:08:37.720	So when we have p number of features, this is a feature matrix, and we add a column that has ones so that it can take care of the intercept term.
3.srt	00:08:38.279 --> 00:08:42.889	So together with this, this torus matrix is called design matrix.
3.srt	00:08:54.700 --> 00:08:58.639	And this index 1 to n is for the sample index, and this 0 to p is for the feature index including the intercept.
3.srt	00:09:01.040 --> 00:09:07.220	So MSE in matrix form is going to look like this.
3.srt	00:09:07.560 --> 00:09:13.889	Y-Xβ and these are all matrices and then two norm of the matrices.
3.srt	00:09:14.070 --> 00:09:19.200	That is actually the Y-Xβ transpose and Y-Xβ.
3.srt	00:09:30.159 --> 00:09:38.120	So let me take a derivative with respect to beta, then we're going to get this equation and if we further simplify it will look like this.
3.srt	00:09:40.309 --> 00:09:42.029	And this is called a normal equation.
3.srt	00:09:46.220 --> 00:09:50.539	And then solving this equation for beta, it gives a solution like this.
3.srt	00:10:02.090 --> 00:10:05.189	So, it involves the inverse of this matrix inside and sometimes it can be a problem if the rank of this matrix xt and x are not equal to n. And when does it happen?
3.srt	00:10:05.389 --> 00:10:12.389	It happens when there are two or more variables or the features are linearly correlated.
3.srt	00:10:13.350 --> 00:10:33.059	So for example, if my x1 values were 1, 2, 3, and some of the other feature, let's say x5 was linearly dependent on x1, so for example, two times of this, something like that, then these two features are redundant.
3.srt	00:10:33.360 --> 00:10:37.070	Therefore, these metrics becomes non-invertible.
3.srt	00:10:37.070 --> 00:10:42.370	And then there is a problem when we try to get the solution beta.
3.srt	00:10:42.420 --> 00:10:45.300	It actually doesn't mean that we don't have solution.
3.srt	00:10:45.300 --> 00:10:49.870	It means that we have a solution that are not unique.
3.srt	00:10:49.870 --> 00:10:52.930	So we're going to have a hard time to determine unique solution.
3.srt	00:10:52.930 --> 00:11:04.840	But anyway, almost all Python packages that solves the ordinary least squares or less has some mechanism to find the inverse metrics of this.
3.srt	00:11:05.640 --> 00:11:13.230	called pseudo inverse and sometimes this is called Moore-Penrose inverse.
3.srt	00:11:13.310 --> 00:11:21.310	So, with this, we don't have to worry about non-invertible matrices.
