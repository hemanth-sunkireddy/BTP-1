WEBVTT In this video, we're going to talk about feature selection method and things to consider when we select features.
Last time we tried to fit the model that has all the features inside and found that some of the coefficients were not significant.
After removing those, we tried again and found that the coefficients are significant, but still we had some linearly dependent features in it.
It's a lot of manual process to figure out which features to select.
Instead of doing that, we're going to introduce some methods that automatically select the features.
The first method is called the forward selection, which add the feature one-by-one by looking at the one that maximize the R^2 value.
The add feature that maximize the R^2 value, and that's the forward selection.
Another method is called the backward selection, which starts from the full model, by full model I mean there are all the features inside the model already, and remove the one feature that has maximum p-value.
Remove x_j that has maximum p-value.
We repeat this process until we reach the tolerance of the p-value or sub-criteria.
Another good method is called the mixed selection, which combines the forward selection and backward selection to mean that we add some feature that maximize the R^2 first, and then fit the new model and inspect the results and see if there are coefficients that are insignificant.
If there are insignificant coefficients, just remove the features.
Then we add another feature again and then inspect the result and remove all the features that have large p-values and so on.
Let's compare the result.
So forward selection gives this result that they add squarefoot_living first and then latitude, then view, and grade, and so on.
It doesn't have a stock criteria so we're going to just fit all of them to the last feature.
This is resulted from the backward selection.
We start from the full model and then it removes one by one.
First, it removes the floors and then squarefoot_lot second, and sales_month removed, and so on, all the way to the top.
You'll build towards the last one that was removed.
But as you can see, the feature importance are the orders are very different from the forward selection because the forward selection cares about the R^2 value, whereas a backward selection cares about the p-value.
As you can imagine, the mixed selection resembles the forward selection result, however, it stops at some point.
Because it also look at the maximum R^2, the orders are pretty similar to forward selection.
But then at some point, adding another feature will lead always have a p-value that's larger than certain criteria, so it stops there.
Actually, practically, mixed section is a good way to use as a feature selection.
Here is a result of the correlation matrix after we select the features from mixed selection.
Good news is that now we don't have the linearly dependent feature, such as a square foot above.
These are gone, but still we see large correlation values between the features.
Let's talk about correlated features.
Why do they occur?
High correlation among features may occur from different regions.
One of them would be redundant information.
When the features are linearly dependent on each other, the information is redundant and they may have a high correlation.
When there is underlying effect such as confounding or causality, the features may be highly correlated.
For example, ice cream sales and the sharks attack, they have nothing to do with each other, however, they can be caused by hot weather.
Hot weather here is called the confounding.
Then because of this or confounding ice cream sales and then sharks attack, they will have a high correlation in the data.
An example of causality is heart disease can lead to heart attack and diabetes.
Doesn't cause heart attack directly, but it can cause a heart disease, some type of heart disease, and then cause a heart attack.
When we look at the diabetes and heart attack, they may look highly correlated.
In some cases, just the variables are correlated in nature.
for example, number of bedrooms and the size of house, they don't cause each other, they don't have confounding.
However, they are correlated, so they may have high correlation.
We mentioned that it is problematic when there are highly correlated features and why is that?
When the predictors are highly correlated, the coefficient estimate becomes very inaccurate, and also the interpretation of the coefficient as a variable contribution to the response becomes inaccurate.
When there is a high correlation between features more than 0.7, we consider it's problematic.
This is specially called the collinearity when two features are very similar to each other, like we saw previously in the pair plot that the distribution of the data was pretty skinny between Feature 1 and Feature 2, for example, then they are very collinear.
Well, it's not always possible to detect the collinearity using correlation matrix.
Because if there are multiple variables that are involved in the collinearity, they may look okay in the correlation matrix.
However, they could be still co-linear.
This special case is called the multicollinearity.
Beside the correlation matrix, variance inflation factor is a better way to detect this multicollinearity.
VIF is defined by this formula.
VIF of our coefficient Beta i is 1 over 1 minus R squared value of this fitting.
And this fitting is actually not hitting the target variable y, but fitting that variable x_i using all other variables.
Using other variables that are not this one and we are fitting the model and we're going to get the R-squared value and we can get the VIF value.
If the VIF value is larger than 5 in general, or sometimes 10, it means that there is strong multicollinearity.
Let's have a look which variables had multicollinearity in the original model that we had all the features in it.
Clearly sqft_living, sqft_above, sqft_basement, they are linearly dependent each other.
They show strong multicollinearity.
Then after we have mixed a selection, some of the redundant features are gone, for example, this one is gone, and let's inspect, sqft_living has still high VIF value, but it's much better than previous one because of one of the dependent feature was gone.
I think that was the only one that was bad here.
But when we see the correlation matrix, there are still some highly correlated features, for example, this one.
Let's remove these highly correlated features after the mixed selection.
When we remove those variables with a very high correlation to the square foot living, we end up with a much lower VIF value for square for living because the collinearity is gone.
Here are some things to consider when you select features.
We talked about model fitness for selection gives a maximum model fitness by adding one features at a time.
Also, we talked about removing variables with the insignificant coefficients, so backward selection was good at this.
If we combine this, we can have mixed selection.
We also talked about some problems that may occur when you have a multicollinearity.
We haven't talked about this, so let's have a look.
Here is a graph that shows the performance of each model.
This is including intercepts.
This is just the intercept and this model is interceptors one feature and so on, all the way to 14th feature plus the intercept.
It shows here.
This star represent the model performance after we removing the variables with a high correlation.
When we remove the highly correlated features, they may have a better estimation of the coefficient value and then better interpretation.
However, it can have some less performance.
Another thing that we can think about is that, do we need all these 14 features?
It seems that the model complexity six or seven gives efficient result.
This is still a less performance than having 14 features.
However, it's good enough.
We can consider that as well.
By looking at the VIF of these six-feature model, it has a pretty good VIF as well.
Here is all the result from the models that we considered so far.
Again, this number of interesting include the intercept.
It's actually 19-feature model plus one intercept.
Mixed selection gives 14 number of features selected, and so on.
Then if we look at just the feature coefficient for square foot living, the coefficient values are all different and all of them are statistically significant.
However, if you can see the coefficient values, they are very different.
In particular, this model removed all the features that are highly correlated to the square feet living.
Therefore, the coefficient value for square foot living is more accurate and more interpretable.
This means that when we increase the square foot living by one, then the house price goes up by 313 dollars.
On the other hand, the coefficient value for square foot living is lower in other models.
That's because the other models still had other variables that are highly correlated to the square foot living.
Therefore, the coefficient values are not accurate, and all these are correlated features.
They share the contribution to the house price.
Lastly, let's talk about what to do when there are interactions.
What is the interactions?
Interactions can happen when this coefficient is not constant, but is a function of some other variable, say x_3.
In that case, what we want to do is that we're going to have interaction term, so x_1*x_3, and then assign another coefficient.
Let's say Beta_13, and then add to the model.
Not only this, we can also do all the combinations such as adding x_1*x_2, adding x_2*x_3 and all combinations, and we can also have higher-order terms, something like that.
In that case, we have infinite menu of features and we don't want to do that, but however, we can just choose the order.
Maybe the maximum order is just one feature times another, and then we can add them up.
Then we have a problem of how to select all these many combination features.
Again, we're going to apply the same method that we talked about before.
Mixed selection method is a good way to do that.
One thing that is a little different from the previous case is that when we have this interaction term, then we must include also the Beta_1 x_1 + Beta_3 x_3.
Sometimes you might see these coefficient values may not be significant.
However, we should still include these terms in order to have this term.
With that difference, having interaction terms in the model is the same as having multiple features in the model.
WEBVTT Hi everyone in this video, we're going to talk about multi linear regression.
So last time we talked about multi linear regression with the high order terms of a single variable.
And this time we're going to talk about multi linear regression model when there are multiple variables.
So we're going to see how to interpret these coefficients and then how to inspect whether these coefficients are significant.
And we're going to talk about how to select the features and what to consider when we select the features.
And we'll talk about high correlated features and multi collinearity.
What are they, how does it affect the model?
And what can we do when we select features?
And we're going to talk about what other things to consider when we select features.
And lastly, we'll talk about what to do when there are interactions between the features.
So as you know, the multi linear regression model can be formulated by this, so all the variables are linearly combined to represent the model to predict the target variable, Y.
And the coefficient, each coefficient is an average effect of that variable to Y, target variable when we consider all other variables are independent and fixed.
This assumption may not be true in general.
So variables or the predictors might be correlated in real world scenario.
And also, we also assume these coefficients are constant.
However, that might not be true in general.
So if there is an interaction between two variables or more, this constant or coefficient may not be true constant, but it is a function of some other variables.
So we'll talk about what to do when it happens.
Let's take an example that we saw previously.
It's a house price prediction.
So house price is a Y, target variable, and all other variables are the features.
We're going to inspect the types of variables and see if they are suitable for linear regression model.
So, what are the types of variables?
There can be real value number and there could be categorical variable.
And categorical variable can have ordinal and non-ordinal categorical variable.
What is the ordinal categorical variable?
These are categories that have meaning in their order.
So something like age group or grade a, b, c or one, two, three, four.
Those can be ordinal categorical variable because they have a meaning in their order.
The examples of non ordinal categorical variables are male, female, race, ethnicity and so on.
Some classes, they don't have any meaning in their order.
So we can permute the orders of categories and they don't have any effect.
So non ordinal categories are difficult to use in linear regression model.
Because in linear regression model, a variable value times the coefficient present how much of the value in target variable is contributed by that variable.
Therefore, if the variable can be permuted arbitrarily, it's not easy to use in the linear regression.
However, there are ways to use this non ordinal categorical of variables in the linear regression.
For example, we can code the male female into 0 or 1 or 1 or 0 or sometimes it's- 1 to 1.
So if you choose one of these, it will work.
And then how about race?
Let's say race had only three categories, Asian Black and Caucasian.
So this variable has a three categorical values.
So we can convert this into individual three binary categorical variable.
So is the person Asian or not?
Is the person black or not?
Is the person Caucasian or not?
However, we don't need all three of them because if the two are known, the other one can be known as well.
So they are dependent.
So, if we just get rid of one of them and use only two into the model, then it works better.
So in general, if the non ordinal categorical variable had N categories, we could convert them into N- 1 binary categorical variables.
But you have to also consider whether you want to include the N- 1 new features into your model.
So what if you had a really large N, do you want to add large N- 1 features into your model?
Probably not.
That is an example of this zip code.
So zip code had 70 something categories and I didn't want to add 70 something new binary variables into my model because my model then becomes too big.
So hopefully, the other variables such as latitude and longitude can capture some information about the location of the house.
So I'm going to just get rid of zip code and then use other variables.
Before we build the model, let's have a qualitative inspection.
So this case, we're going to see the correlation between the price and all other variables and see which variables might be useful to predict the price.
So, sqft_living could be useful, grade could be useful.
And some other variables such as sqft_above or sqft_living15 could be useful.
By the way, this sqft_living15 is a square foot living of similar 15 houses.
So therefore, this must have some redundant information as sqft_living.
And also the sqft_living is a square foot above plus the square foot basement.
So they are linearly dependent.
So they must have redundant information.
And this is redundant information shows the high correlation between the features.
So these two variables have really high correlation because they are linearly dependent.
And this sqft_living and sqft_living15 because they are similar in the definition, they are also highly correlated.
So we identified a few variables that are correlated to each other and some variables are linearly dependent to each other.
This can be also visually inspected in the para plot.
So para plot is distribution plot between two features.
So in the diagonal element, it shows the distribution of itself, the feature itself and the off diagonal elements shows that the distribution between one feature to the another.
So for example, sqft_above and sqft_living had a really high correlation.
And you can see very skinny distribution of the data.
Actually, this is a very good indication of a collinearity, which we will talk about later.
But essentially that happens because these two features are dependent to each other or nearly dependent to each other.
Okay, so we've qualitatively inspected variables and their correlations and we found that some features may have some redundant information and they may cause some problems.
So with that in mind, let's see what happens if we throw all the features into the model and fit to the data.
So here are the result summary table and we can see that model fit with the R squared is almost a 0.7, which is good value.
And previously, we didn't explain what this F-statistic value is and what the P-value for the F-static is.
And this actually shows whether there is at least one significant variable in the model.
So the null hypothesis for F-test would be all the coefficient values are zero.
So the F value is defined by this formula.
So TSS- RSS divided by a number of features and divided by RSS times n- p- 1.
So this is a formula for F test.
And then the bigger this number we are more sure about there is at least one significant variable in the model.
So in our case, the statistic value is big and the P-value for that is almost a zero, that means it's smaller than a certain threshold of error rate.
Therefore, we can conclude that our model has at least one significant variable.
So let's have a look at the P-values for individual variables and we can see immediately that some variables have insignificant coefficient values.
So, sqft_lot, floors have a really high P-value as well as sales_month have high P-value.
So we can reject these features because their coefficient values are essentially zero.
All right., so after removing the features that has a high P-values, we get this result.
So our, [INAUDIBLE] value is similar, F-statics value is still large and let's inspect the t score and the P-values of each individual coefficients.
And all of them looks statistically significant and that's good.
However, this is not the complete story because we still see some features such as these three are linearly dependent each other are still exist in the model and they still have a high correlation value.
So we're going to talk about some better ways to automatically add or remove the features in the next video.
WEBVTT Hi everyone. We're going to talk about introduction to logistic regression.
Periphery view of machine learning problems.
In machine learning, we have supervised learning, we do labels, and unsupervised learning, which doesn't have label and reinforcement learning with feedback signals.
We're going to focus on supervised learning.
Largely, it has two tasks regression and classification.
In classification, binary class and multi class classification, we're going to talk about.
Previously we talked about linear regression can do the regression task, and logistic regression, we're going to talk about in this video.
Although its name says regression, it's actually for classification, especially to useful for binary class classification.
There are some ways to do the multiclass classification with the logistic regression method, but it's going to require some engineering to do that.
Other models that we're going to talk about it later, and some of them will not talk about it in this course, they can do different things.
For example, support vector machine can do both regression and classification.
Similar to logistic regression it's usually good for binary class rather than multi-class.
But it can't work on multi class.
If we engineered the label and some algorithms inside of the model correctly, and then decision trees can do everything.
You can do regression, and binary class, multi-class without any problem.
Also, it's nice that it can take categorical variable very efficiently.
Neural Network, same thing, can do everything, and many other models that we may not introduce in this course, can do different things.
With this higher level of introduction, let's dive in and let's talk about what is the binary class classification?
It is essentially Yes or No problem.
The label is binary.
For example, credit card default, whether this customer that uses a credit card will likely to default on that given some historic data.
Maybe there is a insurance claims and some insurance claim can be fradulant.
That can be on binary class classification.
Spam filtering.
Given this email texts, this is a spam or not.
Medical diagnosis.
Given this patient's information and lab tests and data, is this person have disease or not?
Survivor prediction given this patient's information and history and things like that, whether these patients will survive for next five years or not?
How about customer retention?
Is this customer behavior is likely to churn or not?
Then marketing action can be taken.
Image recognition, various kinds can also be binary class classification.
For example, is this animal, dog or cat?
Sentiment analysis, given this texts or Twitters sentences, what is the sentiment?
Is it negative or positive?
Things like that.
As you can see, binary class classification can have a variety of different types of data input.
It could be tabulated data, it could be image, it could be text, it could be even speeches.
That determines the binary class or not, is actually entirely for the label instead of the data itself, or the features itself.
Brief example, we can talk about some breast cancer diagnosis problem.
This is one of the features that can determine whether this tumor is malignant or not.
It can be a binary class classification problem.
We want to have some threshold or some decision value that above this value, maybe we are more sure that this is going to be malignant.
Maybe below this certain value, maybe it's less likely to be malignant.
Building a logistic regression model, will help us to find this threshold value, which is called the decision boundary by the way.
If you have more than one features, let's say we have a two features, it can be shown as a 2D diagram like here.
Our decision boundary will be likely to be a line instead of a threshold value.
Maybe this side is malignant and this side is likely to be benign.
Logistic function provides some convenient way to construct a model like this.
Loss function look like this.
It's between 0-1 and it's smoothly connect the line between 0 and 1.
There is a sharp transition around the certain special value.
Let's say this is zero, but it could be any other value.
This represents because it's between 0 and 1.
Logistic function can be a probability function.
Actually the logistic function has another name called the Sigmoid.
This is also called the Sigmoid function and the form takes this one.
The z is the linear combination of the features with the weight and bias, like we did in the linear regression.
Then this z goes through our nonlinear function, 1/1+ e to the - t. Then this entire function, as a function of z is called the sigmoid, and takes the shape of this curve here.
By the way, this z is called logit and this is a related decision boundary.
When it's set to zero, that means this is our threshold value and the probability here is going to be this one, and this g is zero, then it's one-half.
It's going to meet the 0.5.
With the 0.5 threshold we can say this is going to be malignant, and below 0.5 probability, we can say it's going to be benign.
Well, some people might ask, why don't we use linear regression instead and maybe we can fit it here.
We can fit this and then maybe find some threshold and it can also fit the probability of 0.5.
We can try to do that.
It's not easy.
First of all, we will have to find out where this threshold is and maybe we can just fit a line first and then just figure out which value will give 0.5 threshold.
But if we do that, it gives a different threshold value to the logistic regression.
The one problem with the linear regression model, if it fit it, and then find the threshold where the probability value becomes 0.5 is that it's not very interpretable.
Whereas the logistic regression with a sigmoid function, it is a well-defined the probability function.
It's very interpretable that we can find where probability becomes 0.5 and this gives the right threshold for us.
Let's talk about decision boundary more.
In univariate case, where we have only one feature.
The decision boundary is a point where it meets the probability equals 0.5.
The equation looks like this and you can get the value out of it.
If we have a two features, the data will lie in the two-dimensional space and then the decision boundary becomes a line, so we can find the line equation here, which will draw this line.
If it's a multivariate have a multidimensional, more than three, the decision boundary will be our hyperplane.
Let's talk about what if we have a multiple categories?
Instead of having yes-or-no problem, maybe we can have multiple categories.
Such as, maybe we would like to predict whether this animal is cat or dog, or maybe cow.
For the logistic regression, the logistic, which is the decision boundary, takes this form.
Then for softmax, which is multinomial.
This has another name, multinomial.
Multinomial logistic regression has this form.
They are very similar except that there is no index for k category.
This is index for category.
For example, for category number 1, we can construct this model so there will be different weights assigned to each category and for each feature.
Now with this logic.
For logistic regression, we use the sigmoid function as a probability, and we show this form, but it can be rewritten as this form as well.
This is very similar to softmax.
The softmax function takes the same form as this one, except that it now has our index for the category.
Then instead of this, now it has all the summation over of all the possible or exponent of this corresponding categories.
Softmax is called multinomial logistic regression.
However, there is another similar way that we can use the original logistic regression for multi-categories.
Maybe category A, B, C. We can construct such that it is a binary classification for A versus naught A, which we will have to combine these two cases.
This is maybe logistic regression model 1 and this is a logistic regression model 2.
We're going to do B versus naught B.
Then we're going to construct certain model that says C versus naught C. This approach is called one versus the rest, an OVR problem.
There are different ways to get the multi-category classification done.
One is, like we mentioned, we use a multinomial approach, which is a softmax.
Another way to do is using OVR.
You can find SKLearn library that utilize these two.
But I think softmax or multinomial is more common, and you will see later other classification models such as SVM and decision trees.
They have a preferred way of being multinomial verses logistic, or maybe some model is more convenient to use one versus the other.
We'll talk about that later.
By the way, both OVR and softmax, their probabilities for categories they sum to 1.
For example, probability for A plus probability for B plus probability for being C category for the sample number 1, they sum to 1.
That's the same for logistic and the softmax regression.
However, there could be some problem where maybe there are A, B, C category and we don't necessarily need to pick one of them, but maybe the category doesn't exist at all.
So neither cat, nor dog nor cow, but something else, then this should be 000.
In that case, it's called multi-label problem.
I know it sounds strange because labeling categories, what's the difference?
But this type of problem where we don't necessarily have to pick one of them in the categories are called multi-label problem.
Versus if we have to pick one of the categories, then it's a multi-class problem.
In both the logistic and softmax models they are for multi-class classification.
Then there can be some other ways to treat the multi-label problem, but we can still use the same models, but we will have to construct the labels differently and construct the training process a little differently.
That's a little bit of difference, but you will see more often the multi-class classification problems then multi-level problems, but just keep in mind that they exist.
But anyway, softmax regression can give this visualization.
Let's say we had only two features in the data-set, and the data will lay in the 2D plane, and this is going to be the decision boundary the softmax will give us.
You can see more examples here.
This ends our video.
Then in the next video, we're going to talk about how optimization works in logistic regression and how the coefficients are determined.
WEBVTT Hi everyone, welcome to the class.
This video will talk about introduction to machine learning.
Before we talk about machine learning, Let's talk about a separate password here.
Or you might have heard about data science, which some of you might be taking other courses in data science.
Surely you heard about machine learning because this course is going to be about machine learning.
Many of you might have heard about artificial intelligence, which is another buzzword along with the machine learning and data science these days.
Maybe you also heard about deep learning, which is another height world.
Let's briefly talk about what these are.
For data science.
It's really big interdisciplinary field about data.
You can think about it's actually anything to do with data, including Data Pipelining, even Data Collection and Data Munging and cleaning and Data Analysis, which may include the manual data analysis, including some simple checks or exploratory data analysis.
Or can also include the machine learning techniques to analyze the data.
Since the data science that has a really big spectrum, it can oftentimes called the soft and hard data science.
Soft data science means dealing with the techniques that doesn't require a lot of software engineering skills are a lot of math skills.
Something like data visualization and reporting, dashboard, those thing.
As well as simple Data Analysis, can fall into the category of where's the hard data science.
Those involve more mathematical and more technical skills, such as analyzing data or building systems using machine learning.
Also, data science can deal with the data that's a small size that can fit into your Excel file or something like that.
Or it's a big data that sits in the Big Data Warehouse.
In industry, the job description looks like this.
The data scientist's the job role can be very, they can do data collection, cleaning and munging data are preparing data for whatever the company needs.
Or they can build machine learning models and do the testing on those data would build a system as well as they can also do the visualization and stuff.
Usually data scientists have diverse backgrounds and they require interdisciplinary knowledge.
Machine learning.
You mentioned the machine learning several times during the talk about data science.
Machine learning is part of data science and it is also a sub field of artificial intelligence.
It focuses on learning algorithm and building models and training them on the data.
Machine learning consists of different types of loading, such as supervised learning, unsupervised learning, or reinforcement learning.
Many machine learning models, they are coming from statistical learning.
Machine learning extends the statistical learning by including more complex algorithm, which deal with more complex data and bigger data and more efficient algorithms.
Any industry machine learning engineers can develop and test the machine learning models and design machine learning experiments and build machine learning systems.
Artificial intelligence.
It has a long history in the CNS, and it is about problem-solving with the intelligence.
That means an AI agent will make an optimal decision according to its algorithm.
Whether it has a learning component or not, to maximize the goal as a response to the environment.
You might think that I feel this very practical because you're seeing a lot of applications these days.
However, AI also has a lot of theoretical components in it.
Industry AI engineers and experts.
They are more or less similar to ML engineers.
But in broad sets skills including mass and programming skills as well as machine-learning.
They work on building AI system, building machine learning models that drilling the processing, robotics and computer vision and stuff.
In deep learning focuses on neural network models.
Building neural network models and training them on data.
It also tells us a lot of optimization algorithms and training techniques in order to deal with a complex neural network model training.
It is a very suitable for complex data such as images, texts and voice, and graphs, and hybrid types of data.
It is a subfield of artificial intelligence and subfield of machine learning.
In industry them learning engineers to work on machine learning problems.
It tells is a complex data such as images and texts and things like that.
Or if a high performance computing.
We're going to show some summary diagram.
There's a Data Science which is a big interdisciplinary field, and there's AI also very big field.
Data science is about anything to do with data, including data analysis.
Whereas artificial intelligence is about solving problem using intelligent algorithms.
In intersection, when the AI algorithm is learning from the data, it is called the machine learning.
Particularly if it tells you is a complex data with a neural network architecture, it is called deep learning.
Here is a Google trend on the term on machine learning and software engineering.
Just to compare how the machine learning becomes popular for recent few years.
The term machine learning has been around for a long time.
However, it became much more popular during the last five or more years.
This graph also indicates that the job girls in machine learning has grown up really much, about 350% during the past few years.
As you can see, machine learning is a top skills in the jobs that involve the AI skills.
That already sounds like ML is very cool.
Let's talk about what ML can do.
Machine learning is applied everywhere these days.
For example, when you do online shopping, you often see this product recommendation based on your browsing and shopping history.
Those use machine learning algorithms to predict the products that are more likely to be purchased by the customers.
The same goes for movie recommendation and music recommendations.
Sentiment analysis is very popular applications these days.
It is standard by now, the data scientists analyze the text, such as news articles and social media articles to figure our citizens sentiment on political events.
Similarly, the product review scales or restaurant review scales can be predicted by machine learning algorithms, which information can be important for businesses.
Machine learning is also used a lot in the financial industry.
For example, we can forecast the stock price using machine learning.
Machine learning is also used for algorithmic trading, as well as robo-advisor, which gives advice for people or how to allocate their assets.
Also, it can be used for forecasting housing price.
As you can imagine, machine learning can be also useful in medical industry.
By applying machine learning models in the images or tables, you can help doctors to make a medical diagnosis or medical decisions.
It is also used in many science disciplines, such as the bioscience.
For example, machine learning techniques can be applied to this graph data to inspect the protein interactions or this type of data for the study of genetics.
Machine learning is also key component in other things.
Smart sensors and smart devices produce a lot of data.
Also, machine learning plays a key role on analyzing those data.
Machine learning is also used in self-driving cars.
Self-driving cars can use machine learning and deep learning to recognize images and make decisions.
Let's talk about what we'll learn in this course.
Here is a data science project life cycle.
Data should be collected and pipelined into data warehouse.
There's a data governance that though data warehouse it has to implement.
There will be also data pooling and cleaning and maintaining the data.
That part is called data engineering mostly.
Data science, on the other hand, focuses on using the data and analyzing those data that were prepared by the data engineering process.
You can include the selection of the dose data from the warehouse and then cleaning those data and exploratory data analysis and data preprocessing, which means that we prepare data for the model to consume.
After that, data scientists will build the models and then do the model training.
There is a result and depending on the result, they had to go back to build models.
Or if the result is so strange, then they will have to collect more data or select more data and do this cycle again.
Among the steps that we mentioned in the data science project cycle, we're going to talk about a few things and we don't cover everything for example, this course is not about data collection and pipelining.
We'll talk about a little bit of data cleaning and EDA and data preprocessing.
But the main focus will be how to build a model, how to select models, and how to do the training and do the testing and analyze those results.
Let's talk about what is learning?
When these children learn alphabets, they can learn to generalize.
For example, they can recognize this letter, whether it's a small or large.
It has a different font or it has a different color.
Or the image it looks angled, or the letter is in the word.
It seems so obvious for people, however, to make a motion to learn this.
It is not trivial.
Here's some example of supervised learning.
There are images and the labels.
The labels are the names of these animals.
A supervised learning model learns to predict the label given data.
Unsupervised learning actually resembles very much how a human learn.
In babies stage, they don't know about the geometric shapes and colors but overtime, they learn to recognize this visual properties and also recognize the similarities and dissimilarities between them, even before they learn about the names of this color and shapes.
Those type of learning without labels are called unsupervised learning.
Unsupervised learning is all about learning underlying features and extracting information, recognizing patterns in the data, or clustering similar data points.
Another type of learning is reinforcement learning, which the AI agent learns how to act from experience.
Experience is either reward or punishment.
It is very much similar to the animal training.
We give treats or punishment to make the animal behave desired way.
Reinforcement learning is used a lot in AI and robotics.
Let's change gears and then talk about some definitions that frequently show up in machine learning.
Data in machine learning can be any forms, such as tables, images and texts, and sounds and graph.
It can be any format.
However, we're going to talk about mostly the tabulated data format.
Let's take one example of this table.
Let's say the unsupervised learning task is to predict a house price.
This one would be our labels.
In this labels also are called targets.
All this columns except the labels are there called the features and also they are called predictors.
Which means that those features and predictors are used in the machine learning models to predict the labels.
This row of the tables are called observations or samples.
Which means that they are instances of data.
Here are some few examples of machine learning tasks for example, prediction include the classification regression, and these are in supervised learning and clustering into groups, the similar data points together and anomaly detection and dimensionality reduction.
They are in a category of unsupervised learning.
There are other emotional learning tasks, such as data generation and feature selection which are typically not categorized as supervised learning or unsupervised learning.
However, this type of machine learning tasks can be used to enhance the performance of a supervised tasks.
Let's talk about prediction tasks in supervised learning.
Prediction task can be either classification or regression depending on the labels datatype.
When the label is categorical variable, which means a zero or one, or 1,2,3 or A, B, C. These are called categorical variables, and in that case, it becomes classification.
If the categories are binary, it is called the binary class classification.
If it's a multi categories call them multi-class classification.
Binary multiclass.
The other hand, if the label variable is a real valued variable, so something like 0.1, 0.999, something like that, or 3.4.
Real value variables to predict those labels given the data is called a regression problem.
With that in mind, let's talk about how supervised learning works briefly.
Here's the data. Data consists of features and target.
Feature usually we call x and target is y.
Then we have a model, and this feature is input to the model.
The model might have a parameters inside or hyper parameters.
That means some settings the user get to choose.
Then after feeding the features into model the border will make a prediction.
Initially the model does not predict very well.
There will be some error between the prediction and target variable.
This adder can be used to tweak the model to have a better prediction next iteration and over-designed iteration, the model becomes more accurate.
That's how supervised learning works.
Here is a brief taxonomy of unsupervised learning models.
Models that has internal parameters, it is called the parametric models.
Without explaining what those are individually, these are examples of parametric models, whereas non-parametric models or doesn't have internal parameters.
These are examples of non-parametric models.
Although we're not going to talk about every single model here.
We'll talk about most of these models in this class.
WEBVTT All right, so how do we find the coefficients?
So again, this beta 0 beta 1 are coefficients and this is my model and the difference between the target value and the predicted value is called the residual.
So here is the plot that plots the residuals.
So this is a residue that has positive value and these are the residues that have negative value.
So when you say how good my model is, that means how small is the error overall?
So we need to find an error measure that accounts to all these residuals from all the points.
So, how do we do that?
One way to do it is just sum them up.
However, it's going to be 0 all the times if the regression line was fit, so this is not very useful.
So, we're going to define another error measure that measures the distance instead of just summing all of these residuals.
So, we're going to have absolute value and then sum them up to N samples.
And in case we have many, many samples, this quantity can be very big.
So we want to divide by N, then it becomes a mean absolute error which is a one good way to measure error.
Another way we can do it is we can maybe scare each residuals and then sum them up.
And also we can divide by N and this gives mean squared error.
So these two are very popular error measuring regression tasks.
There are some other error metric that can be also useful.
So, we talked about MA E but MAE can be arbitrary large depending on how large Ys are.
Therefore, we can define percent absolute error instead.
So percent absolute error is a mean of absolute value of this.
This is a target variable value and this is a prediction value and that's divided by target variable value again, and takes the absolute and sum them up and takes an average.
So that can be handy, that can be useful metric.
We talked about mean squared error, but mean squared error unit is different from Y's unit.
So in case we want to compare in the same unit, we can take a square root, then it becomes a root mean square error, which is also good metric in regression.
All right, so let's talk about how the optimization in linear regression work.
There could be various methods but this method called least squares method is the most popular and almost all Python package that solves a linear regression uses this method.
So as a reminder, the model takes the features and it has an internal parameters and linear regression does not have a hyper parameters.
And it makes a prediction, and we want to find out the value of the parameters that will make this prediction accurate as possible.
And this is done by optimization.
And for this squared method, the linear regression mostly use takes the feature and target value and find a solution for the parameters.
And the name suggests least squares because it uses a squared error.
So we're going to use MSE and let's have a look what the error surface of MSE look like.
So in MSE, the error in the coefficient space where this axis is one of the coefficient and the other axis is the other coefficient and this axis represent the error, the size of the error.
Then this takes a kind of bow shape like this, so if you look at from the top, the contour will look like an ellipsoid like this.
And then if you look from the side, then it will look like parabola.
So it has some minimum value at the bottom of this bowl, so we would like to find a solution in one of the values for A and B at the bottom of this MSE surface.
To find out the minimum value of the MSE, we'll take a derivative with respect to each of coefficient.
So for example, partial derivative MSE with respect to coefficient a and set it to 0.
And, Similarly, we can take a partial derivative of MCE with respect to b and set it to 0.
Why we do that?
If you think about the parabola shape at the bottom, the slope or the gradient becomes 0, so we'll use that fact.
And if we do the algebra, we're going to get the solution without derivation, but you can look at the supplemental node that has all the derivation.
Important thing to remember is that the slope is proportional to the covariance of the variable X and Y and then inversely proportional to the variance of X.
And similarly, intercept has this relation.
And if you look at carefully, this suggests that actually the regression line passes through the center which is mean of X and comma mean of Y.
So regression line is centered around this mean values for the X and Y.
What happens when we change the scale of the variables?
So for example, we have a big value for living space square foot and a really big value for sales price.
So we want to change the unit, for example, makes it million dollar as a unit and use a small number and maybe we can divide by 1,000 for the square foot living.
So in that case, if we change this by 1 over 1000 of original value and this is 10 to the- 6 of original value, what happens to my value for beta1 and beta0?
You can think about it for a while.
All right, we're back.
So we're going to call it r and we're going to call this as s.
And as you know, covariance is calculated by this formula x- x mean times y- y mean, an expectation of this value.
And then similarly, the expectation of x- x mean squared.
So, this part is scaled by r, and this part is scaled by s, and this part scaled by r squared.
So as a result, we're going to get s divided by r times original value of beta 1.
So if we plug these numbers, we're going to get 10 to the- 3 times original value for beta 1.
So my slope get 1,000 times smaller, if I make my x variable 1,000 times smaller and make my y variable a million times smaller.
What happens to beta 0, my intercept?
So this is going to be s times original value of E[Y].
And this, we already calculated it's going to be s over r times the original value, which I'm going to just say.
And then this quantity becomes r times the E[X], so this cancels out and then we're going to get s times original value beta 0.
So my intercept doesn't change when I scale the X, however, it's going to change when I scale the Y, and it only depends on the scaling of the Y.
So let's talk about how we generalize the least squares method to multivariate case.
So when we have P number of features, this is a feature matrix, and we add a column that has a ones, so that it can take care of the intercept term.
So together with this, this total metrics is called design metrics.
And this index 1 to n, is for the sample index and this 0 to p is for the feature index including the intercept.
So, MSE in matrix form is going to look like this, y- x beta, and these are all matrices.
And then two norm of the matrices, that is actually the y- x beta transpose and y- x beta.
So when we take a derivative with respect to beta, then we're going to get this equation.
And if you further simplify it will look like this.
And this is called a normal equation.
And then solving this equation for beta, it gives a solution like this.
So it involves an inverse of these metrics inside.
And sometimes it can be a problem if the rank of this matrix X T and X are not equal to N.
And when does it happen?
It happens when there are two or more variables or the features are linearly correlated.
So for example, if my X1 values were 1, 2, 3 and some of the other feature, let's say X5 was linearly dependent on X1, so for example, two times of this, something like that, then these two features are redundant.
Therefore, this metrics becomes a non-invertible.
And then there is a problem when we try to get the solution beta.
It actually doesn't mean that we don't have solution, it means that we have a solution that are not unique.
So we're going to have a hard time to determine unique solution.
But anyway, almost all Python packages that solves the ordinary least squares, OLS, has some mechanism to find the inverse metrics of this called pseudo inverse.
And sometimes this is called Moore-Penrose, Inverse.
So with this, we don't have to worry about non-inverted matrices.
WEBVTT Hi everyone. In this video, we're going to talk about linear regression.
We'll begin by the definition of linear regression.
We'll talk about how this model get optimized to get the best estimate value.
Then we're going to talk about important quantities for linear regression, such as fitness, performance metric, things like that.
We'll talk about how statistically significant these estimated values are.
Let's begin by reviewing how supervised learning works.
Supervised learning needs the training data that feeds to the model.
This model has internal parameters.
Sometimes some model don't have parameters at all.
The models that have a hyperparameter as well, the user need to tweak.
But anyway, with that, the model can predict the value.
If the parameters for the parametric model is not optimized, then this prediction value will be far away from the target.
Our goal is to tweak this parameter by optimization so that the model makes a prediction that's close to the target as much as possible.
So what is the linear regression?
It is one of the simplest kind of supervised learning model.
It predicts a real value number, which is regression.
Then it has the parameters inside.
These parameters are often called the coefficients.
It does not have a hyperparameters.
That means the user doesn't need to figure out some design parameters in advance or during the training.
Importantly, linear regression model assumes a linear relationship between the features and the target variable.
Well, what does it mean?
It means the feature, let's say we have only one feature for now, has a linear relationship to the target variable.
Let's say it's a house size.
This is the house price.
Then there could be some data like this, that tells us that when the house size gets larger, then the house price gets larger.
Another example could be maybe we want to predict the salary of a person as a function of their years of experience.
We might have some data like that, that shows that in general, when the years of experience goes up, then the salary goes up.
It doesn't have to be positive slope all the time.
There could be some other example like this.
Maybe the data looks like this and this is age.
This is a survival rate from some disease such as cancer.
Then maybe there is a trend that looks like this.
As the age goes up, maybe survival rate goes down.
These examples show some kind of linear relationship of the feature to the target variable.
When we have a multiple feature, linear model also have some linear combination shape.
What that means is this.
If I have a feature X1 all the way to feature Xp, and they are linearly combined to each other.
X1, there is a coefficient a1, and I add up another coefficient times x2 plus et cetera.
Coefficient for feature p.
Then I can also add some free parameter A0 for the intercept.
This becomes my linear model.
This is called a linear combination.
This type of model, whether we have many variables or one variable that shows some linear relationship of the variable to the target, and this type of model is called linear regression.
Let's take an example.
This data is coming from Kaggle website.
Kaggle is a repository for machine learning data.
So if you want to build a machine learning model and train to the data, this is a place to go.
This website also hosts the ML competition.
That means a lot of competitors build their models that fits the data and then they will compare their model performance on this platform.
This is super fun, so you should try.
Anyway, this data comes from there, and this data is about predicting the house sales price in Washington state where there are a bunch of features that describes the house.
Price is our target variable Y, and all these other columns are our features.
And because we want to build a simple regression model like this, we want to find out which feature could be a good predictor to predict the house sales price.
If you have a domain knowledge, you can think about what feature will be useful to predict the house price, or you can think about maybe number of bedrooms are important.
The more number of bedrooms then maybe it's more expensive, or you can think about the size of the house matters, or you can think about the location of the house matters most, things like that.
However, to quantify and have some evidence that which features is most important or likely to important to predict the price, we can have a look at the correlation matrix.
Correlation matrix gives a correlation values between the features.
Diagonal elements shows the correlation to the cell.
It has the value of one all the time.
However, the other off-diagonal terms, they show the correlation between different features.
Because it's too many, 21 features, I'm going to select first a few and then I'll look at it.
As you can see from the first row, which is correlation values for all other features to the price, you can figure out the square foot living, which is a house size, is most correlated to the price.
There are other features such as the grade of the house that comparably good to predict the price.
You should be careful when you select multiple features based on correlation matrix because the order of correlation, that means a high correlation or absolute value of a correlation to lower ones.
These orders are not directly related to how important the features are.
For example, this feature may have the same or comparable correlation value to the price with the square foot living.
However, square foot living in grays are highly correlated, so when I add this feature to my model on top of a square foot living, that doesn't add so much value because this is pretty similar to this one.
In that case, some other variables such as floors or something like that, or maybe view would add better value to predict the price than this one that has a high correlation to the price.
You have to be little bit careful.
We're going to go through a method that actually helps to select the features in right order, but to select just one feature correlation matrix gives a good information.
Let's begin by that.
Let's talk about univariate linear regression.
Univariate means the variable is only one.
Also for that same reason, univariate linear regression is called the simple linear regression and often takes this form that we have a coefficient Beta 0 and Beta 1, which represents the intercept and slope.
Then it has residuals that measures the difference between the target value and the prediction value by our model.
This residual is important to measure the error and this is for each data point.
For example, if we have some data that looks like this and maybe this is my regression line, then this is going to be my intercept and the slope Beta 1.
Each discrepancy of the data points to the regression line is called the residuals.
Our goal is to minimize the overall residuals of my model and make my model to produce or predict the value that's as close as possible to the target variable.
This can be done using a single line using statsmodel OLS package, or there are other packages such as sklearn linear model.
However, this is useful because it generates some summary table like this.
This summary table has a lot of information including the most interesting part, or rather my coefficient values.
With this coefficient value, I can determine what's my slope and my intercept is for my simple linear regression model.
Beside of coefficient values, we can ask some other questions that are important to linear regression.
We'll begin by how do we determine the coefficients?
In other words, how does the model training works under the hood of this package?
We'll also discuss how well my model fits.
From the summary table values, what gives an idea of how my model fits?
Then we'll also talk about how statistically significant my coefficients are.
That means how robust our estimation for the coefficient is.
We're going to also talk about how well my model predicts on unseen data.
That means, how well does it generalize, which is very important in machine learning.
WEBVTT Hi everyone in this video, we're going to talk about multi linear regression.
So previously, we talked about simple linear regression where we have only one variable.
And now we're going to add more variables, whether it's a high order terms for that simple variable or other features into the model.
And then the key idea we're going to discuss is that when the model complexity increases by adding more features, we can fit the data better, but it can also introduce some other problems.
So we'll introduce a concept of bias variance tradeoff and then we'll talk about how to select the features that are most contributing to the model.
So last time we talked about simple variable linear regression, which takes the form of y=a0+da1x1.
So x1 is one feature that we care about and 1 is a slope and a 0 is a coefficient for intercept.
So the example we had was the, you can predict the price of the house sales as a function of size of the house, the size could be x1.
And the price is the y that we want to predict.
And now let's say we want to add another feature, say and besides of the lot.
So when it has a big lot, then maybe it's more expensive than the same small house that has a smaller lot.
So we can think about this and we can add the new term, new feature into our model a2x2.
And similarly, we can add more features such as a number of bedrooms and things like that, then it becomes more complex model and so on.
So this is also linear regression, especially it's called multi-linear regression because it has a multiple features.
But we can also make some other model that has a high order terms of the house size.
For example, we can have a square term of the house size.
So in that case, we'll have a1x1+a2x1 squared.
And that could be also a good model.
And if you want to add more complexity or high order to this model with the same feature, we could add a third term the cubic term of the house size like this and we can add more.
So in this case, it's called the polynomial regression.
It is small multi-linear regression.
We can also engineer some features instead of having square term and cubic term and so on, we are not restrict to have just a high order terms.
But we can create some other variable or features using existing features.
So for example, if we are predicting some probability of getting diabetes based on height of a person and weight of the person and some other features that we measured from the lab and so on.
Instead of having this model, height plus a2 weight plus and so on, we can construct another variable, let's say called x prime and which is BMI, which is proportional to weight divide by heights squared.
So this BMI is a function of x1 and x2 and this becomes a new feature x prime and we can have instead a0+a1 x prime.
And the things that we wanted to add like lab test and things like that, something like this instead of having a height and weight, separate features.
So there are many different possibilities that we can engineer like relevant features depending on your domain knowledge or your intuition on the problem and so on.
So linear model can become really flexible in this case.
So we're going to talk about what happens if we start adding more complexity into model.
And then there are some things that we need to be careful.
So we'll talk about those.
So let's start by polynomial regression.
This m represent the order of the maximum term.
So m=1 represent that the simple linear regression ax+b and then m=2 will be a0+a1x+a2x squared and so on.
So these are the complexity of our model.
So when you look at the simple linear regression, it looks a straight line which is okay.
But it's still maybe a little too simple for this data.
So let's add another term square term and then maybe it fits a little bit better and we can add a cubic term.
And then you can see as you add more high order to the line, the fitted line becomes a little more flexible and have different shapes of the curve some point.
The fitting fails actually.
And what happens here is that I wasn't very careful about scaling of the feature x.
So in my simple linear regression model.
This was on the order of 1000 and my y is going to be on the order of million.
Then this coefficient could be on the order of 1000 or less and so on.
And then this square term would be on the order of million.
And by the time I have the size of the house is 6 power, this could be 10 to 18, which is a really big number and the coefficient to match this number should be very small.
That means the computer has a hard time to calculate all these coefficients.
Therefore, the fitting may not work very well in order to prevent this disaster.
One way you can do it is just to scale the feature to something on the order of 1 instead of thousands.
So if you just divide by 1000 of your features, then you could have 1 to 6 7, something like that.
And then here you're going to have 1 to 6 or 10 to 6.
So it's a more manageable.
So therefore, you can add more high order terms if you want to.
However, you will see shortly that we don't want to add high order terms indefinitely.
So it leads to a question.
Where do we want to stop adding high order terms.
Obviously, when you see the model fitness will go up and up as you add more model complexity.
So you have some data, like this and your model could be a little crazy that it has a really high order and can fit everything like this.
This model is not very good.
First, it's not very interpretable, but second, it's more vulnerable to new data point.
Say this one, it will have a huge error or maybe like something like here, you will have a huge error with this.
However, if you have a simpler model, it will have a smaller error at this new data point and things like that.
That's the motivation, how do we determine where to stop when we add model complexity is we want to monitor errro that's introduced when we introduce new data points.
So you remember we talked about how to measure the test data error and training data error.
So we had a data set that we have both feature and label and then we set aside some portion of this data and called it test data.
Another name for test data that's used while we are training is called the validation.
So we can call them interchangeably.
But in motion learning community validation error is a more used term for the data set that's set aside for the purpose of testing while you're training the model.
But anyway, with this, we can measure editors for the training and testing.
So let's say we picked MSC then as you mentioned before, we have a trained model and measure, we can have the prediction from the training data.
And with the training label, we can calculate the mean score error for any error metric of your choice.
So that becomes error for the training.
And we can do the similar for the test data MSE prediction value.
And then Yte, then we can have the error for the test data.
And this f correspond to each different model with the different high order terms or different model complexity.
So this is m=1 and this is model with the m=2 etc.
And then when you plot the exact shape of the curve for training error and test error will be different depending on your number of data and the data itself that you randomly sample.
And also it will depend on your model complexity and so on.
However, in general, you're going to see this type of error curves.
So for training error, it will go down as you increase your model complexity.
However, the tested will go down in the beginning and then at some point, it will start going up again as the model complexity is increased, then we can find the sweet spot here that the test error is minimized.
So we can pick our best model complexity equals 2.
You can also see this model complexity.
And because the three model is also comparably good.
And in some case, depending on your data draw, it can show you actually slightly the better results than model complexity equals 2.
However, if they are similar, then you want to still choose the simpler model.
And this kind of principle is called Occam's razor insistently telling that if the model performances are similar for simpler model and complex model, we prefer choosing simpler model.
WEBVTT The behavior of test error that goes down first and goes up later as we increase the model flexibility can be explained by bias-variance trade-off.
What is bias and variance?
Let's have a look at graphical explanation.
When the bullets are well-centered and well-grouped, they are called the low bias and low variance, and when the bullets are well-grouped but far away from the target center, then it has a high bias because it's far away from the center or true value, but it has a low variance because they are well grouped.
On the other hand, if the bullets are quite spread, but it is still well-centered around the target, then we can say it has a low bias and high variance.
As you can imagine, if bullets are not close to the center, but it also has a large spread, then we say it's a high bias and high variance.
How does that translate to machine learning?
In machine learning, we have data from real life and this data can be very complex and we don't know what the true model is.
By making a model, we introduce some assumption, and there's an error that's caused by a simplification, by choosing our model, and this error is called bias.
On the other hand, variance in machine learning means variability of the model.
What is the variability of the model?
Let's say we had some data that look like this, and then we were to fit the data with the simple model and maybe we can also fit with the more complex model that tends to have a lower error, so lower bias.
However, if we chose different data set like this, for example, then if we fit the simple model again, they will be very similar.
But if we fit the complex model, now it's going to be little different from the previous.
This variability of the model is called the variance of the model.
If the model is simple, they tend to have low variance.
They don't change much even though we changed the training data, but when we have more flexibility in the model, it may change quite a bit depending on how we choose the training samples, so they tend to have high variance.
Simpler model tends to have a high bias and low variance, so it will correspond to this one, and more complex or flexible model tend to have low bias but has a higher variance, so it will be this case.
In machine learning, a lot of models are either this case or this case.
There is a trade-off between the two.
That's where the bias-variance trade-off coming from.
Sometimes if the model is not very good, then you may encounter this case.
Also some type of model such as a deep neural network with some other tricks, they may have low bias and low variance.
But most of cases, we have the trade-off between the bias and variance.
Back to our test error, why it goes down and then goes up because of the bias-variance trade-off?
This is model complexity, this is test error or error in general.
The bias goes down as our model complexity increases, and the model variability goes up as our model complexity goes up.
When you use a squared error, you can actually derive the general relationship between bias and variance to the test error.
Test error MSE can be written as a variance of the model or estimated model, and then the bias of the estimated model also in squared plus some irreducible error, the variance of the residuals.
You can have a look at the supplement or not, but this is the result and according to this, the test error is the sum of this variance of the model and bias squared of the model.
In the end, our test error will have a shape of this because it adds to this too, and then there is some irreducible error from the residuals.
That's how the test error shape looks like this.
However, in reality, depending on your model and data, your test error may just go down and then flattens and that's very common, whereas your training error goes down and down.
Sometimes, the simple model fits well to the data already in the case you may have already good test error for the simple model as well like this, and then it goes up like this.
Something like this. Also note that this doesn't have to be squared error.
It is very general behavior no matter which loss function or error function you have.
In summary, we talked about what happens if we add more complexity to our model, we talked about polynomial regression and where we stop adding more terms to the model by monitoring training and test error, and we also talked about the bias-variance trade-off principle.
WEBVTT Let's talk about how significant the coefficient values are.
When do you say the coefficient values are significant?
Conversely, when can we say that the coefficient value is not significant?
Think about this, when the coefficient value is not significant, that means we picked up some noise from the data and assigned some value for coefficients when in fact the coefficient value is zero.
When you hear coefficient value is not significant, that means the coefficient value should be actually zero.
Let's look at this coefficient value, it's minus 4,000 something and it's a 280 something.
That looks like that's very big number, big difference from zero so maybe we can just say my coefficient values are all significant.
The absolute value of the coefficient value is not enough to say whether it's significant or not, because consider we change the unit of this target variable, then we can certainly have a very small coefficient number and it's hard to tell then whether this number should be zero or not.
We need some comparison, we need some way to compare whether this number is good enough or not.
Usually the standard error is a good way to tell it, so that means if my coefficient value is maybe here, let's say Mu average of my coefficient value is here and this is zero, and we want to know how far away is my average coefficient value, and also we want to know how much of a spread I have.
If the spread is pretty large like this, then maybe this mean value for my coefficient isn't very real.
However, if I have this sharp distribution, that essentially says spread of my values for the coefficients are this small and this far away from zero, then I can say with confidence that my coefficient value is actually real.
We're going to talk about this and all these values that shows here are good measure of those confidence or statistical significance, so let's dive in.
We mentioned that it is important to know the standard error or the spread of my coefficient value, and there are different ways to get the standard error or the spread of my coefficient value, one is using some theory or assumption that the residual is some normal distribution with zero mean and certain spread or variance.
Another way to do that is we just re-sample the data multiple times and then fit onto that data and get the coefficient value and we do that experiment multiple times.
Then we can get the standard error of my coefficients and all kinds of statistical values from there.
But let's briefly talk about what this model-based method is.
We wrote derivation, this is called the covariance metrics, which is variance of Beta 0, variance of Beta 1, and covariance of Beta 0 and Beta 1 , something like that.
Matrix looks like this but, however, we don't have to remember all this math.
This is given by this formula and this leads to the standard error value for intercept and slope looks like this.
However, we don't have to remember all this formula.
But important thing to remember is that all of these variants or standard error value is proportional to the variance of the residual.
That means if I have data that has the largest spread like this, then it's likely that my coefficient values also have a larger spread.
Also you can see that the spread of the coefficients not only depends on this variance of the residuals, but also the variance of the data itself.
This model assumes homoscedasticity, that means the spread of the data is homogeneous over the data.
However, if you look at our data that looks like some cone shaped like this, the spread of the residual is not homogeneous.
However, we can still assume this model and then derive the quantities that we need.
If you're not convinced by the model's assumption, then maybe we can use bootstrapping method.
Bootstrapping method is re-sampling method, so let's say we had data point that looks like this originally.
Then we can sample some experiment, the samples are some of the data point like this.
Then we can have that and then draw another one that samples this data and so on.
We can have multiple copies of this.
We can have many samples that we want.
We can even sample the same data twice or multiple times, it doesn't matter.
We can have some sampling with replacement.
Let's say we have many data like that and then we can fit the coefficient values and this coefficient value will be different from this one slightly but they will be similar so we get all these values then we can get the mean value of this as well as the standard deviation or variance of that value.
That's how we get standard error for the coefficient values.
Let's talk about how we determine whether our coefficient values are statistically significant.
To do that we're going to do the hypothesis testing with the two hypothesis, the null hypothesis that say that our coefficient value is zero and our alternative hypothesis saying that our coefficient value is not zero.
To test that we're going to construct a t-score which is given by this.
T-score is a standardized our coefficient value or estimated value by subtracting the mean which is given by our hypothesis and the standard error of our estimated coefficient.
This is similar to the score in normal distribution and actually when the number of samples is larger than 30 the t-distribution approximate the normal distribution so they are essentially the same most of cases.
We're going to calculate the p-value and we can briefly review.
Let's say we have a standard normal distribution like this, this is zero mean and has a unit variance.
Then let's say we want to have a five percent of error rate so that gives us critical value which defines that this area within these critical values, +1.96 and -1.96 for standard normal distribution.
This area is 0.95 and the rest, this region and that region, the combined area will be 0.05.
That's our error rate and this value is also called Alpha.
In standard normal distribution this shaded area are symmetric so each of them is going to be 0.025.
Then we're going to have p-value according to our t-score so whenever our t-score lies in the rejection region which is shaded in this red area or maybe here then we can reject the null hypothesis.
What is the p-value here?
P-value is this area under the curve and close by this t-score or this area in case the t-score was negative.
In that case, in this particular example, our p-value is smaller than the half of the Alpha so this green area is smaller than the rejection area and in that case our t-score lies in the rejection region therefore we can reject the null hypothesis.
What if our t-score lied in here?
Then again our p-value will be this big so when our p-value is bigger than the half of the Alpha we cannot reject the null hypothesis.
Let's see if we can reject the null hypothesis from our regression result.
That looks like this.
When we look at the t-value it's -10.
I'm going to change my color, it's -10 Sigma away from the mean and for the intercept and for the slope it's 145 Sigma away from the mean so that's pretty significant.
As you can expect the p-value is fairly small, almost a 0.2 coefficient therefore we can safely reject the null hypothesis and we can conclude that our coefficient values are statistically significant.
Similarly, we can also define 95 percent CI, confidence interval for the coefficients.
To calculate that, the formula is given by this, so mean of the coefficient plus-minus two, or actually to 1.96 times the standard error and the standard error is given here.
We can also define 95% confidence interval for the regression line.
That means 95% of time my regression line will lie within this orange shaded region and 95% prediction interval, which is for the sample points, that means 95% of the time the sample points will be reading this blue shaded region.
This analysis can be handy when you have some outliers and these outliers may be good to remove to have better regression.
Let's talk about how we measure the error from the test data and the training data and how to compare them.
We talked about this popular error measure, so we're going to use them or one of them.
Let's say we have original training data that we used to use to fit the model and instead of using all of them to fit a model, we're going to set aside some data, some portion of data as test data and the rest, we're going to use it for training, test set.
Each of them have feature and label, so trains, it has feature x train and label y train and the test set has feature test and labeled test.
Using the train set, we're going to fit the model.
It's a model initially had the undefined coefficient values, so we're going to do fit and then supply our train data, x train and y train.
This fit function will determine the coefficient values and now this model internally will have optimal coefficient values.
With that, we're going to predict this time , so that predict.
For prediction, we don't need a label.
We're going to put train data then it becomes y prediction.
I'm going to put that here but from the training data.
We can do the similar with the already fitted model and that predict and supply test data instead this time and this will give y prediction from the test data.
What do we do with this, this value and this value?
We can measure the error between the prediction value and the label, so for example so training MSE or error for the training data is going to be MSE of y true value for the training labels, so which is this one and the y prediction value from the training data from this one.
That's going to be our train error.
For example this one.
We can carcass similarly MSE for test data.
MSE test TE is going to be MSE y test.
The true value and then the y prediction from the test data and this value is this for example.
It's very common that the test error is slightly larger than the train error or if the data were pretty homogeneous and your model is doing well, then train error and test error could be similar value.
Will say later that if my model is over parameterize, then it doesn't do very well in the test data and it's important way to figure out whether my model is over parametrized or not.
We'll talk about that later.
In summary, we talked about how we determine the coefficients, so we talked about least squares method which minimize the residual sum of squares.
We talked about what the RSS is, what the mean squared error is, and bunch of other error metrics and we also talked about the goodness of the model fit.
We talked about R-squared and how R-squared is derived.
We derive it using RSS and TSS.
We also talked about some things that we need to be careful when we interpret the R-square value.
We also talked about significance of the coefficients, so we talked about standard error of the coefficients, how they are derived or can be determined.
Then we talked about t-score and the p-value and hypothesis testing to say whether the coefficients values are significant or not and then we talked about communists intervals as well.
Lastly, we talked about how to better the error for training data and test data and how to compare them and that's it for the simple linear regression.
In the next videos, we're going to talk about what happens when we add more model complexity, such as higher-order terms or other features into the model.
WEBVTT Let's talk about how well my model fits.
We're going to look at the numbers R-squared value and Adj-R-squared.
These are metric for how well the model fits.
Adj-R-squared is actually same as R-squared except that it also takes number of features into account.
However, when the number of samples are much larger than number of features in the model, these two numbers are essentially the same.
Let's derive R-squared as a measure of model fit.
When do we know that model has a good fit?
From the least squares method that we used to determine our coefficient values, we know that model has a good fit when we have a squared error is minimized.
Again, we can use MAC or RSS.
RSS is the residual sum of squares is nothing but same as MSE.
We wrote the averaging vector.
We define this quantity and we know that if this quantity is minimized, we know the model has a good fit.
However, there is a little bit of problem with this metric.
One is that this value can be arbitrarily large depending on our unit of the target variable.
Also if you have a different set of data, this quantity will be different.
We want to normalize by something similar error measure that has same union.
What would it be a good way to do that?
We can define a benchmark model, say y equals y min and then we can compare how good is my error from my model y equals Beta zero plus Beta one x compared to the editor of my benchmark model, which is y equals y min.
We're going to define another quantity called the TSS, total sum of squares that actually quantifies the error between my null model and my training data points.
With that, we can define a dimensionless quantity by dividing RSS by TSS.
This is the quantity essentially telling that what's the ratio of the error from my model to the error from the null model or benchmark model.
This can be a good quantity that measures how well my model fits compared to my null model.
But we also won our continuity or R-squared value to be higher when my model fits better.
If you see RSS goes down when the model fits better so we're going to flip the sign.
We're going to subtract this quantity from one.
Then actually it becomes the definition of R-squared.
Let's take a moment and think about what values R-square can take.
We're going to think about two extreme cases.
One extreme case is that when my RSS is 0, that means my model fits perfectly all of the data points, which we'll never happen in practice.
I think that my model is so good that all the data points are on my models line.
Then this term goes to 0 and my R-squared value will go 1. That's one extreme.
Can R-square go larger than 1?
R-squared value cannot be larger than 1 because RSS cannot be negative.
Another extreme case is that by model is actually just as good as my null model, y equals y mean.
In that case, my RSS value will be same as TSS so this goes to one.
Then my R-squared will go to 0.
Can R-squared value go negative? Yes, it can.
In practice, if you use a package to fit your regression line, you will almost never happen.
But in case your model is this bad, like this, the slope is totally wrong.
Then you might have an RSS that's larger than TSS than this R-squared value can go negative.
For simple linear regression, this may not happen.
But as you might see later in the more complex models, sometimes the model can fit worse than the baseline so remember the R-squared can go negative as well.
We saw the R-square value could be a good measure of how my model fits.
However, you have to be careful when you interpret the value from your summary table.
Let's take an example where we might want a model that takes the form of ax and there's no intercept.
Why would we want to do that?
Let's have a look at the intercept value.
It's a negative value.
That means my sales price will go negative when my living space is 0.
That doesn't make a lot of sense so maybe instead of having this uninterpretable intercept, maybe we want to have a model that has no intercept.
Then yeah, that sounds good.
My sales price of house should be 0 when the living space is 0.
Let's take a fit and look at the summary table.
We have a square feet living coefficient, which is similar to the previous value, which is good.
But then we suddenly see R-squared value has gone up. What does that mean?
Does it mean our new model, y equals ax is better than our old model, ax plus b?
Well, not necessarily.
If you look at carefully, you're going to see uncentered next to the R-squared.
What does that mean?
It turns out that this R-squared value is calculated such that RSS of our new model, this guy and then divide by TSS of the new null model, which is not y equals y min.
But now our new null model is y equals 0.
This goes to here.
Then the total sum of squares from y equals will be way higher.
Therefore, the R-squared value can be much larger than the previous one.
If you want to compare apple to apple, how my new model is doing in terms of the error, you can just directly calculate RSS for our new model.
Let's say y equals ax.
Then compare with the previous model.
Notice as y equals ax plus b, then you're going to see this RSS is larger than this one.
But the value that gives here in the summary table is a little bit deceptive.
