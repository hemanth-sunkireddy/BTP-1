WEBVTT

1
00:00:04.430 --> 00:00:07.185
Hi everyone. In this video,

2
00:00:07.185 --> 00:00:09.345
we're going to talk
about linear regression.

3
00:00:09.345 --> 00:00:12.495
We'll begin by the definition
of linear regression.

4
00:00:12.495 --> 00:00:14.400
We'll talk about
how this model get

5
00:00:14.400 --> 00:00:17.415
optimized to get the
best estimate value.

6
00:00:17.415 --> 00:00:19.215
Then we're going to talk about

7
00:00:19.215 --> 00:00:22.140
important quantities
for linear regression,

8
00:00:22.140 --> 00:00:26.550
such as fitness, performance
metric, things like that.

9
00:00:26.550 --> 00:00:28.455
We'll talk about
how statistically

10
00:00:28.455 --> 00:00:31.530
significant these
estimated values are.

11
00:00:31.530 --> 00:00:35.815
Let's begin by reviewing how
supervised learning works.

12
00:00:35.815 --> 00:00:38.600
Supervised learning
needs the training data

13
00:00:38.600 --> 00:00:40.385
that feeds to the model.

14
00:00:40.385 --> 00:00:43.660
This model has
internal parameters.

15
00:00:43.660 --> 00:00:47.310
Sometimes some model don't
have parameters at all.

16
00:00:47.310 --> 00:00:50.105
The models that have a
hyperparameter as well,

17
00:00:50.105 --> 00:00:51.310
the user need to tweak.

18
00:00:51.310 --> 00:00:53.105
But anyway, with that,

19
00:00:53.105 --> 00:00:56.340
the model can predict the value.

20
00:00:56.420 --> 00:00:59.900
If the parameters for

21
00:00:59.900 --> 00:01:02.675
the parametric model
is not optimized,

22
00:01:02.675 --> 00:01:04.280
then this prediction value will

23
00:01:04.280 --> 00:01:06.980
be far away from the target.

24
00:01:06.980 --> 00:01:10.595
Our goal is to tweak
this parameter by

25
00:01:10.595 --> 00:01:13.790
optimization so that
the model makes

26
00:01:13.790 --> 00:01:15.350
a prediction that's close to

27
00:01:15.350 --> 00:01:18.060
the target as much as possible.

28
00:01:18.600 --> 00:01:21.205
So what is the
linear regression?

29
00:01:21.205 --> 00:01:23.200
It is one of the simplest kind

30
00:01:23.200 --> 00:01:25.970
of supervised learning model.

31
00:01:26.060 --> 00:01:31.535
It predicts a real value
number, which is regression.

32
00:01:31.535 --> 00:01:34.780
Then it has the
parameters inside.

33
00:01:34.780 --> 00:01:37.940
These parameters are often
called the coefficients.

34
00:01:37.940 --> 00:01:40.600
It does not have a
hyperparameters.

35
00:01:40.600 --> 00:01:43.630
That means the user
doesn't need to figure out

36
00:01:43.630 --> 00:01:47.930
some design parameters in
advance or during the training.

37
00:01:49.190 --> 00:01:52.930
Importantly, linear
regression model assumes

38
00:01:52.930 --> 00:01:54.610
a linear relationship between

39
00:01:54.610 --> 00:01:57.080
the features and the
target variable.

40
00:01:57.080 --> 00:01:59.205
Well, what does it mean?

41
00:01:59.205 --> 00:02:00.975
It means the feature,

42
00:02:00.975 --> 00:02:03.890
let's say we have only
one feature for now,

43
00:02:03.890 --> 00:02:07.890
has a linear relationship
to the target variable.

44
00:02:08.960 --> 00:02:13.590
Let's say it's a house size.

45
00:02:13.590 --> 00:02:16.480
This is the house price.

46
00:02:16.960 --> 00:02:20.480
Then there could be
some data like this,

47
00:02:20.480 --> 00:02:24.210
that tells us that when the
house size gets larger,

48
00:02:24.210 --> 00:02:26.750
then the house
price gets larger.

49
00:02:26.750 --> 00:02:32.240
Another example could
be maybe we want to

50
00:02:32.240 --> 00:02:34.880
predict the salary
of a person as

51
00:02:34.880 --> 00:02:38.550
a function of their
years of experience.

52
00:02:39.820 --> 00:02:43.620
We might have some
data like that,

53
00:02:44.230 --> 00:02:48.010
that shows that in general,

54
00:02:48.010 --> 00:02:51.505
when the years of
experience goes up,

55
00:02:51.505 --> 00:02:53.755
then the salary goes up.

56
00:02:53.755 --> 00:02:58.100
It doesn't have to be
positive slope all the time.

57
00:02:58.100 --> 00:03:02.005
There could be some
other example like this.

58
00:03:02.005 --> 00:03:08.650
Maybe the data looks like
this and this is age.

59
00:03:08.650 --> 00:03:17.935
This is a survival rate from
some disease such as cancer.

60
00:03:17.935 --> 00:03:22.735
Then maybe there is a trend
that looks like this.

61
00:03:22.735 --> 00:03:24.085
As the age goes up,

62
00:03:24.085 --> 00:03:26.690
maybe survival rate goes down.

63
00:03:26.690 --> 00:03:29.570
These examples show some kind of

64
00:03:29.570 --> 00:03:31.970
linear relationship
of the feature

65
00:03:31.970 --> 00:03:34.180
to the target variable.

66
00:03:34.180 --> 00:03:37.200
When we have a multiple feature,

67
00:03:37.200 --> 00:03:42.255
linear model also have some
linear combination shape.

68
00:03:42.255 --> 00:03:44.010
What that means is this.

69
00:03:44.010 --> 00:03:47.400
If I have a feature X1 all
the way to feature Xp,

70
00:03:47.400 --> 00:03:51.400
and they are linearly
combined to each other.

71
00:03:51.400 --> 00:03:55.080
X1, there is a coefficient a1,

72
00:03:55.080 --> 00:04:02.410
and I add up another coefficient
times x2 plus et cetera.

73
00:04:02.410 --> 00:04:06.765
Coefficient for feature p.

74
00:04:06.765 --> 00:04:08.780
Then I can also add

75
00:04:08.780 --> 00:04:12.905
some free parameter
A0 for the intercept.

76
00:04:12.905 --> 00:04:18.150
This becomes my linear model.

77
00:04:19.280 --> 00:04:22.535
This is called a
linear combination.

78
00:04:22.535 --> 00:04:24.425
This type of model,

79
00:04:24.425 --> 00:04:27.920
whether we have many variables
or one variable that

80
00:04:27.920 --> 00:04:29.780
shows some linear relationship

81
00:04:29.780 --> 00:04:31.610
of the variable to the target,

82
00:04:31.610 --> 00:04:36.070
and this type of model is
called linear regression.

83
00:04:38.020 --> 00:04:40.385
Let's take an example.

84
00:04:40.385 --> 00:04:43.045
This data is coming
from Kaggle website.

85
00:04:43.045 --> 00:04:47.175
Kaggle is a repository for
machine learning data.

86
00:04:47.175 --> 00:04:49.780
So if you want to build
a machine learning model

87
00:04:49.780 --> 00:04:52.150
and train to the data,

88
00:04:52.150 --> 00:04:54.295
this is a place to go.

89
00:04:54.295 --> 00:04:57.610
This website also hosts
the ML competition.

90
00:04:57.610 --> 00:05:02.590
That means a lot of competitors
build their models that

91
00:05:02.590 --> 00:05:04.810
fits the data and then
they will compare

92
00:05:04.810 --> 00:05:08.110
their model performance
on this platform.

93
00:05:08.110 --> 00:05:10.735
This is super fun,
so you should try.

94
00:05:10.735 --> 00:05:12.775
Anyway, this data
comes from there,

95
00:05:12.775 --> 00:05:14.200
and this data is about

96
00:05:14.200 --> 00:05:16.210
predicting the house
sales price in

97
00:05:16.210 --> 00:05:18.970
Washington state where there

98
00:05:18.970 --> 00:05:22.145
are a bunch of features
that describes the house.

99
00:05:22.145 --> 00:05:24.730
Price is our target variable Y,

100
00:05:24.730 --> 00:05:28.230
and all these other
columns are our features.

101
00:05:28.230 --> 00:05:31.310
And because we want

102
00:05:31.310 --> 00:05:35.690
to build a simple
regression model like this,

103
00:05:35.690 --> 00:05:39.680
we want to find out
which feature could be

104
00:05:39.680 --> 00:05:43.760
a good predictor to predict
the house sales price.

105
00:05:43.760 --> 00:05:46.250
If you have a domain knowledge,

106
00:05:46.250 --> 00:05:48.980
you can think about what feature

107
00:05:48.980 --> 00:05:51.830
will be useful to
predict the house price,

108
00:05:51.830 --> 00:05:53.525
or you can think about

109
00:05:53.525 --> 00:05:56.360
maybe number of
bedrooms are important.

110
00:05:56.360 --> 00:05:58.610
The more number of bedrooms

111
00:05:58.610 --> 00:06:00.905
then maybe it's more expensive,

112
00:06:00.905 --> 00:06:04.280
or you can think about the
size of the house matters,

113
00:06:04.280 --> 00:06:06.260
or you can think
about the location of

114
00:06:06.260 --> 00:06:08.885
the house matters most,
things like that.

115
00:06:08.885 --> 00:06:14.060
However, to quantify and have
some evidence that which

116
00:06:14.060 --> 00:06:17.135
features is most important

117
00:06:17.135 --> 00:06:21.020
or likely to important
to predict the price,

118
00:06:21.020 --> 00:06:25.310
we can have a look at
the correlation matrix.

119
00:06:25.310 --> 00:06:27.530
Correlation matrix gives

120
00:06:27.530 --> 00:06:30.545
a correlation values
between the features.

121
00:06:30.545 --> 00:06:35.750
Diagonal elements shows the
correlation to the cell.

122
00:06:35.750 --> 00:06:38.570
It has the value of
one all the time.

123
00:06:38.570 --> 00:06:41.450
However, the other
off-diagonal terms,

124
00:06:41.450 --> 00:06:44.120
they show the correlation
between different features.

125
00:06:44.120 --> 00:06:47.480
Because it's too
many, 21 features,

126
00:06:47.480 --> 00:06:52.325
I'm going to select first a
few and then I'll look at it.

127
00:06:52.325 --> 00:06:56.015
As you can see from
the first row,

128
00:06:56.015 --> 00:06:58.370
which is correlation values

129
00:06:58.370 --> 00:07:00.905
for all other features
to the price,

130
00:07:00.905 --> 00:07:03.680
you can figure out the
square foot living,

131
00:07:03.680 --> 00:07:05.135
which is a house size,

132
00:07:05.135 --> 00:07:08.165
is most correlated to the price.

133
00:07:08.165 --> 00:07:11.660
There are other features such
as the grade of the house

134
00:07:11.660 --> 00:07:16.710
that comparably good
to predict the price.

135
00:07:17.860 --> 00:07:20.450
You should be careful when you

136
00:07:20.450 --> 00:07:22.460
select multiple
features based on

137
00:07:22.460 --> 00:07:28.070
correlation matrix because
the order of correlation,

138
00:07:28.070 --> 00:07:30.830
that means a high correlation or

139
00:07:30.830 --> 00:07:35.135
absolute value of a
correlation to lower ones.

140
00:07:35.135 --> 00:07:37.700
These orders are not

141
00:07:37.700 --> 00:07:41.930
directly related to how
important the features are.

142
00:07:41.930 --> 00:07:45.740
For example, this
feature may have

143
00:07:45.740 --> 00:07:49.160
the same or comparable
correlation value

144
00:07:49.160 --> 00:07:51.590
to the price with the
square foot living.

145
00:07:51.590 --> 00:07:53.720
However, square foot living

146
00:07:53.720 --> 00:07:56.090
in grays are highly correlated,

147
00:07:56.090 --> 00:07:57.920
so when I add this feature to

148
00:07:57.920 --> 00:08:00.620
my model on top of a
square foot living,

149
00:08:00.620 --> 00:08:03.020
that doesn't add so much value

150
00:08:03.020 --> 00:08:06.365
because this is pretty
similar to this one.

151
00:08:06.365 --> 00:08:09.290
In that case, some
other variables

152
00:08:09.290 --> 00:08:11.375
such as floors or
something like that,

153
00:08:11.375 --> 00:08:15.920
or maybe view would add
better value to predict

154
00:08:15.920 --> 00:08:19.430
the price than this one

155
00:08:19.430 --> 00:08:21.665
that has a high
correlation to the price.

156
00:08:21.665 --> 00:08:23.585
You have to be
little bit careful.

157
00:08:23.585 --> 00:08:26.840
We're going to go through a
method that actually helps

158
00:08:26.840 --> 00:08:31.140
to select the features
in right order,

159
00:08:31.150 --> 00:08:36.860
but to select just one
feature correlation matrix

160
00:08:36.860 --> 00:08:39.180
gives a good information.

161
00:08:39.790 --> 00:08:42.605
Let's begin by that.

162
00:08:42.605 --> 00:08:45.680
Let's talk about univariate
linear regression.

163
00:08:45.680 --> 00:08:49.550
Univariate means the
variable is only one.

164
00:08:49.550 --> 00:08:53.195
Also for that same reason,

165
00:08:53.195 --> 00:08:55.250
univariate linear
regression is called

166
00:08:55.250 --> 00:08:57.920
the simple linear
regression and often takes

167
00:08:57.920 --> 00:09:04.250
this form that we have a
coefficient Beta 0 and Beta 1,

168
00:09:04.250 --> 00:09:07.115
which represents the
intercept and slope.

169
00:09:07.115 --> 00:09:11.000
Then it has residuals
that measures

170
00:09:11.000 --> 00:09:13.460
the difference between
the target value

171
00:09:13.460 --> 00:09:16.175
and the prediction
value by our model.

172
00:09:16.175 --> 00:09:19.850
This residual is
important to measure

173
00:09:19.850 --> 00:09:23.780
the error and this is
for each data point.

174
00:09:23.780 --> 00:09:28.730
For example, if we have
some data that looks like

175
00:09:28.730 --> 00:09:34.265
this and maybe this is
my regression line,

176
00:09:34.265 --> 00:09:36.740
then this is going to be

177
00:09:36.740 --> 00:09:41.360
my intercept and
the slope Beta 1.

178
00:09:41.360 --> 00:09:45.995
Each discrepancy
of the data points

179
00:09:45.995 --> 00:09:49.295
to the regression line
is called the residuals.

180
00:09:49.295 --> 00:09:51.230
Our goal is to minimize

181
00:09:51.230 --> 00:09:55.130
the overall residuals
of my model and make

182
00:09:55.130 --> 00:09:57.980
my model to produce or predict

183
00:09:57.980 --> 00:09:59.510
the value that's as

184
00:09:59.510 --> 00:10:02.550
close as possible to
the target variable.

185
00:10:03.580 --> 00:10:06.650
This can be done using

186
00:10:06.650 --> 00:10:10.160
a single line using
statsmodel OLS package,

187
00:10:10.160 --> 00:10:15.320
or there are other packages
such as sklearn linear model.

188
00:10:15.320 --> 00:10:18.410
However, this is

189
00:10:18.410 --> 00:10:22.950
useful because it generates
some summary table like this.

190
00:10:22.950 --> 00:10:26.560
This summary table has
a lot of information

191
00:10:26.560 --> 00:10:30.355
including the most
interesting part,

192
00:10:30.355 --> 00:10:33.020
or rather my coefficient values.

193
00:10:34.510 --> 00:10:36.890
With this coefficient value,

194
00:10:36.890 --> 00:10:38.870
I can determine
what's my slope and

195
00:10:38.870 --> 00:10:43.560
my intercept is for my simple
linear regression model.

196
00:10:45.010 --> 00:10:48.035
Beside of coefficient values,

197
00:10:48.035 --> 00:10:50.630
we can ask some other questions

198
00:10:50.630 --> 00:10:54.290
that are important to
linear regression.

199
00:10:54.290 --> 00:10:58.025
We'll begin by how do we
determine the coefficients?

200
00:10:58.025 --> 00:11:01.040
In other words, how does
the model training works

201
00:11:01.040 --> 00:11:04.985
under the hood of this package?

202
00:11:04.985 --> 00:11:09.980
We'll also discuss how
well my model fits.

203
00:11:09.980 --> 00:11:12.245
From the summary table values,

204
00:11:12.245 --> 00:11:16.050
what gives an idea of
how my model fits?

205
00:11:16.180 --> 00:11:19.340
Then we'll also talk about how

206
00:11:19.340 --> 00:11:22.475
statistically significant
my coefficients are.

207
00:11:22.475 --> 00:11:25.460
That means how robust

208
00:11:25.460 --> 00:11:28.830
our estimation for
the coefficient is.

209
00:11:29.140 --> 00:11:31.790
We're going to also
talk about how

210
00:11:31.790 --> 00:11:34.115
well my model predicts
on unseen data.

211
00:11:34.115 --> 00:11:36.875
That means, how well
does it generalize,

212
00:11:36.875 --> 00:11:39.660
which is very important
in machine learning.